#!/usr/bin/env python3
"""
í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ ëª¨ë¸ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤.
GridSearchCVì™€ RandomizedSearchCVë¥¼ ëª¨ë‘ ì§€ì›í•˜ë©°, 
ê¸°ë³¸ ëª¨ë¸ ëŒ€ë¹„ ì„±ëŠ¥ ê°œì„ ì„ ì¶”ì í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python scripts/hyperparameter_tuning.py --method random --n_iter 50
    python scripts/hyperparameter_tuning.py --method grid --strategy quick

ì‘ì„±ì: ML Team
ìƒì„±ì¼: 2025-01-15
"""

import os
import sys
import argparse
import logging
import warnings
import pandas as pd
import numpy as np
import joblib
import yaml
from datetime import datetime
from pathlib import Path
import gc

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ
from src.training.hyperparameter_tuner import HyperparameterTuner
from src.training.trainer import MLTrainer
from src.evaluation.evaluator import ModelEvaluator
from src.evaluation.visualizer import ResultVisualizer
from sklearn.preprocessing import StandardScaler

# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings('ignore')

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


def setup_directories():
    """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
    directories = [
        "experiments/hyperparameter_tuning",
        "models/tuned",
        "logs"
    ]
    
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
        logger.info(f"ë””ë ‰í† ë¦¬ ìƒì„±/í™•ì¸: {directory}")


def load_data():
    """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
    logger.info("=== ë°ì´í„° ë¡œë“œ ì‹œì‘ ===")
    
    # ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    train_path = "data/splits/full_dataset/train.csv"
    val_path = "data/splits/full_dataset/val.csv"
    test_path = "data/splits/full_dataset/test.csv"
    
    # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    for path in [train_path, val_path, test_path]:
        if not os.path.exists(path):
            raise FileNotFoundError(f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")
    
    # ë°ì´í„° ë¡œë“œ
    train_df = pd.read_csv(train_path)
    val_df = pd.read_csv(val_path)
    test_df = pd.read_csv(test_path)
    
    logger.info(f"í›ˆë ¨ ë°ì´í„°: {train_df.shape}")
    logger.info(f"ê²€ì¦ ë°ì´í„°: {val_df.shape}")
    logger.info(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_df.shape}")
    
    # íŠ¹ì§•ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
    X_train = train_df.drop('sweetness', axis=1).values
    y_train = train_df['sweetness'].values
    X_val = val_df.drop('sweetness', axis=1).values
    y_val = val_df['sweetness'].values
    X_test = test_df.drop('sweetness', axis=1).values
    y_test = test_df['sweetness'].values
    
    # íŠ¹ì§• ìŠ¤ì¼€ì¼ë§ (SVMì„ ìœ„í•´ í•„ìˆ˜)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    X_test_scaled = scaler.transform(X_test)
    
    logger.info("ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ")
    logger.info(f"íŠ¹ì§• ìˆ˜: {X_train_scaled.shape[1]}")
    logger.info(f"ë‹¹ë„ ë²”ìœ„: {np.min(y_train):.2f} ~ {np.max(y_train):.2f} Brix")
    
    return {
        'X_train': X_train_scaled,
        'y_train': y_train,
        'X_val': X_val_scaled,
        'y_val': y_val,
        'X_test': X_test_scaled,
        'y_test': y_test,
        'scaler': scaler,
        'feature_names': train_df.drop('sweetness', axis=1).columns.tolist()
    }


def load_baseline_results():
    """ê¸°ë³¸ ëª¨ë¸ ê²°ê³¼ ë¡œë“œ"""
    logger.info("ê¸°ë³¸ ëª¨ë¸ ê²°ê³¼ ë¡œë“œ ì‹œë„...")
    
    baseline_path = "models/saved/training_summary_simple.yaml"
    
    if os.path.exists(baseline_path):
        with open(baseline_path, 'r', encoding='utf-8') as f:
            baseline_data = yaml.safe_load(f)
        
        # ê²°ê³¼ êµ¬ì¡° ë³€í™˜
        baseline_results = {}
        for model_name, metrics in baseline_data.get('test_results', {}).items():
            baseline_results[model_name] = {'test': metrics}
        
        logger.info("ê¸°ë³¸ ëª¨ë¸ ê²°ê³¼ ë¡œë“œ ì™„ë£Œ")
        for model, metrics in baseline_results.items():
            mae = metrics['test'].get('mae', 0)
            r2 = metrics['test'].get('r2', 0)
            logger.info(f"  {model}: MAE={mae:.4f}, RÂ²={r2:.4f}")
        
        return baseline_results
    else:
        logger.warning("ê¸°ë³¸ ëª¨ë¸ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {}


def run_hyperparameter_tuning(data, method='random', strategy='medium', n_iter=50):
    """í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤í–‰"""
    logger.info(f"=== í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘ ({method}) ===")
    
    # íŠœë„ˆ ì´ˆê¸°í™”
    tuner = HyperparameterTuner(
        config_path="configs/hyperparameter_search.yaml",
        scoring="neg_mean_absolute_error",
        cv=5,
        n_jobs=-1,
        verbose=1,
        random_state=42
    )
    
    # ì „ì²´ ëª¨ë¸ íŠœë‹ ì‹¤í–‰
    start_time = datetime.now()
    
    try:
        if method == 'grid':
            # GridSearchCV ì‚¬ìš©
            # strategyì— ë”°ë¼ ì„¤ì • ë³€ê²½ í•„ìš” (ì¶”í›„ êµ¬í˜„)
            results = tuner.tune_all_models(
                data['X_train'], 
                data['y_train'], 
                method='grid',
                parallel=False  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ìˆœì°¨ ì²˜ë¦¬
            )
        elif method == 'random':
            # RandomizedSearchCV ì‚¬ìš©
            results = tuner.tune_all_models(
                data['X_train'], 
                data['y_train'], 
                method='random',
                n_iter=n_iter,
                parallel=False  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ìˆœì°¨ ì²˜ë¦¬
            )
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°©ë²•: {method}")
        
        end_time = datetime.now()
        total_time = (end_time - start_time).total_seconds()
        
        logger.info(f"ì „ì²´ íŠœë‹ ì™„ë£Œ ì‹œê°„: {total_time:.2f}ì´ˆ")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()
        
        return tuner, results
        
    except Exception as e:
        logger.error(f"í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise


def evaluate_tuned_models(tuner, data):
    """íŠœë‹ëœ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
    logger.info("=== íŠœë‹ëœ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ===")
    
    # ì„±ëŠ¥ í‰ê°€
    evaluation_results = tuner.evaluate_tuned_models(
        data['X_test'], 
        data['y_test'],
        data['X_val'],
        data['y_val']
    )
    
    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    logger.info("\níŠœë‹ëœ ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½:")
    for model_name, metrics in evaluation_results.items():
        test_mae = metrics['test']['mae']
        test_r2 = metrics['test']['r2']
        val_mae = metrics['validation']['mae']
        val_r2 = metrics['validation']['r2']
        
        logger.info(f"  {model_name}:")
        logger.info(f"    í…ŒìŠ¤íŠ¸  - MAE: {test_mae:.4f}, RÂ²: {test_r2:.4f}")
        logger.info(f"    ê²€ì¦    - MAE: {val_mae:.4f}, RÂ²: {val_r2:.4f}")
    
    return evaluation_results


def compare_with_baseline(tuner, evaluation_results, baseline_results):
    """ê¸°ë³¸ ëª¨ë¸ê³¼ ì„±ëŠ¥ ë¹„êµ"""
    logger.info("=== ê¸°ë³¸ ëª¨ë¸ ëŒ€ë¹„ ì„±ëŠ¥ ê°œì„  ===")
    
    if not baseline_results:
        logger.warning("ê¸°ë³¸ ëª¨ë¸ ê²°ê³¼ê°€ ì—†ì–´ ë¹„êµë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
    
    improvements = {}
    
    for model_name in evaluation_results.keys():
        if model_name in baseline_results:
            # íŠœë‹ëœ ëª¨ë¸ ì„±ëŠ¥
            tuned_mae = evaluation_results[model_name]['test']['mae']
            tuned_r2 = evaluation_results[model_name]['test']['r2']
            
            # ê¸°ë³¸ ëª¨ë¸ ì„±ëŠ¥
            baseline_mae = baseline_results[model_name]['test']['mae']
            baseline_r2 = baseline_results[model_name]['test']['r2']
            
            # ê°œì„  ì •ë„ ê³„ì‚°
            mae_improvement = ((baseline_mae - tuned_mae) / baseline_mae) * 100
            r2_improvement = ((tuned_r2 - baseline_r2) / baseline_r2) * 100 if baseline_r2 > 0 else 0
            
            improvements[model_name] = {
                'mae_improvement': mae_improvement,
                'r2_improvement': r2_improvement,
                'baseline_mae': baseline_mae,
                'tuned_mae': tuned_mae,
                'baseline_r2': baseline_r2,
                'tuned_r2': tuned_r2
            }
            
            logger.info(f"  {model_name}:")
            logger.info(f"    MAE: {baseline_mae:.4f} â†’ {tuned_mae:.4f} ({mae_improvement:+.2f}%)")
            logger.info(f"    RÂ²:  {baseline_r2:.4f} â†’ {tuned_r2:.4f} ({r2_improvement:+.2f}%)")
    
    return improvements


def save_comprehensive_results(tuner, evaluation_results, improvements, data, method, strategy):
    """í¬ê´„ì ì¸ ê²°ê³¼ ì €ì¥"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_dir = f"experiments/hyperparameter_tuning/tuning_{method}_{timestamp}"
    os.makedirs(results_dir, exist_ok=True)
    
    logger.info(f"ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬: {results_dir}")
    
    # 1. íŠœë‹ ê²°ê³¼ ì €ì¥
    tuner.save_results(results_dir, save_models=True)
    
    # 2. í‰ê°€ ê²°ê³¼ ì €ì¥
    evaluation_path = os.path.join(results_dir, "evaluation_results.yaml")
    with open(evaluation_path, 'w', encoding='utf-8') as f:
        yaml.dump(evaluation_results, f, default_flow_style=False, allow_unicode=True)
    
    # 3. ê°œì„  ê²°ê³¼ ì €ì¥
    if improvements:
        improvement_path = os.path.join(results_dir, "improvements.yaml")
        with open(improvement_path, 'w', encoding='utf-8') as f:
            yaml.dump(improvements, f, default_flow_style=False, allow_unicode=True)
    
    # 4. ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
    scaler_path = os.path.join(results_dir, "scaler.pkl")
    joblib.dump(data['scaler'], scaler_path)
    
    # 5. ì‹¤í—˜ ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata = {
        'timestamp': timestamp,
        'method': method,
        'strategy': strategy,
        'data_shape': {
            'train': data['X_train'].shape,
            'val': data['X_val'].shape,
            'test': data['X_test'].shape
        },
        'feature_count': len(data['feature_names']),
        'target_range': {
            'min': float(data['y_train'].min()),
            'max': float(data['y_train'].max())
        }
    }
    
    metadata_path = os.path.join(results_dir, "experiment_metadata.yaml")
    with open(metadata_path, 'w', encoding='utf-8') as f:
        yaml.dump(metadata, f, default_flow_style=False, allow_unicode=True)
    
    logger.info("ëª¨ë“  ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return results_dir


def generate_summary_report(tuner, evaluation_results, improvements, results_dir):
    """ìš”ì•½ ë³´ê³ ì„œ ìƒì„±"""
    logger.info("ìš”ì•½ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ
    best_model_name, best_model = tuner.get_best_model()
    best_metrics = evaluation_results[best_model_name]['test']
    
    # ë³´ê³ ì„œ ì‘ì„±
    report = f"""
# í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê²°ê³¼ ë³´ê³ ì„œ

ìƒì„±ì¼: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ì‹¤í—˜ ê°œìš”

- **ëª©ì **: ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ ëª¨ë¸ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
- **ë°ì´í„°**: 51ê°œ íŠ¹ì§•, 146ê°œ ìƒ˜í”Œ
- **í‰ê°€ ì§€í‘œ**: MAE (Mean Absolute Error), RÂ² (R-squared)

## ìµœê³  ì„±ëŠ¥ ëª¨ë¸

**ëª¨ë¸**: {best_model_name}
- **í…ŒìŠ¤íŠ¸ MAE**: {best_metrics['mae']:.4f} Brix
- **í…ŒìŠ¤íŠ¸ RÂ²**: {best_metrics['r2']:.4f}
- **í…ŒìŠ¤íŠ¸ RMSE**: {best_metrics['rmse']:.4f} Brix

## ëª¨ë“  ëª¨ë¸ ì„±ëŠ¥

"""
    
    # ëª¨ë“  ëª¨ë¸ ì„±ëŠ¥ ì¶”ê°€
    for model_name, metrics in evaluation_results.items():
        test_metrics = metrics['test']
        report += f"""
### {model_name}
- MAE: {test_metrics['mae']:.4f} Brix
- RÂ²: {test_metrics['r2']:.4f}
- RMSE: {test_metrics['rmse']:.4f} Brix
"""
    
    # ê°œì„  ì‚¬í•­ ì¶”ê°€
    if improvements:
        report += "\n## ê¸°ë³¸ ëª¨ë¸ ëŒ€ë¹„ ê°œì„ \n\n"
        for model_name, imp in improvements.items():
            report += f"""
### {model_name}
- MAE ê°œì„ : {imp['mae_improvement']:+.2f}%
- RÂ² ê°œì„ : {imp['r2_improvement']:+.2f}%
"""
    
    # ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
    report += f"""
## ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„± ì—¬ë¶€

- **MAE < 1.0 Brix**: {'âœ… ë‹¬ì„±' if best_metrics['mae'] < 1.0 else 'âŒ ë¯¸ë‹¬ì„±'} ({best_metrics['mae']:.4f})
- **RÂ² > 0.8**: {'âœ… ë‹¬ì„±' if best_metrics['r2'] > 0.8 else 'âŒ ë¯¸ë‹¬ì„±'} ({best_metrics['r2']:.4f})

## ê²°ë¡ 

ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì¸ **{best_model_name}**ì´ MAE {best_metrics['mae']:.4f} Brix, RÂ² {best_metrics['r2']:.4f}ì˜ 
{'ìš°ìˆ˜í•œ' if best_metrics['mae'] < 1.0 and best_metrics['r2'] > 0.8 else 'ì–‘í˜¸í•œ'} ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
"""
    
    # ë³´ê³ ì„œ ì €ì¥
    report_path = os.path.join(results_dir, "TUNING_REPORT.md")
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    logger.info(f"ìš”ì•½ ë³´ê³ ì„œ ì €ì¥: {report_path}")
    
    # ì½˜ì†”ì—ë„ í•µì‹¬ ë‚´ìš© ì¶œë ¥
    print("\n" + "="*60)
    print("ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì™„ë£Œ!")
    print("="*60)
    print(f"ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name}")
    print(f"í…ŒìŠ¤íŠ¸ MAE: {best_metrics['mae']:.4f} Brix")
    print(f"í…ŒìŠ¤íŠ¸ RÂ²: {best_metrics['r2']:.4f}")
    print(f"ëª©í‘œ ë‹¬ì„±: {'âœ…' if best_metrics['mae'] < 1.0 and best_metrics['r2'] > 0.8 else 'âš ï¸'}")
    print(f"ê²°ê³¼ ì €ì¥: {results_dir}")
    print("="*60)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤í–‰')
    parser.add_argument('--method', choices=['grid', 'random'], default='random',
                        help='íŠœë‹ ë°©ë²• (default: random)')
    parser.add_argument('--strategy', choices=['quick', 'medium', 'thorough'], default='medium',
                        help='ê²€ìƒ‰ ì „ëµ (default: medium)')
    parser.add_argument('--n_iter', type=int, default=50,
                        help='RandomizedSearchCV ë°˜ë³µ íšŸìˆ˜ (default: 50)')
    parser.add_argument('--no_baseline', action='store_true',
                        help='ê¸°ë³¸ ëª¨ë¸ê³¼ ë¹„êµí•˜ì§€ ì•ŠìŒ')
    
    args = parser.parse_args()
    
    logger.info("ğŸ‰ ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘")
    logger.info(f"ì„¤ì •: method={args.method}, strategy={args.strategy}, n_iter={args.n_iter}")
    
    try:
        # 1. í™˜ê²½ ì„¤ì •
        setup_directories()
        
        # 2. ë°ì´í„° ë¡œë“œ
        data = load_data()
        
        # 3. ê¸°ë³¸ ëª¨ë¸ ê²°ê³¼ ë¡œë“œ
        baseline_results = {} if args.no_baseline else load_baseline_results()
        
        # 4. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤í–‰
        tuner, tuning_results = run_hyperparameter_tuning(
            data, 
            method=args.method, 
            strategy=args.strategy, 
            n_iter=args.n_iter
        )
        
        # 5. ì„±ëŠ¥ í‰ê°€
        evaluation_results = evaluate_tuned_models(tuner, data)
        
        # 6. ê¸°ë³¸ ëª¨ë¸ê³¼ ë¹„êµ
        improvements = compare_with_baseline(tuner, evaluation_results, baseline_results)
        
        # 7. ê²°ê³¼ ì €ì¥
        results_dir = save_comprehensive_results(
            tuner, evaluation_results, improvements, data, args.method, args.strategy
        )
        
        # 8. ìš”ì•½ ë³´ê³ ì„œ ìƒì„±
        generate_summary_report(tuner, evaluation_results, improvements, results_dir)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del tuner, data
        gc.collect()
        
        logger.info("ğŸ‰ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    except Exception as e:
        logger.error(f"âŒ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)
    finally:
        # ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()


if __name__ == "__main__":
    main() 