#!/usr/bin/env python3
"""
ğŸ‰ ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ ëª¨ë¸ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸

ì „í†µì ì¸ ML ëª¨ë¸(GBT, SVM, Random Forest)ì„ í›ˆë ¨í•˜ê³  í‰ê°€í•˜ëŠ” ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

Usage:
    python scripts/train_models.py [--config CONFIG_PATH] [--quick] [--no-viz]

Author: Watermelon ML Team
Date: 2025-01-15
"""

import os
import sys
import argparse
import logging
import warnings
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Tuple

import yaml
import pandas as pd
import numpy as np
from sklearn.metrics import mean_absolute_error, r2_score

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from src.training.trainer import MLTrainer, create_trainer_from_config
from src.evaluation.evaluator import ModelEvaluator
from src.evaluation.visualizer import ResultVisualizer
from src.models.traditional_ml import ModelFactory

# ê²½ê³  ë©”ì‹œì§€ ì–µì œ
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', category=FutureWarning)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(PROJECT_ROOT / 'experiments' / f'training_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
    ]
)
logger = logging.getLogger(__name__)


def setup_directories() -> None:
    """í•„ìš”í•œ ë””ë ‰í† ë¦¬ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    directories = [
        PROJECT_ROOT / 'experiments',
        PROJECT_ROOT / 'models' / 'saved',
        PROJECT_ROOT / 'experiments' / 'results',
        PROJECT_ROOT / 'experiments' / 'plots'
    ]
    
    for directory in directories:
        directory.mkdir(parents=True, exist_ok=True)
    
    logger.info(f"ë””ë ‰í† ë¦¬ ì„¤ì • ì™„ë£Œ: {len(directories)}ê°œ")


def load_config(config_path: str) -> Dict[str, Any]:
    """ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        logger.info(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {config_path}")
        return config
    except Exception as e:
        logger.error(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise


def load_datasets() -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        base_path = PROJECT_ROOT / 'data' / 'splits' / 'full_dataset'
        
        train_df = pd.read_csv(base_path / 'train.csv')
        val_df = pd.read_csv(base_path / 'val.csv')
        test_df = pd.read_csv(base_path / 'test.csv')
        
        logger.info(f"ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ:")
        logger.info(f"  - í›ˆë ¨: {len(train_df)}ê°œ")
        logger.info(f"  - ê²€ì¦: {len(val_df)}ê°œ") 
        logger.info(f"  - í…ŒìŠ¤íŠ¸: {len(test_df)}ê°œ")
        logger.info(f"  - ë‹¹ë„ ë²”ìœ„: {train_df['sweetness'].min():.1f} ~ {train_df['sweetness'].max():.1f} Brix")
        
        return train_df, val_df, test_df
        
    except Exception as e:
        logger.error(f"ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise


def print_training_summary(config: Dict[str, Any]) -> None:
    """í›ˆë ¨ ì„¤ì • ìš”ì•½ì„ ì¶œë ¥í•©ë‹ˆë‹¤."""
    print("\n" + "="*80)
    print("ğŸ‰ ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
    print("="*80)
    
    print(f"ğŸ“… í›ˆë ¨ ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ¯ ì„±ëŠ¥ ëª©í‘œ:")
    print(f"   - MAE < {config['performance']['target_mae']:.1f} Brix")
    print(f"   - RÂ² > {config['performance']['target_r2']:.2f}")
    
    print(f"ğŸ¤– í›ˆë ¨ ëª¨ë¸:")
    for model_name in config['models'].keys():
        print(f"   - {model_name}")
    
    print(f"âš™ï¸  í›ˆë ¨ ì„¤ì •:")
    print(f"   - êµì°¨ ê²€ì¦: {config['cross_validation']['n_folds']}-fold")
    print(f"   - ìŠ¤ì¼€ì¼ëŸ¬: {config['preprocessing']['scaler_type']}")
    print(f"   - Random State: {config['global']['random_state']}")
    print("="*80 + "\n")


def evaluate_and_visualize(
    trainer: MLTrainer,
    train_df: pd.DataFrame,
    val_df: pd.DataFrame,
    test_df: pd.DataFrame,
    config: Dict[str, Any],
    create_visualizations: bool = True
) -> Dict[str, Any]:
    """ëª¨ë¸ì„ í‰ê°€í•˜ê³  ì‹œê°í™”í•©ë‹ˆë‹¤."""
    
    logger.info("ëª¨ë¸ í‰ê°€ ì‹œì‘...")
    
    # íŠ¹ì§•ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
    feature_cols = [col for col in train_df.columns if col != 'sweetness']
    
    X_train = train_df[feature_cols].values
    y_train = train_df['sweetness'].values
    X_val = val_df[feature_cols].values
    y_val = val_df['sweetness'].values
    X_test = test_df[feature_cols].values
    y_test = test_df['sweetness'].values
    
    # í‰ê°€ì ì´ˆê¸°í™”
    evaluator = ModelEvaluator()
    all_results = {}
    
    # ê° ëª¨ë¸ í‰ê°€
    for model_name, model in trainer.models.items():
        logger.info(f"  {model_name} í‰ê°€ ì¤‘...")
        
        # ì˜ˆì¸¡
        y_train_pred = model.predict(X_train)
        y_val_pred = model.predict(X_val)
        y_test_pred = model.predict(X_test)
        
        # í›ˆë ¨ ì„¸íŠ¸ í‰ê°€
        train_results = evaluator.evaluate_model_performance(
            y_train, y_train_pred, model_name, "í›ˆë ¨"
        )
        
        # ê²€ì¦ ì„¸íŠ¸ í‰ê°€
        val_results = evaluator.evaluate_model_performance(
            y_val, y_val_pred, model_name, "ê²€ì¦"
        )
        
        # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€
        test_results = evaluator.evaluate_model_performance(
            y_test, y_test_pred, model_name, "í…ŒìŠ¤íŠ¸"
        )
        
        # ê²°ê³¼ ì €ì¥
        all_results[model_name] = {
            'train': train_results,
            'val': val_results,
            'test': test_results,
            'predictions': {
                'train': y_train_pred,
                'val': y_val_pred,
                'test': y_test_pred
            }
        }
    
    # ì‹œê°í™” ìƒì„±
    if create_visualizations:
        logger.info("ì‹œê°í™” ìƒì„± ì¤‘...")
        visualizer = ResultVisualizer()
        
        # ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸
        performance_data = []
        for model_name, results in all_results.items():
            for dataset_type in ['train', 'val', 'test']:
                result = results[dataset_type]
                performance_data.append({
                    'Model': model_name,
                    'Dataset': dataset_type,
                    'MAE': result.mae,
                    'R2': result.r2,
                    'RMSE': result.rmse
                })
        
        performance_df = pd.DataFrame(performance_data)
        
        # ì„±ëŠ¥ ë¹„êµ í”Œë¡¯
        fig_performance = visualizer.plot_model_comparison(performance_df)
        fig_performance.write_html(PROJECT_ROOT / 'experiments' / 'plots' / 'model_performance_comparison.html')
        
        # ê° ëª¨ë¸ë³„ ì˜ˆì¸¡ vs ì‹¤ì œ í”Œë¡¯
        for model_name, results in all_results.items():
            # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì˜ˆì¸¡ vs ì‹¤ì œ
            fig_pred = visualizer.plot_predictions_vs_actual(
                y_test, results['predictions']['test'],
                f"{model_name} - í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì˜ˆì¸¡ vs ì‹¤ì œ"
            )
            fig_pred.write_html(
                PROJECT_ROOT / 'experiments' / 'plots' / f'{model_name}_predictions_vs_actual.html'
            )
            
            # ì”ì°¨ í”Œë¡¯
            fig_residual = visualizer.plot_residuals(
                y_test, results['predictions']['test'],
                f"{model_name} - ì”ì°¨ ë¶„ì„"
            )
            fig_residual.write_html(
                PROJECT_ROOT / 'experiments' / 'plots' / f'{model_name}_residuals.html'
            )
        
        # íŠ¹ì§• ì¤‘ìš”ë„ (Random Forestì™€ GBTë§Œ)
        for model_name, model in trainer.models.items():
            if hasattr(model.model, 'feature_importances_'):
                importance_dict = model.get_feature_importance()
                if importance_dict:
                    fig_importance = visualizer.plot_feature_importance(
                        importance_dict, f"{model_name} - íŠ¹ì§• ì¤‘ìš”ë„"
                    )
                    fig_importance.write_html(
                        PROJECT_ROOT / 'experiments' / 'plots' / f'{model_name}_feature_importance.html'
                    )
        
        logger.info(f"ì‹œê°í™” ì™„ë£Œ: experiments/plots/ ë””ë ‰í† ë¦¬")
    
    return all_results


def print_results_summary(all_results: Dict[str, Any], config: Dict[str, Any]) -> str:
    """ê²°ê³¼ ìš”ì•½ì„ ì¶œë ¥í•˜ê³  ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    
    print("\n" + "="*80)
    print("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½")
    print("="*80)
    
    # í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ìš”ì•½ í…Œì´ë¸”
    print(f"{'ëª¨ë¸':<20} {'MAE':<8} {'RÂ²':<8} {'RMSE':<8} {'ëª©í‘œë‹¬ì„±':<10}")
    print("-" * 70)
    
    best_model = None
    best_mae = float('inf')
    target_mae = config['performance']['target_mae']
    target_r2 = config['performance']['target_r2']
    
    for model_name, results in all_results.items():
        test_result = results['test']
        mae = test_result.mae
        r2 = test_result.r2
        rmse = test_result.rmse
        
        # ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
        mae_ok = "âœ…" if mae < target_mae else "âŒ"
        r2_ok = "âœ…" if r2 > target_r2 else "âŒ"
        goal_status = f"{mae_ok} {r2_ok}"
        
        print(f"{model_name:<20} {mae:<8.3f} {r2:<8.3f} {rmse:<8.3f} {goal_status:<10}")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸° (MAE ê¸°ì¤€)
        if mae < best_mae:
            best_mae = mae
            best_model = model_name
    
    print("-" * 70)
    print(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model} (MAE: {best_mae:.3f})")
    
    # ëª©í‘œ ë‹¬ì„± ìš”ì•½
    print(f"\nğŸ¯ ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„± í˜„í™©:")
    models_meeting_mae = sum(1 for results in all_results.values() if results['test'].mae < target_mae)
    models_meeting_r2 = sum(1 for results in all_results.values() if results['test'].r2 > target_r2)
    total_models = len(all_results)
    
    print(f"   - MAE < {target_mae}: {models_meeting_mae}/{total_models} ëª¨ë¸")
    print(f"   - RÂ² > {target_r2}: {models_meeting_r2}/{total_models} ëª¨ë¸")
    
    if best_mae < target_mae:
        print(f"   âœ… ì£¼ìš” ëª©í‘œ ë‹¬ì„±! (MAE < {target_mae})")
    else:
        print(f"   âŒ ì£¼ìš” ëª©í‘œ ë¯¸ë‹¬ì„± (MAE >= {target_mae})")
    
    print("="*80 + "\n")
    
    return best_model


def save_best_model(trainer: MLTrainer, best_model_name: str) -> None:
    """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤."""
    
    try:
        best_model = trainer.models[best_model_name]
        
        # ëª¨ë¸ ì €ì¥
        model_path = PROJECT_ROOT / 'models' / 'saved' / 'best_model.pkl'
        best_model.save_model(str(model_path))
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ (ìˆëŠ” ê²½ìš°)
        if hasattr(best_model, 'scaler') and best_model.scaler is not None:
            scaler_path = PROJECT_ROOT / 'models' / 'saved' / 'scaler.pkl'
            import joblib
            joblib.dump(best_model.scaler, scaler_path)
            logger.info(f"ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥: {scaler_path}")
        
        # ëª¨ë¸ ì„¤ì • ì €ì¥
        config_path = PROJECT_ROOT / 'models' / 'saved' / 'model_config.yaml'
        model_config = {
            'best_model': best_model_name,
            'model_type': type(best_model).__name__,
            'training_date': datetime.now().isoformat(),
            'feature_count': len(best_model.feature_names_) if hasattr(best_model, 'feature_names_') else 51,
            'performance': {
                'test_mae': float(trainer.latest_results[best_model_name]['test'].mae),
                'test_r2': float(trainer.latest_results[best_model_name]['test'].r2)
            }
        }
        
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(model_config, f, default_flow_style=False, allow_unicode=True)
        
        logger.info(f"ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ ì™„ë£Œ:")
        logger.info(f"  - ëª¨ë¸: {model_path}")
        logger.info(f"  - ì„¤ì •: {config_path}")
        
    except Exception as e:
        logger.error(f"ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")


def save_training_results(
    all_results: Dict[str, Any],
    training_summary: Dict[str, Any],
    config: Dict[str, Any]
) -> None:
    """í›ˆë ¨ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    try:
        # ì„±ëŠ¥ ìš”ì•½ CSV
        performance_data = []
        for model_name, results in all_results.items():
            for dataset_type in ['train', 'val', 'test']:
                result = results[dataset_type]
                performance_data.append({
                    'model': model_name,
                    'dataset': dataset_type,
                    'mae': result.mae,
                    'mse': result.mse,
                    'rmse': result.rmse,
                    'r2': result.r2,
                    'mape': result.mape,
                    'brix_accuracy_0_5': result.domain_metrics.get('brix_accuracy_0_5', 0),
                    'brix_accuracy_1_0': result.domain_metrics.get('brix_accuracy_1_0', 0),
                    'performance_grade': result.performance_grade
                })
        
        performance_df = pd.DataFrame(performance_data)
        performance_path = PROJECT_ROOT / 'experiments' / 'results' / f'performance_summary_{timestamp}.csv'
        performance_df.to_csv(performance_path, index=False)
        
        # ìƒì„¸ ê²°ê³¼ YAML
        detailed_results = {
            'experiment_info': {
                'timestamp': timestamp,
                'config_used': config,
                'training_summary': training_summary
            },
            'model_results': {}
        }
        
        for model_name, results in all_results.items():
            detailed_results['model_results'][model_name] = {
                'train': results['train'].to_dict(),
                'val': results['val'].to_dict(),
                'test': results['test'].to_dict()
            }
        
        detailed_path = PROJECT_ROOT / 'experiments' / 'results' / f'detailed_results_{timestamp}.yaml'
        with open(detailed_path, 'w', encoding='utf-8') as f:
            yaml.dump(detailed_results, f, default_flow_style=False, allow_unicode=True)
        
        logger.info(f"í›ˆë ¨ ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
        logger.info(f"  - ì„±ëŠ¥ ìš”ì•½: {performance_path}")
        logger.info(f"  - ìƒì„¸ ê²°ê³¼: {detailed_path}")
        
    except Exception as e:
        logger.error(f"ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±
    parser = argparse.ArgumentParser(description='ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ ëª¨ë¸ í›ˆë ¨')
    parser.add_argument('--config', default='configs/models.yaml', help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--quick', action='store_true', help='ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì‘ì€ í•˜ì´í¼íŒŒë¼ë¯¸í„°)')
    parser.add_argument('--no-viz', action='store_true', help='ì‹œê°í™” ê±´ë„ˆë›°ê¸°')
    
    args = parser.parse_args()
    
    try:
        # ì´ˆê¸° ì„¤ì •
        setup_directories()
        
        # ì„¤ì • ë¡œë“œ
        config_path = PROJECT_ROOT / args.config
        config = load_config(config_path)
        
        # ë¹ ë¥¸ ëª¨ë“œ ì„¤ì • ì¡°ì •
        if args.quick:
            logger.info("ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ í™œì„±í™”")
            if 'gradient_boosting' in config['models']:
                config['models']['gradient_boosting']['n_estimators'] = 50
            if 'random_forest' in config['models']:
                config['models']['random_forest']['n_estimators'] = 50
            config['cross_validation']['n_folds'] = 3
        
        # ë°ì´í„° ë¡œë“œ
        train_df, val_df, test_df = load_datasets()
        
        # í›ˆë ¨ ìš”ì•½ ì¶œë ¥
        print_training_summary(config)
        
        # íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° í›ˆë ¨
        logger.info("ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
        trainer = create_trainer_from_config(config)
        
        # íŠ¹ì§•ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
        feature_cols = [col for col in train_df.columns if col != 'sweetness']
        X_train = train_df[feature_cols].values
        y_train = train_df['sweetness'].values
        X_val = val_df[feature_cols].values  
        y_val = val_df['sweetness'].values
        
        # í›ˆë ¨ ì‹¤í–‰
        training_results = trainer.train_models(X_train, y_train, X_val, y_val)
        
        logger.info("í›ˆë ¨ ì™„ë£Œ! í‰ê°€ ì‹œì‘...")
        
        # í‰ê°€ ë° ì‹œê°í™”
        all_results = evaluate_and_visualize(
            trainer, train_df, val_df, test_df, config, 
            create_visualizations=not args.no_viz
        )
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        best_model_name = print_results_summary(all_results, config)
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        save_best_model(trainer, best_model_name)
        
        # í›ˆë ¨ ê²°ê³¼ ì €ì¥
        training_summary = training_results.get_summary()
        save_training_results(all_results, training_summary, config)
        
        # ì™„ë£Œ ë©”ì‹œì§€
        print("\nğŸ‰ ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€ ì™„ë£Œ!")
        print(f"ğŸ“ ê²°ê³¼ íŒŒì¼: experiments/results/")
        print(f"ğŸ“Š ì‹œê°í™”: experiments/plots/")
        print(f"ğŸ† ìµœê³  ëª¨ë¸: models/saved/")
        
        return 0
        
    except Exception as e:
        logger.error(f"í›ˆë ¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    finally:
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        import gc
        gc.collect()
        logger.info("ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")


if __name__ == "__main__":
    sys.exit(main()) 