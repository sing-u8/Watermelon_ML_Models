#!/usr/bin/env python3
"""
ğŸ‰ ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ ëª¨ë¸ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸

ì „í†µì ì¸ ML ëª¨ë¸(GBT, SVM, Random Forest)ì„ í›ˆë ¨í•˜ê³  í‰ê°€í•˜ëŠ” ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

Usage:
    python scripts/train_models.py [--config CONFIG_PATH] [--quick] [--no-viz]

Author: Watermelon ML Team
Date: 2025-01-15
"""

import os
import sys
import argparse
import logging
import warnings
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Tuple

import yaml
import pandas as pd
import numpy as np
from sklearn.metrics import mean_absolute_error, r2_score

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from src.training.trainer import MLTrainer, create_trainer_from_config
from src.evaluation.evaluator import ModelEvaluator
from src.evaluation.visualizer import ResultVisualizer
from src.training.trainer import MLTrainer, create_trainer_from_config
from src.models.traditional_ml import ModelFactory

# ê²½ê³  ë©”ì‹œì§€ ì–µì œ
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', category=FutureWarning)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(PROJECT_ROOT / 'experiments' / f'training_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
    ]
)
logger = logging.getLogger(__name__)


def setup_directories() -> None:
    """í•„ìš”í•œ ë””ë ‰í† ë¦¬ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    directories = [
        PROJECT_ROOT / 'experiments',
        PROJECT_ROOT / 'models' / 'saved',
        PROJECT_ROOT / 'experiments' / 'results',
        PROJECT_ROOT / 'experiments' / 'plots'
    ]
    
    for directory in directories:
        directory.mkdir(parents=True, exist_ok=True)
    
    logger.info(f"ë””ë ‰í† ë¦¬ ì„¤ì • ì™„ë£Œ: {len(directories)}ê°œ")


def load_config(config_path: str) -> Dict[str, Any]:
    """ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        logger.info(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {config_path}")
        return config
    except Exception as e:
        logger.error(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise


def load_datasets() -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        base_path = PROJECT_ROOT / 'data' / 'splits' / 'full_dataset'
        
        train_df = pd.read_csv(base_path / 'train.csv')
        val_df = pd.read_csv(base_path / 'val.csv')
        test_df = pd.read_csv(base_path / 'test.csv')
        
        logger.info(f"ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ:")
        logger.info(f"  - í›ˆë ¨: {len(train_df)}ê°œ")
        logger.info(f"  - ê²€ì¦: {len(val_df)}ê°œ") 
        logger.info(f"  - í…ŒìŠ¤íŠ¸: {len(test_df)}ê°œ")
        logger.info(f"  - ë‹¹ë„ ë²”ìœ„: {train_df['sweetness'].min():.1f} ~ {train_df['sweetness'].max():.1f} Brix")
        
        return train_df, val_df, test_df
        
    except Exception as e:
        logger.error(f"ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise


def print_training_summary(config: Dict[str, Any]) -> None:
    """í›ˆë ¨ ì„¤ì • ìš”ì•½ì„ ì¶œë ¥í•©ë‹ˆë‹¤."""
    print("\n" + "="*80)
    print("ğŸ‰ ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
    print("="*80)
    
    print(f"ğŸ“… í›ˆë ¨ ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ¯ ì„±ëŠ¥ ëª©í‘œ:")
    print(f"   - MAE < {config['performance']['target_mae']:.1f} Brix")
    print(f"   - RÂ² > {config['performance']['target_r2']:.2f}")
    
    print(f"ğŸ¤– í›ˆë ¨ ëª¨ë¸:")
    for model_name in config['models'].keys():
        print(f"   - {model_name}")
    
    print(f"âš™ï¸  í›ˆë ¨ ì„¤ì •:")
    print(f"   - êµì°¨ ê²€ì¦: {config['cross_validation']['n_folds']}-fold")
    print(f"   - ìŠ¤ì¼€ì¼ëŸ¬: {config['preprocessing']['scaler_type']}")
    print(f"   - Random State: {config['global']['random_state']}")
    print("="*80 + "\n")


def evaluate_and_visualize(
    trainer: MLTrainer,
    train_df: pd.DataFrame,
    val_df: pd.DataFrame,
    test_df: pd.DataFrame,
    config: Dict[str, Any],
    create_visualizations: bool = True
) -> Dict[str, Any]:
    """ëª¨ë¸ì„ í‰ê°€í•˜ê³  ì‹œê°í™”í•©ë‹ˆë‹¤."""
    
    logger.info("ëª¨ë¸ í‰ê°€ ì‹œì‘...")
    
    # íŠ¹ì§•ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
    feature_cols = [col for col in train_df.columns if col != 'sweetness']
    
    X_train = train_df[feature_cols].values
    y_train = train_df['sweetness'].values
    X_val = val_df[feature_cols].values
    y_val = val_df['sweetness'].values
    X_test = test_df[feature_cols].values
    y_test = test_df['sweetness'].values
    
    # í‰ê°€ì ì´ˆê¸°í™”
    evaluator = ModelEvaluator()
    all_results = {}
    
    # ê° ëª¨ë¸ í‰ê°€
    for model_name, model in trainer.models.items():
        # ëª¨ë¸ì´ í›ˆë ¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
        if not hasattr(model, 'is_fitted') or not model.is_fitted:
            logger.warning(f"  {model_name} ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
            continue
            
        logger.info(f"  {model_name} í‰ê°€ ì¤‘...")
        
        try:
            # ì˜ˆì¸¡
            y_train_pred = model.predict(X_train)
            y_val_pred = model.predict(X_val)
            y_test_pred = model.predict(X_test)
        except Exception as e:
            logger.error(f"  {model_name} ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            continue
        
        # í›ˆë ¨ ì„¸íŠ¸ í‰ê°€
        train_results = evaluator.evaluate_model_performance(
            y_train, y_train_pred, model_name, "í›ˆë ¨"
        )
        
        # ê²€ì¦ ì„¸íŠ¸ í‰ê°€
        val_results = evaluator.evaluate_model_performance(
            y_val, y_val_pred, model_name, "ê²€ì¦"
        )
        
        # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€
        test_results = evaluator.evaluate_model_performance(
            y_test, y_test_pred, model_name, "í…ŒìŠ¤íŠ¸"
        )
        
        # ê²°ê³¼ ì €ì¥
        all_results[model_name] = {
            'train': train_results,
            'val': val_results,
            'test': test_results,
            'predictions': {
                'train': y_train_pred,
                'val': y_val_pred,
                'test': y_test_pred
            }
        }
    
    # ì‹œê°í™” ìƒì„±
    if create_visualizations:
        logger.info("ì‹œê°í™” ìƒì„± ì¤‘...")
        visualizer = ResultVisualizer()
        
        # ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸
        performance_data = []
        for model_name, results in all_results.items():
            for dataset_type in ['train', 'val', 'test']:
                result = results[dataset_type]
                metrics = result['metrics'] if isinstance(result, dict) else result
                performance_data.append({
                    'Model': model_name,
                    'Dataset': dataset_type,
                    'MAE': metrics.get('mae', 0) if isinstance(metrics, dict) else getattr(metrics, 'mae', 0),
                    'R2': metrics.get('r2', 0) if isinstance(metrics, dict) else getattr(metrics, 'r2', 0),
                    'RMSE': metrics.get('rmse', 0) if isinstance(metrics, dict) else getattr(metrics, 'rmse', 0)
                })
        
        performance_df = pd.DataFrame(performance_data)
        
        # ì„±ëŠ¥ ë°ì´í„°ë¥¼ CSVë¡œ ì €ì¥ (ì‹œê°í™” ëŒ€ì‹ )
        performance_df.to_csv(PROJECT_ROOT / 'experiments' / 'plots' / 'model_performance_comparison.csv', index=False)
        logger.info("ì„±ëŠ¥ ë¹„êµ ë°ì´í„°ë¥¼ CSVë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        
        # ê° ëª¨ë¸ë³„ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
        for model_name, results in all_results.items():
            predictions_df = pd.DataFrame({
                'actual': y_test,
                'predicted': results['predictions']['test'],
                'residuals': y_test - results['predictions']['test']
            })
            predictions_df.to_csv(
                PROJECT_ROOT / 'experiments' / 'plots' / f'{model_name}_predictions.csv', 
                index=False
            )
        
        # íŠ¹ì§• ì¤‘ìš”ë„ ì €ì¥ (Random Forestì™€ GBTë§Œ)
        for model_name, model in trainer.models.items():
            if hasattr(model.model, 'feature_importances_') and model.is_fitted:
                try:
                    importance_dict = model.get_feature_importance()
                    if importance_dict and isinstance(importance_dict, dict):
                        # íŠ¹ì§• ì¤‘ìš”ë„ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
                        importance_df = pd.DataFrame([
                            {'feature': feature, 'importance': importance}
                            for feature, importance in importance_dict.items()
                        ]).sort_values('importance', ascending=False)
                        
                        importance_df.to_csv(
                            PROJECT_ROOT / 'experiments' / 'plots' / f'{model_name}_feature_importance.csv',
                            index=False
                        )
                        logger.info(f"{model_name} íŠ¹ì§• ì¤‘ìš”ë„ë¥¼ CSVë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
                except Exception as e:
                    logger.warning(f"{model_name} íŠ¹ì§• ì¤‘ìš”ë„ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        logger.info(f"ì‹œê°í™” ì™„ë£Œ: experiments/plots/ ë””ë ‰í† ë¦¬")
    
    return all_results


def print_results_summary(all_results: Dict[str, Any], config: Dict[str, Any]) -> str:
    """ê²°ê³¼ ìš”ì•½ì„ ì¶œë ¥í•˜ê³  ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    
    print("\n" + "="*80)
    print("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½")
    print("="*80)
    
    # í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ìš”ì•½ í…Œì´ë¸”
    print(f"{'ëª¨ë¸':<20} {'MAE':<8} {'RÂ²':<8} {'RMSE':<8} {'ëª©í‘œë‹¬ì„±':<10}")
    print("-" * 70)
    
    best_model = None
    best_mae = float('inf')
    target_mae = config['performance']['target_mae']
    target_r2 = config['performance']['target_r2']
    
    for model_name, results in all_results.items():
        test_result = results['test']
        metrics = test_result['metrics'] if isinstance(test_result, dict) else test_result
        mae = metrics.get('mae', 0) if isinstance(metrics, dict) else getattr(metrics, 'mae', 0)
        r2 = metrics.get('r2', 0) if isinstance(metrics, dict) else getattr(metrics, 'r2', 0)
        rmse = metrics.get('rmse', 0) if isinstance(metrics, dict) else getattr(metrics, 'rmse', 0)
        
        # ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
        mae_ok = "âœ…" if mae < target_mae else "âŒ"
        r2_ok = "âœ…" if r2 > target_r2 else "âŒ"
        goal_status = f"{mae_ok} {r2_ok}"
        
        print(f"{model_name:<20} {mae:<8.3f} {r2:<8.3f} {rmse:<8.3f} {goal_status:<10}")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸° (MAE ê¸°ì¤€)
        if mae < best_mae:
            best_mae = mae
            best_model = model_name
    
    print("-" * 70)
    print(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model} (MAE: {best_mae:.3f})")
    
    # ëª©í‘œ ë‹¬ì„± ìš”ì•½
    print(f"\nğŸ¯ ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„± í˜„í™©:")
    models_meeting_mae = sum(1 for results in all_results.values() 
                           if (results['test']['metrics'].get('mae', float('inf')) if isinstance(results['test'], dict) 
                               else getattr(results['test'], 'mae', float('inf'))) < target_mae)
    models_meeting_r2 = sum(1 for results in all_results.values() 
                          if (results['test']['metrics'].get('r2', 0) if isinstance(results['test'], dict) 
                              else getattr(results['test'], 'r2', 0)) > target_r2)
    total_models = len(all_results)
    
    print(f"   - MAE < {target_mae}: {models_meeting_mae}/{total_models} ëª¨ë¸")
    print(f"   - RÂ² > {target_r2}: {models_meeting_r2}/{total_models} ëª¨ë¸")
    
    if best_mae < target_mae:
        print(f"   âœ… ì£¼ìš” ëª©í‘œ ë‹¬ì„±! (MAE < {target_mae})")
    else:
        print(f"   âŒ ì£¼ìš” ëª©í‘œ ë¯¸ë‹¬ì„± (MAE >= {target_mae})")
    
    print("="*80 + "\n")
    
    return best_model


def save_best_model(trainer: MLTrainer, best_model_name: str) -> None:
    """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤."""
    
    try:
        best_model = trainer.models[best_model_name]
        
        # ëª¨ë¸ ì €ì¥
        model_path = PROJECT_ROOT / 'models' / 'saved' / 'best_model.pkl'
        best_model.save_model(str(model_path))
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ (ìˆëŠ” ê²½ìš°)
        if hasattr(best_model, 'scaler') and best_model.scaler is not None:
            scaler_path = PROJECT_ROOT / 'models' / 'saved' / 'scaler.pkl'
            import joblib
            joblib.dump(best_model.scaler, scaler_path)
            logger.info(f"ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥: {scaler_path}")
        
        # ëª¨ë¸ ì„¤ì • ì €ì¥
        config_path = PROJECT_ROOT / 'models' / 'saved' / 'model_config.yaml'
        model_config = {
            'best_model': best_model_name,
            'model_type': type(best_model).__name__,
            'training_date': datetime.now().isoformat(),
            'feature_count': len(best_model.feature_names_) if hasattr(best_model, 'feature_names_') else 51,
            'performance': {
                'test_mae': float(trainer.latest_results[best_model_name]['test'].mae),
                'test_r2': float(trainer.latest_results[best_model_name]['test'].r2)
            }
        }
        
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(model_config, f, default_flow_style=False, allow_unicode=True)
        
        logger.info(f"ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ ì™„ë£Œ:")
        logger.info(f"  - ëª¨ë¸: {model_path}")
        logger.info(f"  - ì„¤ì •: {config_path}")
        
    except Exception as e:
        logger.error(f"ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")


def save_training_results(
    all_results: Dict[str, Any],
    training_summary: Dict[str, Any],
    config: Dict[str, Any]
) -> None:
    """í›ˆë ¨ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    try:
        # ì„±ëŠ¥ ìš”ì•½ CSV
        performance_data = []
        for model_name, results in all_results.items():
            for dataset_type in ['train', 'val', 'test']:
                result = results[dataset_type]
                metrics = result['metrics'] if isinstance(result, dict) else result
                performance_data.append({
                    'model': model_name,
                    'dataset': dataset_type,
                    'mae': metrics.get('mae', 0) if isinstance(metrics, dict) else getattr(metrics, 'mae', 0),
                    'mse': metrics.get('mse', 0) if isinstance(metrics, dict) else getattr(metrics, 'mse', 0),
                    'rmse': metrics.get('rmse', 0) if isinstance(metrics, dict) else getattr(metrics, 'rmse', 0),
                    'r2': metrics.get('r2', 0) if isinstance(metrics, dict) else getattr(metrics, 'r2', 0),
                    'mape': metrics.get('mape', 0) if isinstance(metrics, dict) else getattr(metrics, 'mape', 0),
                    'accuracy_0_5': metrics.get('accuracy_0_5', 0) if isinstance(metrics, dict) else 0,
                    'accuracy_1_0': metrics.get('accuracy_1_0', 0) if isinstance(metrics, dict) else 0,
                    'performance_grade': result.get('performance_grade', 'N/A') if isinstance(result, dict) else 'N/A'
                })
        
        performance_df = pd.DataFrame(performance_data)
        performance_path = PROJECT_ROOT / 'experiments' / 'results' / f'performance_summary_{timestamp}.csv'
        performance_df.to_csv(performance_path, index=False)
        
        # ìƒì„¸ ê²°ê³¼ YAML
        detailed_results = {
            'experiment_info': {
                'timestamp': timestamp,
                'config_used': config,
                'training_summary': training_summary
            },
            'model_results': {}
        }
        
        for model_name, results in all_results.items():
            detailed_results['model_results'][model_name] = {
                'train': results['train'] if isinstance(results['train'], dict) else results['train'].__dict__,
                'val': results['val'] if isinstance(results['val'], dict) else results['val'].__dict__,
                'test': results['test'] if isinstance(results['test'], dict) else results['test'].__dict__
            }
        
        detailed_path = PROJECT_ROOT / 'experiments' / 'results' / f'detailed_results_{timestamp}.yaml'
        with open(detailed_path, 'w', encoding='utf-8') as f:
            yaml.dump(detailed_results, f, default_flow_style=False, allow_unicode=True)
        
        logger.info(f"í›ˆë ¨ ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
        logger.info(f"  - ì„±ëŠ¥ ìš”ì•½: {performance_path}")
        logger.info(f"  - ìƒì„¸ ê²°ê³¼: {detailed_path}")
        
    except Exception as e:
        logger.error(f"ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±
    parser = argparse.ArgumentParser(description='ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ ëª¨ë¸ í›ˆë ¨')
    parser.add_argument('--config', default='configs/models.yaml', help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--quick', action='store_true', help='ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì‘ì€ í•˜ì´í¼íŒŒë¼ë¯¸í„°)')
    parser.add_argument('--no-viz', action='store_true', help='ì‹œê°í™” ê±´ë„ˆë›°ê¸°')
    
    args = parser.parse_args()
    
    try:
        # ì´ˆê¸° ì„¤ì •
        setup_directories()
        
        # ì„¤ì • ë¡œë“œ
        config_path = PROJECT_ROOT / args.config
        config = load_config(config_path)
        
        # ë¹ ë¥¸ ëª¨ë“œ ì„¤ì • ì¡°ì •
        if args.quick:
            logger.info("ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ í™œì„±í™”")
            if 'gradient_boosting' in config['models']:
                config['models']['gradient_boosting']['n_estimators'] = 50
            if 'random_forest' in config['models']:
                config['models']['random_forest']['n_estimators'] = 50
            config['cross_validation']['n_folds'] = 3
        
        # ë°ì´í„° ë¡œë“œ
        train_df, val_df, test_df = load_datasets()
        
        # í›ˆë ¨ ìš”ì•½ ì¶œë ¥
        print_training_summary(config)
        
        # íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° í›ˆë ¨
        logger.info("ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
        trainer = create_trainer_from_config(str(config_path))
        
        # íŠ¹ì§•ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
        feature_cols = [col for col in train_df.columns if col != 'sweetness']
        X_train = train_df[feature_cols].values
        y_train = train_df['sweetness'].values
        X_val = val_df[feature_cols].values  
        y_val = val_df['sweetness'].values
        X_test = test_df[feature_cols].values
        y_test = test_df['sweetness'].values
        
        # ë°ì´í„° ë”•ì…”ë„ˆë¦¬ ìƒì„±
        data = {
            'X_train': X_train,
            'y_train': y_train,
            'X_val': X_val,
            'y_val': y_val,
            'X_test': X_test,
            'y_test': y_test
        }
        
        # í›ˆë ¨ ì‹¤í–‰
        training_results = trainer.train_all_models(data, feature_cols)
        
        logger.info("í›ˆë ¨ ì™„ë£Œ! í‰ê°€ ì‹œì‘...")
        
        # í‰ê°€ ë° ì‹œê°í™”
        all_results = evaluate_and_visualize(
            trainer, train_df, val_df, test_df, config, 
            create_visualizations=not args.no_viz
        )
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        best_model_name = print_results_summary(all_results, config)
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        save_best_model(trainer, best_model_name)
        
        # í›ˆë ¨ ê²°ê³¼ ì €ì¥
        training_summary = training_results.get_summary()
        save_training_results(all_results, training_summary, config)
        
        # === ğŸ”— í˜¸í™˜ì„± ìš”ì•½ íŒŒì¼ ìƒì„± (í›„ì† ìŠ¤í¬ë¦½íŠ¸ ì—°ë™ìš©) ===
        logger.info("í›„ì† ìŠ¤í¬ë¦½íŠ¸ í˜¸í™˜ì„±ì„ ìœ„í•œ ìš”ì•½ íŒŒì¼ ìƒì„± ì¤‘...")
        try:
            # í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ì„ 3_simple_train.py í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            compatible_test_performance = {}
            for model_name, results in all_results.items():
                test_result = results['test']
                metrics = test_result['metrics'] if isinstance(test_result, dict) else test_result
                
                # numpy ê°ì²´ë¥¼ floatë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜
                mae = float(metrics.get('mae', 0)) if hasattr(metrics.get('mae', 0), 'item') else float(metrics.get('mae', 0))
                r2 = float(metrics.get('r2', 0)) if hasattr(metrics.get('r2', 0), 'item') else float(metrics.get('r2', 0))
                mse = float(metrics.get('mse', 0)) if hasattr(metrics.get('mse', 0), 'item') else float(metrics.get('mse', 0))
                
                compatible_test_performance[model_name] = {
                    'mae': mae,
                    'r2': r2,
                    'mse': mse
                }
            
            # 3_simple_train.py í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ì €ì¥
            compatible_summary = {
                'test_performance': compatible_test_performance,
                'timestamp': datetime.now().isoformat(),
                'source': '3_1_train_models.py'  # ì¶œì²˜ ëª…ì‹œ
            }
            
            # models/saved/training_summary_simple.yaml ìƒì„±
            saved_dir = PROJECT_ROOT / "models" / "saved"
            saved_dir.mkdir(parents=True, exist_ok=True)
            
            compatible_file = saved_dir / "training_summary_simple.yaml"
            with open(compatible_file, 'w', encoding='utf-8') as f:
                yaml.dump(compatible_summary, f, default_flow_style=False, allow_unicode=True)
            
            logger.info(f"âœ… í˜¸í™˜ì„± ìš”ì•½ íŒŒì¼ ìƒì„± ì™„ë£Œ: {compatible_file}")
            
        except Exception as e:
            logger.warning(f"í˜¸í™˜ì„± ìš”ì•½ íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}")
        
        # ì™„ë£Œ ë©”ì‹œì§€
        print("\nğŸ‰ ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€ ì™„ë£Œ!")
        print(f"ğŸ“ ê²°ê³¼ íŒŒì¼: experiments/results/")
        print(f"ğŸ“Š ì‹œê°í™”: experiments/plots/")
        print(f"ğŸ† ìµœê³  ëª¨ë¸: models/saved/")
        
        return 0
        
    except Exception as e:
        logger.error(f"í›ˆë ¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    finally:
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        import gc
        gc.collect()
        logger.info("ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")


if __name__ == "__main__":
    sys.exit(main()) 