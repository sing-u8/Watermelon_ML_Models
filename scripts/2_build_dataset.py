#!/usr/bin/env python3
"""
ğŸ‰ ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ ML í”„ë¡œì íŠ¸ - ë°ì´í„°ì…‹ êµ¬ì¶• ìŠ¤í¬ë¦½íŠ¸
ì „ì²´ ìˆ˜ë°• ì˜¤ë””ì˜¤ ë°ì´í„°ì—ì„œ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ê³  ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.
"""

import sys
import os
import logging
import pandas as pd
import numpy as np
from pathlib import Path
import time

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.data.dataset_builder import DatasetBuilder
from src.data.data_splitter import DataSplitter

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def analyze_metadata():
    """ë©”íƒ€ë°ì´í„° ë¶„ì„"""
    logger.info("=== ë©”íƒ€ë°ì´í„° ë¶„ì„ ì‹œì‘ ===")
    
    metadata_path = project_root / 'data' / 'watermelon_metadata.csv'
    if not metadata_path.exists():
        logger.error(f"ë©”íƒ€ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {metadata_path}")
        return None
    
    df = pd.read_csv(metadata_path)
    logger.info(f"ì´ ë°ì´í„° í¬ì¸íŠ¸: {len(df)}ê°œ")
    logger.info(f"ìœ ë‹ˆí¬ ìˆ˜ë°•: {df['watermelon_id'].nunique()}ê°œ")
    logger.info(f"ë‹¹ë„ ë²”ìœ„: {df['sweetness'].min():.1f} ~ {df['sweetness'].max():.1f} Brix")
    logger.info(f"í‰ê·  ë‹¹ë„: {df['sweetness'].mean():.2f} Â± {df['sweetness'].std():.2f} Brix")
    
    # ë‹¹ë„ ë¶„í¬ í™•ì¸
    sweetness_bins = pd.cut(df['sweetness'], bins=5)
    logger.info("ë‹¹ë„ ë¶„í¬:")
    try:
        bin_counts = pd.Series(sweetness_bins).value_counts().sort_index()
        for bin_range, count in bin_counts.items():
            logger.info(f"  {bin_range}: {count}ê°œ")
    except Exception as e:
        logger.warning(f"ë‹¹ë„ ë¶„í¬ ë¶„ì„ ê±´ë„ˆëœ€: {e}")
    
    return df


def build_full_dataset():
    """ì „ì²´ ë°ì´í„°ì…‹ êµ¬ì¶•"""
    logger.info("=== ì „ì²´ ë°ì´í„°ì…‹ êµ¬ì¶• ì‹œì‘ ===")
    
    # ë©”íƒ€ë°ì´í„° ë¶„ì„
    metadata_df = analyze_metadata()
    if metadata_df is None:
        return False
    
    # DatasetBuilder ì´ˆê¸°í™”
    config_path = project_root / 'configs' / 'preprocessing.yaml'
    builder = DatasetBuilder(config_path=config_path)
    
    # ë°ì´í„° ë£¨íŠ¸ ê²½ë¡œ
    data_root = project_root / 'data' / 'raw'
    
    # ë©”íƒ€ë°ì´í„°ì—ì„œ íŒŒì¼ ê²½ë¡œ ì¶”ì¶œ
    file_paths = []
    sweetness_values = []
    
    for _, row in metadata_df.iterrows():
        file_path = project_root / row['file_path']
        if file_path.exists():
            file_paths.append(file_path)
            sweetness_values.append(row['sweetness'])
        else:
            logger.warning(f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
    
    logger.info(f"ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜: {len(file_paths)}ê°œ")
    
    # íŠ¹ì§• ì¶”ì¶œ ì‹¤í–‰
    start_time = time.time()
    
    # ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    metadata_path = project_root / 'data' / 'watermelon_metadata.csv'
    output_dir = project_root / 'data' / 'processed' / 'full_dataset'
    
    build_result = builder.build_dataset(
        metadata_path=metadata_path,
        output_dir=output_dir,
        batch_size=10
    )
    
    end_time = time.time()
    processing_time = end_time - start_time
    
    if build_result and build_result['processed_files'] > 0:
        logger.info(f"íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ!")
        logger.info(f"ì´ ì²˜ë¦¬ ì‹œê°„: {processing_time:.1f}ì´ˆ")
        logger.info(f"í‰ê·  íŒŒì¼ë‹¹ ì²˜ë¦¬ ì‹œê°„: {build_result['avg_processing_time']:.3f}ì´ˆ")
        logger.info(f"ë°ì´í„°ì…‹ í¬ê¸°: {build_result['feature_shape']}")
        logger.info(f"íŠ¹ì§• ê°œìˆ˜: {build_result['feature_shape'][1] - 1}")  # -1 for sweetness column
        
        # í†µê³„ í™•ì¸
        logger.info(f"DatasetBuilder ê²°ê³¼: {build_result}")
        
        return True
    else:
        logger.error("íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨!")
        return False


def split_dataset():
    """ë°ì´í„°ì…‹ ë¶„í• """
    logger.info("=== ë°ì´í„°ì…‹ ë¶„í•  ì‹œì‘ ===")
    
    # êµ¬ì¶•ëœ íŠ¹ì§• ë°ì´í„° ë¡œë“œ
    features_path = project_root / 'data' / 'processed' / 'full_dataset' / 'features.csv'
    if not features_path.exists():
        logger.error(f"íŠ¹ì§• ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {features_path}")
        return False
    
    features_df = pd.read_csv(features_path)
    logger.info(f"íŠ¹ì§• ë°ì´í„° ë¡œë“œ: {features_df.shape}")
    
    # DataSplitter ì´ˆê¸°í™”
    splitter = DataSplitter(train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, random_state=42)
    
    # ë°ì´í„° ë¶„í•  ì‹¤í–‰
    split_data = splitter.split_dataset(
        features_df=features_df,
        target_column='sweetness',
        stratify_bins=5
    )
    
    # ë¶„í• ëœ ë°ì´í„° ì €ì¥
    output_dir = project_root / 'data' / 'splits' / 'full_dataset'
    saved_files = splitter.save_splits(split_data, output_dir)
    
    # ë¶„í•  ê²€ì¦
    validation_result = splitter.validate_split(split_data, target_column='sweetness')
    
    split_result = True
    
    if split_result:
        logger.info("ë°ì´í„° ë¶„í•  ì™„ë£Œ!")
        
        # ë¶„í•  í†µê³„ í™•ì¸
        split_stats = splitter.get_stats()
        logger.info(f"DataSplitter í†µê³„: {split_stats}")
        
        return True
    else:
        logger.error("ë°ì´í„° ë¶„í•  ì‹¤íŒ¨!")
        return False


def verify_dataset():
    """ë°ì´í„°ì…‹ ê²€ì¦"""
    logger.info("=== ë°ì´í„°ì…‹ ê²€ì¦ ì‹œì‘ ===")
    
    splits_dir = project_root / 'data' / 'splits' / 'full_dataset'
    
    split_files = {
        'train': splits_dir / 'train.csv',
        'val': splits_dir / 'val.csv',
        'test': splits_dir / 'test.csv'
    }
    
    total_samples = 0
    for split_name, file_path in split_files.items():
        if file_path.exists():
            split_df = pd.read_csv(file_path)
            logger.info(f"{split_name.upper()} ì„¸íŠ¸: {split_df.shape[0]}ê°œ ìƒ˜í”Œ")
            logger.info(f"  ë‹¹ë„ ë²”ìœ„: {split_df['sweetness'].min():.1f} ~ {split_df['sweetness'].max():.1f}")
            logger.info(f"  í‰ê·  ë‹¹ë„: {split_df['sweetness'].mean():.2f} Â± {split_df['sweetness'].std():.2f}")
            total_samples += split_df.shape[0]
        else:
            logger.warning(f"{split_name} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {file_path}")
    
    logger.info(f"ì´ ìƒ˜í”Œ ìˆ˜: {total_samples}ê°œ")
    
    # íŠ¹ì§• í’ˆì§ˆ í™•ì¸
    features_path = project_root / 'data' / 'processed' / 'full_dataset' / 'features.csv'
    if features_path.exists():
        features_df = pd.read_csv(features_path)
        
        # NaN/Inf ê°’ í™•ì¸
        nan_count = features_df.isnull().sum().sum()
        inf_count = np.isinf(features_df.select_dtypes(include=[np.number])).sum().sum()
        
        logger.info(f"ë°ì´í„° í’ˆì§ˆ í™•ì¸:")
        logger.info(f"  NaN ê°’: {nan_count}ê°œ")
        logger.info(f"  Inf ê°’: {inf_count}ê°œ")
        
        if nan_count == 0 and inf_count == 0:
            logger.info("âœ… ë°ì´í„° í’ˆì§ˆ: ìš°ìˆ˜")
        else:
            logger.warning("âš ï¸ ë°ì´í„° í’ˆì§ˆ: ë¬¸ì œ ë°œê²¬")
    
    return True


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("ğŸ‰ ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ ë°ì´í„°ì…‹ êµ¬ì¶• ì‹œì‘")
    logger.info("=" * 60)
    
    try:
        # 1. ì „ì²´ ë°ì´í„°ì…‹ êµ¬ì¶•
        if not build_full_dataset():
            logger.error("ë°ì´í„°ì…‹ êµ¬ì¶• ì‹¤íŒ¨")
            return False
        
        # 2. ë°ì´í„°ì…‹ ë¶„í• 
        if not split_dataset():
            logger.error("ë°ì´í„°ì…‹ ë¶„í•  ì‹¤íŒ¨")
            return False
        
        # 3. ë°ì´í„°ì…‹ ê²€ì¦
        if not verify_dataset():
            logger.error("ë°ì´í„°ì…‹ ê²€ì¦ ì‹¤íŒ¨")
            return False
        
        logger.info("=" * 60)
        logger.info("ğŸ‰ ë°ì´í„°ì…‹ êµ¬ì¶•ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        logger.info("=" * 60)
        
        # ê²°ê³¼ ìš”ì•½
        logger.info("ğŸ“Š êµ¬ì¶• ê²°ê³¼ ìš”ì•½:")
        logger.info(f"  â€¢ íŠ¹ì§• ë°ì´í„°: data/processed/full_dataset/features.csv")
        logger.info(f"  â€¢ í›ˆë ¨ ì„¸íŠ¸: data/splits/full_dataset/train.csv")
        logger.info(f"  â€¢ ê²€ì¦ ì„¸íŠ¸: data/splits/full_dataset/val.csv")
        logger.info(f"  â€¢ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: data/splits/full_dataset/test.csv")
        
        return True
        
    except Exception as e:
        logger.error(f"ë°ì´í„°ì…‹ êµ¬ì¶• ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 