#!/usr/bin/env python3
"""
íŠ¹ì§• ì„ íƒ ì‹¤í—˜ ìŠ¤í¬ë¦½íŠ¸

í˜„ì¬ 51ê°œ íŠ¹ì§•ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ íŠ¹ì§•ë“¤ì„ ì„ íƒí•˜ì—¬ 
ë” íš¨ìœ¨ì ì´ê³  í•´ì„ ê°€ëŠ¥í•œ ëª¨ë¸ì„ ë§Œë“œëŠ” ì‹¤í—˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

ë‹¤ì–‘í•œ íŠ¹ì§• ì„ íƒ ë°©ë²•ì„ ë¹„êµ:
- Random Forest íŠ¹ì§• ì¤‘ìš”ë„
- Recursive Feature Elimination (RFE)
- SelectKBest (í†µê³„ì  ë°©ë²•)
- Correlation-based selection

ì‘ì„±ì: ML Team
ìƒì„±ì¼: 2025-01-15
"""

import os
import logging
import warnings
import pandas as pd
import numpy as np
import joblib
import yaml
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import gc

from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import (
    RFE, SelectKBest, f_classif, mutual_info_classif
)
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report
from sklearn.model_selection import cross_val_score
import scipy.stats as stats

# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings('ignore')

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


def load_data():
    """ë°ì´í„° ë° íŠ¹ì§• ì´ë¦„ ë¡œë“œ"""
    logger.info("=== ë°ì´í„° ë¡œë“œ ì‹œì‘ ===")
    
    # ë°ì´í„° ë¡œë“œ
    train_df = pd.read_csv("data/splits/full_dataset/train.csv")
    val_df = pd.read_csv("data/splits/full_dataset/val.csv")
    test_df = pd.read_csv("data/splits/full_dataset/test.csv")
    
    # íŠ¹ì§• ì´ë¦„ ë¡œë“œ
    feature_names = list(train_df.columns[:-1])  # pitch_label ì œì™¸
    
    # ë¼ë²¨ ì¸ì½”ë”©
    label_encoder = LabelEncoder()
    
    # ë°ì´í„° ë¶„ë¦¬ ë° ìŠ¤ì¼€ì¼ë§
    X_train = train_df.drop('pitch_label', axis=1).values
    y_train = label_encoder.fit_transform(train_df['pitch_label'].values)
    X_val = val_df.drop('pitch_label', axis=1).values
    y_val = label_encoder.transform(val_df['pitch_label'].values)
    X_test = test_df.drop('pitch_label', axis=1).values
    y_test = label_encoder.transform(test_df['pitch_label'].values)
    
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    X_test_scaled = scaler.transform(X_test)
    
    logger.info(f"ì „ì²´ íŠ¹ì§• ìˆ˜: {len(feature_names)}")
    logger.info(f"ë°ì´í„° í˜•íƒœ - Train: {X_train_scaled.shape}, Val: {X_val_scaled.shape}, Test: {X_test_scaled.shape}")
    logger.info(f"í´ë˜ìŠ¤ ë¶„í¬ - Train: {np.bincount(y_train)}, Val: {np.bincount(y_val)}, Test: {np.bincount(y_test)}")
    
    return {
        'X_train': X_train_scaled,
        'y_train': y_train,
        'X_val': X_val_scaled,
        'y_val': y_val,
        'X_test': X_test_scaled,
        'y_test': y_test,
        'feature_names': feature_names,
        'scaler': scaler,
        'label_encoder': label_encoder
    }


def load_best_model():
    """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ"""
    logger.info("ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    
    # ìµœì‹  íŠœë‹ ê²°ê³¼ ë””ë ‰í† ë¦¬ ì°¾ê¸°
    tuning_dir = "experiments/hyperparameter_tuning"
    if os.path.exists(tuning_dir):
        subdirs = [d for d in os.listdir(tuning_dir) if d.startswith('simple_tuning_')]
        if subdirs:
            latest_dir = sorted(subdirs)[-1]
            model_path = os.path.join(tuning_dir, latest_dir, "random_forest_tuned.pkl")
            
            if os.path.exists(model_path):
                model = joblib.load(model_path)
                logger.info(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
                return model
    
    # ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
    logger.warning("íŠœë‹ëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ Random Forest ì‚¬ìš©")
    return RandomForestClassifier(n_estimators=300, random_state=42)


def random_forest_importance(data, model):
    """Random Forest íŠ¹ì§• ì¤‘ìš”ë„ ë¶„ì„"""
    logger.info("=== Random Forest íŠ¹ì§• ì¤‘ìš”ë„ ë¶„ì„ ===")
    
    # ëª¨ë¸ì´ ì´ë¯¸ í›ˆë ¨ë˜ì–´ ìˆì§€ ì•Šìœ¼ë©´ í›ˆë ¨
    try:
        importance = model.feature_importances_
    except AttributeError:
        logger.info("ëª¨ë¸ í›ˆë ¨ ì¤‘...")
        model.fit(data['X_train'], data['y_train'])
        importance = model.feature_importances_
    
    # ì¤‘ìš”ë„ì™€ íŠ¹ì§• ì´ë¦„ ë§¤í•‘
    feature_importance = pd.DataFrame({
        'feature': data['feature_names'],
        'importance': importance
    }).sort_values('importance', ascending=False)
    
    logger.info("ìƒìœ„ 10ê°œ ì¤‘ìš” íŠ¹ì§•:")
    for i, (_, row) in enumerate(feature_importance.head(10).iterrows(), 1):
        logger.info(f"  {i:2d}. {row['feature']:30s}: {row['importance']:.4f}")
    
    return feature_importance


def rfe_selection(data, model, n_features_list=[10, 20, 30, 40]):
    """Recursive Feature Elimination"""
    logger.info("=== RFE íŠ¹ì§• ì„ íƒ ===")
    
    rfe_results = {}
    
    for n_features in n_features_list:
        logger.info(f"RFEë¡œ {n_features}ê°œ íŠ¹ì§• ì„ íƒ ì¤‘...")
        
        # RFE ìˆ˜í–‰
        rfe = RFE(estimator=model, n_features_to_select=n_features, step=1)
        rfe.fit(data['X_train'], data['y_train'])
        
        # ì„ íƒëœ íŠ¹ì§•
        selected_features = [data['feature_names'][i] for i in range(len(data['feature_names'])) if rfe.support_[i]]
        
        # ì„±ëŠ¥ í‰ê°€
        X_train_selected = rfe.transform(data['X_train'])
        X_test_selected = rfe.transform(data['X_test'])
        
        model_copy = RandomForestClassifier(n_estimators=100, random_state=42)
        model_copy.fit(X_train_selected, data['y_train'])
        
        y_pred = model_copy.predict(X_test_selected)
        accuracy = accuracy_score(data['y_test'], y_pred)
        f1 = f1_score(data['y_test'], y_pred, average='weighted')
        
        rfe_results[n_features] = {
            'selected_features': selected_features,
            'accuracy': accuracy,
            'f1_score': f1,
            'rfe_ranking': rfe.ranking_
        }
        
        logger.info(f"  {n_features}ê°œ íŠ¹ì§• - ì •í™•ë„: {accuracy:.4f}, F1-score: {f1:.4f}")
    
    return rfe_results


def statistical_selection(data, k_list=[10, 20, 30, 40]):
    """í†µê³„ì  ë°©ë²•ì„ ì´ìš©í•œ íŠ¹ì§• ì„ íƒ"""
    logger.info("=== í†µê³„ì  íŠ¹ì§• ì„ íƒ ===")
    
    statistical_results = {}
    
    # F-classification ì ìˆ˜ ê³„ì‚°
    f_scores, f_pvalues = f_classif(data['X_train'], data['y_train'])
    
    # Mutual information ì ìˆ˜ ê³„ì‚°
    mi_scores = mutual_info_classif(data['X_train'], data['y_train'], random_state=42)
    
    for k in k_list:
        logger.info(f"ìƒìœ„ {k}ê°œ í†µê³„ì  íŠ¹ì§• ì„ íƒ ì¤‘...")
        
        # F-classification ê¸°ë°˜ ì„ íƒ
        selector_f = SelectKBest(score_func=f_classif, k=k)
        X_train_f = selector_f.fit_transform(data['X_train'], data['y_train'])
        X_test_f = selector_f.transform(data['X_test'])
        
        selected_features_f = [data['feature_names'][i] for i in selector_f.get_support(indices=True)]
        
        # ì„±ëŠ¥ í‰ê°€
        model = RandomForestClassifier(n_estimators=100, random_state=42)
        model.fit(X_train_f, data['y_train'])
        y_pred_f = model.predict(X_test_f)
        
        accuracy_f = accuracy_score(data['y_test'], y_pred_f)
        f1_f = f1_score(data['y_test'], y_pred_f, average='weighted')
        
        statistical_results[k] = {
            'f_classification': {
                'selected_features': selected_features_f,
                'accuracy': accuracy_f,
                'f1_score': f1_f,
                'scores': f_scores[selector_f.get_support()]
            }
        }
        
        logger.info(f"  F-classification {k}ê°œ - ì •í™•ë„: {accuracy_f:.4f}, F1-score: {f1_f:.4f}")
    
    return statistical_results, {'f_scores': f_scores, 'f_pvalues': f_pvalues, 'mi_scores': mi_scores}


def correlation_analysis(data):
    """ìƒê´€ê´€ê³„ ë¶„ì„"""
    logger.info("=== ìƒê´€ê´€ê³„ ë¶„ì„ ===")
    
    # ë°ì´í„°í”„ë ˆì„ ìƒì„±
    df = pd.DataFrame(data['X_train'], columns=data['feature_names'])
    df['pitch_label'] = data['y_train']
    
    # íƒ€ê²Ÿê³¼ì˜ ìƒê´€ê´€ê³„
    target_corr = df.corr()['pitch_label'].drop('pitch_label').abs().sort_values(ascending=False)
    
    logger.info("íƒ€ê²Ÿê³¼ ìƒê´€ê´€ê³„ ë†’ì€ ìƒìœ„ 10ê°œ íŠ¹ì§•:")
    for i, (feature, corr) in enumerate(target_corr.head(10).items(), 1):
        logger.info(f"  {i:2d}. {feature:30s}: {corr:.4f}")
    
    # íŠ¹ì§•ê°„ ìƒê´€ê´€ê³„ (ë†’ì€ ìƒê´€ê´€ê³„ íŠ¹ì§• íƒì§€)
    feature_corr = df.drop('pitch_label', axis=1).corr()
    
    # ìƒê´€ê´€ê³„ 0.9 ì´ìƒì¸ íŠ¹ì§• ìŒ ì°¾ê¸°
    high_corr_pairs = []
    for i in range(len(feature_corr.columns)):
        for j in range(i+1, len(feature_corr.columns)):
            corr_val = abs(feature_corr.iloc[i, j])
            if corr_val > 0.9:
                high_corr_pairs.append({
                    'feature1': feature_corr.columns[i],
                    'feature2': feature_corr.columns[j],
                    'correlation': corr_val
                })
    
    logger.info(f"ë†’ì€ ìƒê´€ê´€ê³„ (>0.9) íŠ¹ì§• ìŒ: {len(high_corr_pairs)}ê°œ")
    for pair in high_corr_pairs[:5]:  # ìƒìœ„ 5ê°œë§Œ ì¶œë ¥
        logger.info(f"  {pair['feature1']} - {pair['feature2']}: {pair['correlation']:.4f}")
    
    return target_corr, high_corr_pairs


def progressive_feature_selection(data, model, max_features=30):
    """ì ì§„ì  íŠ¹ì§• ì„ íƒ (Forward Selection)"""
    logger.info(f"=== ì ì§„ì  íŠ¹ì§• ì„ íƒ (ìµœëŒ€ {max_features}ê°œ) ===")
    
    selected_features = []
    remaining_features = list(range(len(data['feature_names'])))
    performance_history = []
    
    for step in range(min(max_features, len(data['feature_names']))):
        best_accuracy = 0.0
        best_feature = None
        
        # ê° ë‚¨ì€ íŠ¹ì§•ì— ëŒ€í•´ ì„±ëŠ¥ í‰ê°€
        for feature_idx in remaining_features:
            current_features = selected_features + [feature_idx]
            
            X_train_subset = data['X_train'][:, current_features]
            X_test_subset = data['X_test'][:, current_features]
            
            # ë¹ ë¥¸ í‰ê°€ë¥¼ ìœ„í•´ ì‘ì€ ëª¨ë¸ ì‚¬ìš©
            temp_model = RandomForestClassifier(n_estimators=50, random_state=42)
            temp_model.fit(X_train_subset, data['y_train'])
            y_pred = temp_model.predict(X_test_subset)
            accuracy = accuracy_score(data['y_test'], y_pred)
            
            if accuracy > best_accuracy:
                best_accuracy = accuracy
                best_feature = feature_idx
        
        # ìµœê³  ì„±ëŠ¥ íŠ¹ì§• ì¶”ê°€
        if best_feature is not None:
            selected_features.append(best_feature)
            remaining_features.remove(best_feature)
            
            # ì„±ëŠ¥ ê¸°ë¡
            feature_name = data['feature_names'][best_feature]
            f1 = f1_score(data['y_test'], temp_model.predict(data['X_test'][:, selected_features]), average='weighted')
            
            performance_history.append({
                'step': step + 1,
                'feature_added': feature_name,
                'accuracy': best_accuracy,
                'f1_score': f1,
                'feature_count': len(selected_features)
            })
            
            logger.info(f"  Step {step+1:2d}: ì¶”ê°€ëœ íŠ¹ì§• '{feature_name}' - ì •í™•ë„: {best_accuracy:.4f}, F1-score: {f1:.4f}")
        
        # ì„±ëŠ¥ì´ ë” ì´ìƒ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ ì¡°ê¸° ì¤‘ë‹¨
        if len(performance_history) >= 3:
            recent_accuracies = [p['accuracy'] for p in performance_history[-3:]]
            if all(acc <= recent_accuracies[0] + 0.001 for acc in recent_accuracies[1:]):
                logger.info(f"  ì„±ëŠ¥ ê°œì„ ì´ ë¯¸ë¯¸í•˜ì—¬ Step {step+1}ì—ì„œ ì¡°ê¸° ì¤‘ë‹¨")
                break
    
    selected_feature_names = [data['feature_names'][i] for i in selected_features]
    
    return selected_feature_names, performance_history


def evaluate_feature_sets(data, feature_sets):
    """ë‹¤ì–‘í•œ íŠ¹ì§• ì„¸íŠ¸ ì„±ëŠ¥ ë¹„êµ"""
    logger.info("=== íŠ¹ì§• ì„¸íŠ¸ ì„±ëŠ¥ ë¹„êµ ===")
    
    results = {}
    
    for set_name, features in feature_sets.items():
        logger.info(f"í‰ê°€ ì¤‘: {set_name} ({len(features)}ê°œ íŠ¹ì§•)")
        
        # íŠ¹ì§• ì¸ë±ìŠ¤ ì°¾ê¸°
        feature_indices = [data['feature_names'].index(f) for f in features if f in data['feature_names']]
        
        if not feature_indices:
            logger.warning(f"  {set_name}: ìœ íš¨í•œ íŠ¹ì§•ì´ ì—†ìŠµë‹ˆë‹¤.")
            continue
        
        # ë°ì´í„° subset ìƒì„±
        X_train_subset = data['X_train'][:, feature_indices]
        X_val_subset = data['X_val'][:, feature_indices]
        X_test_subset = data['X_test'][:, feature_indices]
        
        # ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€
        model = RandomForestClassifier(n_estimators=200, random_state=42)
        model.fit(X_train_subset, data['y_train'])
        
        # ê²€ì¦ ì„¸íŠ¸ ì„±ëŠ¥
        y_pred_val = model.predict(X_val_subset)
        val_accuracy = accuracy_score(data['y_val'], y_pred_val)
        val_f1 = f1_score(data['y_val'], y_pred_val, average='weighted')
        
        # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì„±ëŠ¥
        y_pred_test = model.predict(X_test_subset)
        test_accuracy = accuracy_score(data['y_test'], y_pred_test)
        test_f1 = f1_score(data['y_test'], y_pred_test, average='weighted')
        
        # êµì°¨ ê²€ì¦
        cv_scores = cross_val_score(model, X_train_subset, data['y_train'], 
                                   cv=5, scoring='accuracy')
        cv_accuracy = cv_scores.mean()
        cv_std = cv_scores.std()
        
        results[set_name] = {
            'feature_count': len(feature_indices),
            'features': features,
            'val_accuracy': val_accuracy,
            'val_f1_score': val_f1,
            'test_accuracy': test_accuracy,
            'test_f1_score': test_f1,
            'cv_accuracy': cv_accuracy,
            'cv_std': cv_std
        }
        
        logger.info(f"  í…ŒìŠ¤íŠ¸ - ì •í™•ë„: {test_accuracy:.4f}, F1-score: {test_f1:.4f}")
        logger.info(f"  CV - ì •í™•ë„: {cv_accuracy:.4f} Â± {cv_std:.4f}")
    
    return results


def create_visualizations(importance_df, target_corr, performance_history, results_dir):
    """ì‹œê°í™” ìƒì„±"""
    logger.info("ì‹œê°í™” ìƒì„± ì¤‘...")
    
    plt.style.use('default')
    
    # 1. íŠ¹ì§• ì¤‘ìš”ë„ í”Œë¡¯
    plt.figure(figsize=(12, 8))
    top_features = importance_df.head(20)
    sns.barplot(data=top_features, y='feature', x='importance', palette='viridis')
    plt.title('ìƒìœ„ 20ê°œ íŠ¹ì§• ì¤‘ìš”ë„ (Random Forest)', fontsize=14, fontweight='bold')
    plt.xlabel('Feature Importance', fontsize=12)
    plt.ylabel('Features', fontsize=12)
    plt.tight_layout()
    plt.savefig(os.path.join(results_dir, 'feature_importance.png'), dpi=300, bbox_inches='tight')
    plt.close()
    
    # 2. íƒ€ê²Ÿ ìƒê´€ê´€ê³„ í”Œë¡¯
    plt.figure(figsize=(12, 8))
    top_corr = target_corr.head(20)
    sns.barplot(x=top_corr.values, y=top_corr.index, palette='coolwarm')
    plt.title('ìƒìœ„ 20ê°œ íŠ¹ì§•ê³¼ ìŒ ë†’ë‚®ì´ì˜ ìƒê´€ê´€ê³„', fontsize=14, fontweight='bold')
    plt.xlabel('Absolute Correlation with Pitch Label', fontsize=12)
    plt.ylabel('Features', fontsize=12)
    plt.tight_layout()
    plt.savefig(os.path.join(results_dir, 'target_correlation.png'), dpi=300, bbox_inches='tight')
    plt.close()
    
    # 3. ì ì§„ì  íŠ¹ì§• ì„ íƒ ì„±ëŠ¥ ì¶”ì´
    if performance_history:
        plt.figure(figsize=(12, 6))
        steps = [p['step'] for p in performance_history]
        accuracies = [p['accuracy'] for p in performance_history]
        
        plt.plot(steps, accuracies, 'o-', linewidth=2, markersize=6)
        plt.title('ì ì§„ì  íŠ¹ì§• ì„ íƒ - ì„±ëŠ¥ ì¶”ì´', fontsize=14, fontweight='bold')
        plt.xlabel('Feature Count', fontsize=12)
        plt.ylabel('Test Accuracy', fontsize=12)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(os.path.join(results_dir, 'progressive_selection.png'), dpi=300, bbox_inches='tight')
        plt.close()
    
    logger.info(f"ì‹œê°í™” ì €ì¥ ì™„ë£Œ: {results_dir}")


def save_results(all_results, data, results_dir):
    """ê²°ê³¼ ì €ì¥"""
    logger.info(f"ê²°ê³¼ ì €ì¥ ì¤‘: {results_dir}")
    
    # ì „ì²´ ê²°ê³¼ ì €ì¥
    results_path = os.path.join(results_dir, "feature_selection_results.yaml")
    with open(results_path, 'w', encoding='utf-8') as f:
        yaml.dump(all_results, f, default_flow_style=False, allow_unicode=True)
    
    # ì¶”ì²œ íŠ¹ì§• ì„¸íŠ¸ ì €ì¥
    recommendations = {
        'best_overall': None,
        'best_small': None,
        'best_medium': None,
        'feature_rankings': {}
    }
    
    # ì„±ëŠ¥ ê¸°ì¤€ìœ¼ë¡œ ìµœê³  íŠ¹ì§• ì„¸íŠ¸ ì°¾ê¸°
    if 'feature_set_comparison' in all_results:
        comparison = all_results['feature_set_comparison']
        
        # ì „ì²´ ìµœê³  ì„±ëŠ¥
        best_overall = max(comparison.items(), key=lambda x: x[1]['test_accuracy'])
        recommendations['best_overall'] = {
            'name': best_overall[0],
            'features': best_overall[1]['features'],
            'test_accuracy': best_overall[1]['test_accuracy'],
            'test_f1_score': best_overall[1]['test_f1_score']
        }
        
        # í¬ê¸°ë³„ ìµœê³  ì„±ëŠ¥
        small_sets = {k: v for k, v in comparison.items() if v['feature_count'] <= 15}
        medium_sets = {k: v for k, v in comparison.items() if 15 < v['feature_count'] <= 30}
        
        if small_sets:
            best_small = max(small_sets.items(), key=lambda x: x[1]['test_accuracy'])
            recommendations['best_small'] = {
                'name': best_small[0],
                'features': best_small[1]['features'],
                'test_accuracy': best_small[1]['test_accuracy'],
                'test_f1_score': best_small[1]['test_f1_score']
            }
        
        if medium_sets:
            best_medium = max(medium_sets.items(), key=lambda x: x[1]['test_accuracy'])
            recommendations['best_medium'] = {
                'name': best_medium[0],
                'features': best_medium[1]['features'],
                'test_accuracy': best_medium[1]['test_accuracy'],
                'test_f1_score': best_medium[1]['test_f1_score']
            }
    
    recommendations_path = os.path.join(results_dir, "feature_recommendations.yaml")
    with open(recommendations_path, 'w', encoding='utf-8') as f:
        yaml.dump(recommendations, f, default_flow_style=False, allow_unicode=True)
    
    logger.info("ê²°ê³¼ ì €ì¥ ì™„ë£Œ")
    return recommendations


def generate_report(all_results, recommendations, results_dir):
    """ì¢…í•© ë³´ê³ ì„œ ìƒì„±"""
    best_overall = recommendations.get('best_overall')
    best_small = recommendations.get('best_small')
    best_medium = recommendations.get('best_medium')
    
    report = f"""# íŠ¹ì§• ì„ íƒ ì‹¤í—˜ ê²°ê³¼ ë³´ê³ ì„œ

ìƒì„±ì¼: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ì‹¤í—˜ ê°œìš”

- **ëª©ì **: ìˆ˜ë°• ìŒ ë†’ë‚®ì´ ë¶„ë¥˜ë¥¼ ìœ„í•œ ìµœì  íŠ¹ì§• subset íƒìƒ‰
- **ì›ë³¸ íŠ¹ì§• ìˆ˜**: 51ê°œ
- **ì‹¤í—˜ ë°©ë²•**: Random Forest ì¤‘ìš”ë„, RFE, í†µê³„ì  ì„ íƒ, ì ì§„ì  ì„ íƒ
- **í‰ê°€ ì§€í‘œ**: ì •í™•ë„ (Accuracy), F1-score

## ì£¼ìš” ë°œê²¬ì‚¬í•­

### ğŸ† ìµœê³  ì„±ëŠ¥ íŠ¹ì§• ì„¸íŠ¸
"""
    
    if best_overall:
        report += f"""
**ì„¸íŠ¸ëª…**: {best_overall['name']}
- **íŠ¹ì§• ìˆ˜**: {len(best_overall['features'])}ê°œ
- **í…ŒìŠ¤íŠ¸ ì •í™•ë„**: {best_overall['test_accuracy']:.4f}
- **í…ŒìŠ¤íŠ¸ F1-score**: {best_overall['test_f1_score']:.4f}

**ì„ íƒëœ íŠ¹ì§•ë“¤**:
{', '.join(best_overall['features'][:10])}{'...' if len(best_overall['features']) > 10 else ''}
"""
    
    if best_small:
        report += f"""
### ğŸ’¡ ì†Œí˜• íŠ¹ì§• ì„¸íŠ¸ (â‰¤15ê°œ)
**ì„¸íŠ¸ëª…**: {best_small['name']}
- **íŠ¹ì§• ìˆ˜**: {len(best_small['features'])}ê°œ  
- **í…ŒìŠ¤íŠ¸ ì •í™•ë„**: {best_small['test_accuracy']:.4f}
- **í…ŒìŠ¤íŠ¸ F1-score**: {best_small['test_f1_score']:.4f}
"""
    
    if best_medium:
        report += f"""
### ğŸ¯ ì¤‘í˜• íŠ¹ì§• ì„¸íŠ¸ (16-30ê°œ)
**ì„¸íŠ¸ëª…**: {best_medium['name']}
- **íŠ¹ì§• ìˆ˜**: {len(best_medium['features'])}ê°œ
- **í…ŒìŠ¤íŠ¸ ì •í™•ë„**: {best_medium['test_accuracy']:.4f}  
- **í…ŒìŠ¤íŠ¸ F1-score**: {best_medium['test_f1_score']:.4f}
"""
    
    # ì „ì²´ ê²°ê³¼ ìš”ì•½
    if 'feature_set_comparison' in all_results:
        report += "\n## ëª¨ë“  íŠ¹ì§• ì„¸íŠ¸ ì„±ëŠ¥ ë¹„êµ\n\n"
        comparison = all_results['feature_set_comparison']
        
        for name, metrics in sorted(comparison.items(), key=lambda x: x[1]['test_accuracy'], reverse=True):
            report += f"""### {name}
- **íŠ¹ì§• ìˆ˜**: {metrics['feature_count']}ê°œ
- **í…ŒìŠ¤íŠ¸ ì •í™•ë„**: {metrics['test_accuracy']:.4f}
- **í…ŒìŠ¤íŠ¸ F1-score**: {metrics['test_f1_score']:.4f}
- **CV ì •í™•ë„**: {metrics['cv_accuracy']:.4f} Â± {metrics['cv_std']:.4f}

"""
    
    # ê²°ë¡ 
    full_performance = all_results.get('feature_set_comparison', {}).get('all_features_baseline')
    if full_performance and best_overall:
        original_accuracy = full_performance['test_accuracy']
        best_accuracy = best_overall['test_accuracy']
        improvement = ((best_accuracy - original_accuracy) / original_accuracy) * 100
        
        report += f"""
## ê²°ë¡ 

íŠ¹ì§• ì„ íƒì„ í†µí•´ **{len(best_overall['features'])}ê°œ íŠ¹ì§•**ìœ¼ë¡œ 
ì›ë³¸ 51ê°œ íŠ¹ì§• ëŒ€ë¹„ {'ê°œì„ ëœ' if improvement > 0 else 'ìœ ì‚¬í•œ'} ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

- **ì›ë³¸ ì„±ëŠ¥**: ì •í™•ë„ {original_accuracy:.4f}
- **ìµœì  ì„±ëŠ¥**: ì •í™•ë„ {best_accuracy:.4f}
- **ì„±ëŠ¥ ë³€í™”**: {improvement:+.2f}%
- **íŠ¹ì§• ê°ì†Œ**: {51 - len(best_overall['features'])}ê°œ ({((51 - len(best_overall['features']))/51)*100:.1f}%)

### ê¶Œì¥ì‚¬í•­

1. **í”„ë¡œë•ì…˜ ë°°í¬**: {best_small['name'] if best_small else best_overall['name']} ì‚¬ìš© ê¶Œì¥
2. **í•´ì„ ê°€ëŠ¥ì„±**: ì ì€ ìˆ˜ì˜ íŠ¹ì§•ìœ¼ë¡œ ë†’ì€ ì„±ëŠ¥ ë‹¬ì„± 
3. **ê³„ì‚° íš¨ìœ¨ì„±**: íŠ¹ì§• ìˆ˜ ê°ì†Œë¡œ ì¶”ë¡  ì†ë„ í–¥ìƒ ê¸°ëŒ€
"""
    
    # ë³´ê³ ì„œ ì €ì¥
    report_path = os.path.join(results_dir, "FEATURE_SELECTION_REPORT.md")
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    logger.info(f"ë³´ê³ ì„œ ì €ì¥: {report_path}")
    
    # ì½˜ì†” ì¶œë ¥
    print("\n" + "="*60)
    print("ğŸ” íŠ¹ì§• ì„ íƒ ì‹¤í—˜ ì™„ë£Œ!")
    print("="*60)
    if best_overall:
        print(f"ìµœê³  ì„±ëŠ¥: {best_overall['name']}")
        print(f"íŠ¹ì§• ìˆ˜: {len(best_overall['features'])}ê°œ (ì›ë³¸: 51ê°œ)")
        print(f"í…ŒìŠ¤íŠ¸ ì •í™•ë„: {best_overall['test_accuracy']:.4f}")
        print(f"í…ŒìŠ¤íŠ¸ F1-score: {best_overall['test_f1_score']:.4f}")
    print(f"ê²°ê³¼ ì €ì¥: {results_dir}")
    print("="*60)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸ” íŠ¹ì§• ì„ íƒ ì‹¤í—˜ ì‹œì‘")
    
    try:
        # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_dir = f"experiments/feature_selection/selection_{timestamp}"
        os.makedirs(results_dir, exist_ok=True)
        
        # 1. ë°ì´í„° ë¡œë“œ
        data = load_data()
        
        # 2. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ
        best_model = load_best_model()
        
        # 3. Random Forest íŠ¹ì§• ì¤‘ìš”ë„ ë¶„ì„
        importance_df = random_forest_importance(data, best_model)
        
        # 4. RFE íŠ¹ì§• ì„ íƒ
        rfe_results = rfe_selection(data, best_model, [10, 15, 20, 25, 30])
        
        # 5. í†µê³„ì  íŠ¹ì§• ì„ íƒ
        statistical_results, stat_scores = statistical_selection(data, [10, 15, 20, 25, 30])
        
        # 6. ìƒê´€ê´€ê³„ ë¶„ì„
        target_corr, high_corr_pairs = correlation_analysis(data)
        
        # 7. ì ì§„ì  íŠ¹ì§• ì„ íƒ
        progressive_features, performance_history = progressive_feature_selection(data, best_model, 25)
        
        # 8. ë‹¤ì–‘í•œ íŠ¹ì§• ì„¸íŠ¸ ì •ì˜
        feature_sets = {
            'all_features_baseline': data['feature_names'],
            'top10_importance': importance_df.head(10)['feature'].tolist(),
            'top15_importance': importance_df.head(15)['feature'].tolist(),
            'top20_importance': importance_df.head(20)['feature'].tolist(),
            'top10_correlation': target_corr.head(10).index.tolist(),
            'top15_correlation': target_corr.head(15).index.tolist(),
            'rfe_15': rfe_results[15]['selected_features'],
            'rfe_20': rfe_results[20]['selected_features'],
            'statistical_15': statistical_results[15]['f_classification']['selected_features'],
            'statistical_20': statistical_results[20]['f_classification']['selected_features'],
            'progressive_selection': progressive_features
        }
        
        # 9. íŠ¹ì§• ì„¸íŠ¸ ì„±ëŠ¥ ë¹„êµ
        comparison_results = evaluate_feature_sets(data, feature_sets)
        
        # 10. ê²°ê³¼ í†µí•©
        all_results = {
            'feature_importance': importance_df.to_dict('records'),
            'rfe_results': rfe_results,
            'statistical_results': statistical_results,
            'correlation_analysis': {
                'target_correlation': target_corr.to_dict(),
                'high_correlation_pairs': high_corr_pairs
            },
            'progressive_selection': {
                'selected_features': progressive_features,
                'performance_history': performance_history
            },
            'feature_set_comparison': comparison_results
        }
        
        # 11. ì‹œê°í™” ìƒì„±
        create_visualizations(importance_df, target_corr, performance_history, results_dir)
        
        # 12. ê²°ê³¼ ì €ì¥
        recommendations = save_results(all_results, data, results_dir)
        
        # 13. ë³´ê³ ì„œ ìƒì„±
        generate_report(all_results, recommendations, results_dir)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()
        
        logger.info("ğŸ‰ íŠ¹ì§• ì„ íƒ ì‹¤í—˜ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    except Exception as e:
        logger.error(f"âŒ íŠ¹ì§• ì„ íƒ ì‹¤í—˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise


if __name__ == "__main__":
    main() 