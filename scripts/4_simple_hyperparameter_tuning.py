#!/usr/bin/env python3
"""
ê°„ë‹¨í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ìŠ¤í¬ë¦½íŠ¸

scikit-learn ëª¨ë¸ì„ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
ë³µì¡í•œ ë˜í¼ í´ë˜ìŠ¤ ì—†ì´ ì§ì ‘ì ì´ê³  ì•ˆì •ì ì¸ ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python scripts/simple_hyperparameter_tuning.py

ì‘ì„±ì: ML Team
ìƒì„±ì¼: 2025-01-15
"""

import os
import sys
import logging
import warnings
import pandas as pd
import numpy as np
import joblib
import yaml
from datetime import datetime
from pathlib import Path
import gc

from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import RandomizedSearchCV, cross_val_score
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings('ignore')

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


def load_data():
    """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
    logger.info("=== ë°ì´í„° ë¡œë“œ ì‹œì‘ ===")
    
    # ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    train_path = "data/splits/full_dataset/train.csv"
    val_path = "data/splits/full_dataset/val.csv"
    test_path = "data/splits/full_dataset/test.csv"
    
    # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    for path in [train_path, val_path, test_path]:
        if not os.path.exists(path):
            raise FileNotFoundError(f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")
    
    # ë°ì´í„° ë¡œë“œ
    train_df = pd.read_csv(train_path)
    val_df = pd.read_csv(val_path)
    test_df = pd.read_csv(test_path)
    
    logger.info(f"í›ˆë ¨ ë°ì´í„°: {train_df.shape}")
    logger.info(f"ê²€ì¦ ë°ì´í„°: {val_df.shape}")
    logger.info(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_df.shape}")
    
    # íŠ¹ì§•ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
    X_train = train_df.drop('sweetness', axis=1).values
    y_train = train_df['sweetness'].values
    X_val = val_df.drop('sweetness', axis=1).values
    y_val = val_df['sweetness'].values
    X_test = test_df.drop('sweetness', axis=1).values
    y_test = test_df['sweetness'].values
    
    # íŠ¹ì§• ìŠ¤ì¼€ì¼ë§
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    X_test_scaled = scaler.transform(X_test)
    
    logger.info("ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ")
    logger.info(f"íŠ¹ì§• ìˆ˜: {X_train_scaled.shape[1]}")
    logger.info(f"ë‹¹ë„ ë²”ìœ„: {float(np.array(y_train).min()):.2f} ~ {float(np.array(y_train).max()):.2f} Brix")
    
    return {
        'X_train': X_train_scaled,
        'y_train': y_train,
        'X_val': X_val_scaled,
        'y_val': y_val,
        'X_test': X_test_scaled,
        'y_test': y_test,
        'scaler': scaler
    }


def get_param_grids():
    """í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì •ì˜"""
    return {
        'gradient_boosting': {
            'n_estimators': [50, 100, 200, 300],
            'learning_rate': [0.01, 0.05, 0.1, 0.2],
            'max_depth': [3, 4, 5, 6, 7],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4],
            'subsample': [0.8, 0.9, 1.0],
            'random_state': [42]
        },
        'svm': {
            'C': [0.1, 1, 10, 100],
            'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],
            'kernel': ['rbf', 'poly'],
            'epsilon': [0.01, 0.1, 0.2],
            'degree': [2, 3, 4]  # poly kernelìš©
        },
        'random_forest': {
            'n_estimators': [50, 100, 200, 300],
            'max_depth': [None, 5, 10, 15, 20],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4],
            'max_features': ['sqrt', 'log2', None],
            'bootstrap': [True, False],
            'random_state': [42]
        }
    }


def tune_model(model, param_grid, X_train, y_train, model_name, n_iter=20):
    """ë‹¨ì¼ ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹"""
    logger.info(f"=== {model_name} í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘ ===")
    
    # RandomizedSearchCV ì„¤ì •
    search = RandomizedSearchCV(
        estimator=model,
        param_distributions=param_grid,
        n_iter=n_iter,
        scoring='neg_mean_absolute_error',
        cv=5,
        n_jobs=-1,
        verbose=1,
        random_state=42,
        return_train_score=True
    )
    
    # íŠœë‹ ì‹¤í–‰
    start_time = datetime.now()
    search.fit(X_train, y_train)
    end_time = datetime.now()
    
    tuning_time = (end_time - start_time).total_seconds()
    
    logger.info(f"{model_name} íŠœë‹ ì™„ë£Œ:")
    logger.info(f"  ìµœê³  ì ìˆ˜: {search.best_score_:.4f}")
    logger.info(f"  ìµœì  íŒŒë¼ë¯¸í„°: {search.best_params_}")
    logger.info(f"  ì†Œìš” ì‹œê°„: {tuning_time:.2f}ì´ˆ")
    
    return {
        'model': search.best_estimator_,
        'best_score': search.best_score_,
        'best_params': search.best_params_,
        'tuning_time': tuning_time,
        'cv_results': search.cv_results_
    }


def evaluate_model(model, X_test, y_test, X_val, y_val, model_name):
    """ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
    # ì˜ˆì¸¡
    y_pred_test = model.predict(X_test)
    y_pred_val = model.predict(X_val)
    
    # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€
    test_mae = mean_absolute_error(y_test, y_pred_test)
    test_mse = mean_squared_error(y_test, y_pred_test)
    test_rmse = np.sqrt(test_mse)
    test_r2 = r2_score(y_test, y_pred_test)
    
    # ê²€ì¦ ì„¸íŠ¸ í‰ê°€
    val_mae = mean_absolute_error(y_val, y_pred_val)
    val_mse = mean_squared_error(y_val, y_pred_val)
    val_rmse = np.sqrt(val_mse)
    val_r2 = r2_score(y_val, y_pred_val)
    
    logger.info(f"{model_name} ì„±ëŠ¥ í‰ê°€:")
    logger.info(f"  í…ŒìŠ¤íŠ¸ - MAE: {test_mae:.4f}, RMSE: {test_rmse:.4f}, RÂ²: {test_r2:.4f}")
    logger.info(f"  ê²€ì¦   - MAE: {val_mae:.4f}, RMSE: {val_rmse:.4f}, RÂ²: {val_r2:.4f}")
    
    return {
        'test': {
            'mae': test_mae,
            'mse': test_mse,
            'rmse': test_rmse,
            'r2': test_r2
        },
        'validation': {
            'mae': val_mae,
            'mse': val_mse,
            'rmse': val_rmse,
            'r2': val_r2
        }
    }


def load_baseline_results():
    """ê¸°ë³¸ ëª¨ë¸ ê²°ê³¼ ë¡œë“œ"""
    logger.info("ê¸°ë³¸ ëª¨ë¸ ê²°ê³¼ ë¡œë“œ ì‹œë„...")
    
    baseline_path = "models/saved/training_summary_simple.yaml"
    
    if os.path.exists(baseline_path):
        try:
            # ë¨¼ì € safe_load ì‹œë„
            with open(baseline_path, 'r', encoding='utf-8') as f:
                baseline_data = yaml.safe_load(f)
        except yaml.constructor.ConstructorError as e:
            logger.warning(f"YAML íŒŒì¼ì— numpy ê°ì²´ê°€ í¬í•¨ë˜ì–´ ìˆì–´ safe_loadì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
            try:
                # unsafe loadë¡œ ì‹œë„ (numpy ê°ì²´ í¬í•¨ íŒŒì¼ìš©)
                with open(baseline_path, 'r', encoding='utf-8') as f:
                    baseline_data = yaml.unsafe_load(f)
                logger.info("unsafe_loadë¡œ ê¸°ë³¸ ëª¨ë¸ ê²°ê³¼ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            except Exception as e2:
                logger.error(f"unsafe_loadë„ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e2}")
                logger.warning("ê¸°ë³¸ ëª¨ë¸ ê²°ê³¼ ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                return {}
        except Exception as e:
            logger.error(f"YAML íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.warning("ê¸°ë³¸ ëª¨ë¸ ê²°ê³¼ ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return {}
        
        # ê²°ê³¼ êµ¬ì¡° ë³€í™˜
        baseline_results = {}
        test_performance = baseline_data.get('test_performance', {})
        
        if test_performance:
            for model_name, metrics in test_performance.items():
                # numpy ê°ì²´ë¥¼ floatë¡œ ë³€í™˜
                converted_metrics = {}
                for key, value in metrics.items():
                    if hasattr(value, 'item'):  # numpy scalarì¸ ê²½ìš°
                        converted_metrics[key] = float(value.item())
                    else:
                        converted_metrics[key] = float(value)
                
                baseline_results[model_name] = {'test': converted_metrics}
        
        logger.info(f"ê¸°ë³¸ ëª¨ë¸ ê²°ê³¼ ë¡œë“œ ì™„ë£Œ: {list(baseline_results.keys())}")
        return baseline_results
    else:
        logger.warning("ê¸°ë³¸ ëª¨ë¸ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {}


def compare_with_baseline(tuned_results, baseline_results):
    """ê¸°ë³¸ ëª¨ë¸ê³¼ ì„±ëŠ¥ ë¹„êµ"""
    if not baseline_results:
        logger.warning("ê¸°ë³¸ ëª¨ë¸ ê²°ê³¼ê°€ ì—†ì–´ ë¹„êµë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return {}
    
    logger.info("=== ê¸°ë³¸ ëª¨ë¸ ëŒ€ë¹„ ì„±ëŠ¥ ê°œì„  ===")
    improvements = {}
    
    model_mapping = {
        'gradient_boosting': 'random_forest',  # ê¸°ë³¸ ê²°ê³¼ì—ì„œëŠ” random_forestê°€ ìµœê³ ì˜€ìŒ
        'svm': 'random_forest',
        'random_forest': 'random_forest'
    }
    
    for model_name, evaluation in tuned_results.items():
        if model_mapping[model_name] in baseline_results:
            baseline_model = model_mapping[model_name]
            
            # íŠœë‹ëœ ëª¨ë¸ ì„±ëŠ¥
            tuned_mae = evaluation['test']['mae']
            tuned_r2 = evaluation['test']['r2']
            
            # ê¸°ë³¸ ëª¨ë¸ ì„±ëŠ¥
            baseline_mae = baseline_results[baseline_model]['test']['mae']
            baseline_r2 = baseline_results[baseline_model]['test']['r2']
            
            # ê°œì„  ì •ë„ ê³„ì‚°
            mae_improvement = ((baseline_mae - tuned_mae) / baseline_mae) * 100
            r2_improvement = ((tuned_r2 - baseline_r2) / baseline_r2) * 100 if baseline_r2 > 0 else 0
            
            improvements[model_name] = {
                'mae_improvement': mae_improvement,
                'r2_improvement': r2_improvement,
                'baseline_mae': baseline_mae,
                'tuned_mae': tuned_mae,
                'baseline_r2': baseline_r2,
                'tuned_r2': tuned_r2
            }
            
            logger.info(f"  {model_name} vs {baseline_model}:")
            logger.info(f"    MAE: {baseline_mae:.4f} â†’ {tuned_mae:.4f} ({mae_improvement:+.2f}%)")
            logger.info(f"    RÂ²:  {baseline_r2:.4f} â†’ {tuned_r2:.4f} ({r2_improvement:+.2f}%)")
    
    return improvements


def save_results(tuning_results, evaluation_results, improvements, data):
    """ê²°ê³¼ ì €ì¥"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_dir = f"experiments/hyperparameter_tuning/simple_tuning_{timestamp}"
    os.makedirs(results_dir, exist_ok=True)
    
    logger.info(f"ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬: {results_dir}")
    
    # ëª¨ë¸ ì €ì¥
    for model_name, result in tuning_results.items():
        model_path = os.path.join(results_dir, f"{model_name}_tuned.pkl")
        joblib.dump(result['model'], model_path)
        logger.info(f"ëª¨ë¸ ì €ì¥: {model_path}")
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
    scaler_path = os.path.join(results_dir, "scaler.pkl")
    joblib.dump(data['scaler'], scaler_path)
    
    # íŠœë‹ ê²°ê³¼ ì €ì¥
    tuning_summary = {}
    for model_name, result in tuning_results.items():
        tuning_summary[model_name] = {
            'best_score': result['best_score'],
            'best_params': result['best_params'],
            'tuning_time': result['tuning_time']
        }
    
    tuning_path = os.path.join(results_dir, "tuning_results.yaml")
    with open(tuning_path, 'w', encoding='utf-8') as f:
        yaml.dump(tuning_summary, f, default_flow_style=False, allow_unicode=True)
    
    # í‰ê°€ ê²°ê³¼ ì €ì¥
    evaluation_path = os.path.join(results_dir, "evaluation_results.yaml")
    with open(evaluation_path, 'w', encoding='utf-8') as f:
        yaml.dump(evaluation_results, f, default_flow_style=False, allow_unicode=True)
    
    # ê°œì„  ê²°ê³¼ ì €ì¥
    if improvements:
        improvement_path = os.path.join(results_dir, "improvements.yaml")
        with open(improvement_path, 'w', encoding='utf-8') as f:
            yaml.dump(improvements, f, default_flow_style=False, allow_unicode=True)
    
    return results_dir


def generate_report(tuning_results, evaluation_results, improvements, results_dir):
    """ìš”ì•½ ë³´ê³ ì„œ ìƒì„±"""
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
    best_model_name = min(evaluation_results.keys(), 
                         key=lambda x: evaluation_results[x]['test']['mae'])
    best_metrics = evaluation_results[best_model_name]['test']
    
    # ë³´ê³ ì„œ ì‘ì„±
    report = f"""# í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê²°ê³¼ ë³´ê³ ì„œ

ìƒì„±ì¼: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ì‹¤í—˜ ê°œìš”

- **ëª©ì **: ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ ëª¨ë¸ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
- **ë°ì´í„°**: 51ê°œ íŠ¹ì§•, 146ê°œ ìƒ˜í”Œ (Train: 102, Val: 22, Test: 22)
- **ë°©ë²•**: RandomizedSearchCV (20íšŒ ë°˜ë³µ)
- **í‰ê°€ ì§€í‘œ**: MAE (Mean Absolute Error), RÂ² (R-squared)

## ìµœê³  ì„±ëŠ¥ ëª¨ë¸

**ëª¨ë¸**: {best_model_name}
- **í…ŒìŠ¤íŠ¸ MAE**: {best_metrics['mae']:.4f} Brix
- **í…ŒìŠ¤íŠ¸ RÂ²**: {best_metrics['r2']:.4f}
- **í…ŒìŠ¤íŠ¸ RMSE**: {best_metrics['rmse']:.4f} Brix

## ëª¨ë“  ëª¨ë¸ ì„±ëŠ¥

"""
    
    # ëª¨ë“  ëª¨ë¸ ì„±ëŠ¥ ì¶”ê°€
    for model_name, metrics in evaluation_results.items():
        test_metrics = metrics['test']
        tuning_time = tuning_results[model_name]['tuning_time']
        report += f"""
### {model_name}
- **í…ŒìŠ¤íŠ¸ MAE**: {test_metrics['mae']:.4f} Brix
- **í…ŒìŠ¤íŠ¸ RÂ²**: {test_metrics['r2']:.4f}
- **í…ŒìŠ¤íŠ¸ RMSE**: {test_metrics['rmse']:.4f} Brix
- **íŠœë‹ ì‹œê°„**: {tuning_time:.2f}ì´ˆ
- **ìµœì  íŒŒë¼ë¯¸í„°**: {tuning_results[model_name]['best_params']}
"""
    
    # ê°œì„  ì‚¬í•­ ì¶”ê°€
    if improvements:
        report += "\n## ê¸°ë³¸ ëª¨ë¸ ëŒ€ë¹„ ê°œì„ \n"
        for model_name, imp in improvements.items():
            report += f"""
### {model_name}
- **MAE ê°œì„ **: {imp['mae_improvement']:+.2f}%
- **RÂ² ê°œì„ **: {imp['r2_improvement']:+.2f}%
"""
    
    # ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
    report += f"""
## ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„± ì—¬ë¶€

- **MAE < 1.0 Brix**: {'âœ… ë‹¬ì„±' if best_metrics['mae'] < 1.0 else 'âŒ ë¯¸ë‹¬ì„±'} ({best_metrics['mae']:.4f})
- **RÂ² > 0.8**: {'âœ… ë‹¬ì„±' if best_metrics['r2'] > 0.8 else 'âŒ ë¯¸ë‹¬ì„±'} ({best_metrics['r2']:.4f})

## ê²°ë¡ 

ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì¸ **{best_model_name}**ì´ MAE {best_metrics['mae']:.4f} Brix, RÂ² {best_metrics['r2']:.4f}ì˜ 
{'ìš°ìˆ˜í•œ' if best_metrics['mae'] < 1.0 and best_metrics['r2'] > 0.8 else 'ì–‘í˜¸í•œ'} ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ í†µí•´ ëª¨ë¸ ì„±ëŠ¥ì´ ê°œì„ ë˜ì—ˆìœ¼ë©°, 
{'ëª©í‘œ ì„±ëŠ¥ì„ ë‹¬ì„±' if best_metrics['mae'] < 1.0 and best_metrics['r2'] > 0.8 else 'ëª©í‘œì— ê·¼ì ‘í•œ ì„±ëŠ¥ì„ í™•ë³´'}í–ˆìŠµë‹ˆë‹¤.
"""
    
    # ë³´ê³ ì„œ ì €ì¥
    report_path = os.path.join(results_dir, "TUNING_REPORT.md")
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    logger.info(f"ìš”ì•½ ë³´ê³ ì„œ ì €ì¥: {report_path}")
    
    # ì½˜ì†” ì¶œë ¥
    print("\n" + "="*60)
    print("ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì™„ë£Œ!")
    print("="*60)
    print(f"ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name}")
    print(f"í…ŒìŠ¤íŠ¸ MAE: {best_metrics['mae']:.4f} Brix")
    print(f"í…ŒìŠ¤íŠ¸ RÂ²: {best_metrics['r2']:.4f}")
    print(f"ëª©í‘œ ë‹¬ì„±: {'âœ…' if best_metrics['mae'] < 1.0 and best_metrics['r2'] > 0.8 else 'âš ï¸'}")
    print(f"ê²°ê³¼ ì €ì¥: {results_dir}")
    print("="*60)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸ‰ ê°„ë‹¨í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘")
    
    try:
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs("experiments/hyperparameter_tuning", exist_ok=True)
        
        # 1. ë°ì´í„° ë¡œë“œ
        data = load_data()
        
        # 2. ê¸°ë³¸ ëª¨ë¸ ê²°ê³¼ ë¡œë“œ
        baseline_results = load_baseline_results()
        
        # 3. ëª¨ë¸ ë° íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì •ì˜
        models = {
            'gradient_boosting': GradientBoostingRegressor(),
            'svm': SVR(),
            'random_forest': RandomForestRegressor()
        }
        param_grids = get_param_grids()
        
        # 4. ê° ëª¨ë¸ì— ëŒ€í•´ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
        tuning_results = {}
        for model_name, model in models.items():
            try:
                result = tune_model(
                    model, 
                    param_grids[model_name], 
                    data['X_train'], 
                    data['y_train'], 
                    model_name,
                    n_iter=20
                )
                tuning_results[model_name] = result
            except Exception as e:
                logger.error(f"{model_name} íŠœë‹ ì‹¤íŒ¨: {e}")
                continue
        
        # 5. ì„±ëŠ¥ í‰ê°€
        evaluation_results = {}
        for model_name, result in tuning_results.items():
            evaluation = evaluate_model(
                result['model'],
                data['X_test'],
                data['y_test'],
                data['X_val'],
                data['y_val'],
                model_name
            )
            evaluation_results[model_name] = evaluation
        
        # 6. ê¸°ë³¸ ëª¨ë¸ê³¼ ë¹„êµ
        improvements = compare_with_baseline(evaluation_results, baseline_results)
        
        # 7. ê²°ê³¼ ì €ì¥
        results_dir = save_results(tuning_results, evaluation_results, improvements, data)
        
        # 8. ë³´ê³ ì„œ ìƒì„±
        generate_report(tuning_results, evaluation_results, improvements, results_dir)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()
        
        logger.info("ğŸ‰ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    except Exception as e:
        logger.error(f"âŒ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise


if __name__ == "__main__":
    main() 