#!/usr/bin/env python3
"""
Ensemble Model Experiment Script for Watermelon Sweetness Prediction

This script trains and evaluates various ensemble models:
- Simple Voting (equal weights)
- Weighted Average (performance-based weights)  
- Stacking with different meta-learners (Linear, Ridge, Lasso)

Author: Watermelon ML Project Team
Date: 2025-01-15
"""

import sys
import os
import logging
import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
import yaml
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.append(str(PROJECT_ROOT))

from src.models.ensemble_model import EnsembleTrainer, WatermelonEnsemble
from src.evaluation.visualizer import ResultVisualizer


def setup_logging(experiment_dir: Path) -> None:
    """Setup logging configuration."""
    log_file = experiment_dir / 'ensemble_experiment.log'
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )


def load_data() -> tuple:
    """Load preprocessed training, validation, and test data."""
    logger = logging.getLogger(__name__)
    logger.info("=== ë°ì´í„° ë¡œë“œ ì‹œì‘ ===")
    
    # Load the feature-selected dataset (10 features from progressive selection)
    feature_dir = PROJECT_ROOT / "experiments" / "feature_selection"
    
    # Find the latest feature selection experiment
    feature_experiments = sorted([d for d in feature_dir.iterdir() if d.is_dir() and d.name.startswith('selection_')])
    if not feature_experiments:
        raise FileNotFoundError("íŠ¹ì§• ì„ íƒ ì‹¤í—˜ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    latest_experiment = feature_experiments[-1]
    logger.info(f"ìµœì‹  íŠ¹ì§• ì„ íƒ ì‹¤í—˜ ì‚¬ìš©: {latest_experiment.name}")
    
    # Load the best feature subset (progressive_selection)
    best_features_file = latest_experiment / "progressive_selection_features.txt"
    if not best_features_file.exists():
        # Fallback to the original splits
        logger.warning("ìµœì  íŠ¹ì§• íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ì „ì²´ íŠ¹ì§• ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        train_df = pd.read_csv(PROJECT_ROOT / "data" / "splits" / "full_dataset" / "train.csv")
        val_df = pd.read_csv(PROJECT_ROOT / "data" / "splits" / "full_dataset" / "val.csv")
        test_df = pd.read_csv(PROJECT_ROOT / "data" / "splits" / "full_dataset" / "test.csv")
        selected_features = [col for col in train_df.columns if col != 'sweetness']
    else:
        # Load selected features
        with open(best_features_file, 'r', encoding='utf-8') as f:
            selected_features = [line.strip() for line in f.readlines()]
        
        logger.info(f"ì„ íƒëœ {len(selected_features)}ê°œ íŠ¹ì§• ì‚¬ìš©")
        
        # Load full datasets and select features
        train_df = pd.read_csv(PROJECT_ROOT / "data" / "splits" / "full_dataset" / "train.csv")
        val_df = pd.read_csv(PROJECT_ROOT / "data" / "splits" / "full_dataset" / "val.csv")
        test_df = pd.read_csv(PROJECT_ROOT / "data" / "splits" / "full_dataset" / "test.csv")
    
    # Extract features and targets
    X_train = train_df[selected_features].values
    y_train = train_df['sweetness'].values
    
    X_val = val_df[selected_features].values
    y_val = val_df['sweetness'].values
    
    X_test = test_df[selected_features].values
    y_test = test_df['sweetness'].values
    
    logger.info(f"ë°ì´í„° í˜•íƒœ - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}")
    logger.info(f"ë‹¹ë„ ë²”ìœ„ - Train: {y_train.min():.1f}~{y_train.max():.1f}, "
                f"Val: {y_val.min():.1f}~{y_val.max():.1f}, "
                f"Test: {y_test.min():.1f}~{y_test.max():.1f}")
    
    return X_train, y_train, X_val, y_val, X_test, y_test, selected_features


def scale_features(X_train: np.ndarray, X_val: np.ndarray, X_test: np.ndarray) -> tuple:
    """Scale features using StandardScaler."""
    logger = logging.getLogger(__name__)
    logger.info("íŠ¹ì§• ìŠ¤ì¼€ì¼ë§ ì¤‘...")
    
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    X_test_scaled = scaler.transform(X_test)
    
    logger.info("íŠ¹ì§• ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ")
    return X_train_scaled, X_val_scaled, X_test_scaled, scaler


def create_ensemble_config() -> dict:
    """Create ensemble experiment configuration."""
    config = {
        'ensemble_strategies': ['voting', 'weighted', 'stacking'],
        'meta_learners': ['ridge', 'linear', 'lasso'],
        'cv_folds': 5,
        'random_state': 42,
        'evaluation_metrics': ['mae', 'mse', 'rmse', 'r2']
    }
    return config


def load_tuned_hyperparameters() -> dict | None:
    """í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê²°ê³¼ì—ì„œ ìµœì  íŒŒë¼ë¯¸í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    logger = logging.getLogger(__name__)
    
    try:
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê²°ê³¼ ë””ë ‰í† ë¦¬ ì°¾ê¸°
        hp_dir = PROJECT_ROOT / "experiments" / "hyperparameter_tuning"
        if not hp_dir.exists():
            logger.warning("í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê²°ê³¼ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return None
            
        # ìµœì‹  íŠœë‹ ê²°ê³¼ ì°¾ê¸° (ì‹œê°„ìˆœ ì •ë ¬)
        hp_experiments = [d for d in hp_dir.iterdir() if d.is_dir()]
        if not hp_experiments:
            logger.warning("í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return None
            
        # simple_tuning_ íŒ¨í„´ ìš°ì„  ì„ íƒ
        simple_tuning_dirs = [d for d in hp_experiments if d.name.startswith('simple_tuning_')]
        if simple_tuning_dirs:
            # ì‹œê°„ìˆœ ì •ë ¬
            simple_tuning_dirs.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            latest_hp = simple_tuning_dirs[0]
        else:
            # fallback: ê°€ì¥ ìµœê·¼ ë””ë ‰í† ë¦¬
            hp_experiments.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            latest_hp = hp_experiments[0]
        
        logger.info(f"í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê²°ê³¼ ë¡œë“œ: {latest_hp.name}")
        
        # tuning_results.yamlì—ì„œ ìµœì  íŒŒë¼ë¯¸í„° ë¡œë“œ (ì˜¬ë°”ë¥¸ íŒŒì¼)
        tuning_file = latest_hp / "tuning_results.yaml"
        if tuning_file.exists():
            try:
                with open(tuning_file, 'r', encoding='utf-8') as f:
                    tuning_results = yaml.safe_load(f)
            except yaml.constructor.ConstructorError:
                logger.info("tuning_results.yamlì— numpy ê°ì²´ê°€ í¬í•¨ë˜ì–´ ìˆì–´ unsafe_loadë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                with open(tuning_file, 'r', encoding='utf-8') as f:
                    tuning_results = yaml.unsafe_load(f)
            
            # ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶”ì¶œ
            tuned_params = {}
            for model_name, results in tuning_results.items():
                if isinstance(results, dict) and 'best_params' in results:
                    best_params = results['best_params']
                    # numpy ê°’ë“¤ì„ ì¼ë°˜ Python íƒ€ì…ìœ¼ë¡œ ë³€í™˜
                    converted_params = {}
                    for key, value in best_params.items():
                        if hasattr(value, 'item'):  # numpy scalar
                            converted_params[key] = value.item()
                        else:
                            converted_params[key] = value
                    tuned_params[model_name] = converted_params
                    
            if tuned_params:
                logger.info(f"âœ… íŠœë‹ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¡œë“œ ì™„ë£Œ: {list(tuned_params.keys())}")
                return tuned_params
        
        logger.warning("ìœ íš¨í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return None
        
    except Exception as e:
        logger.warning(f"í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¡œë“œ ì‹¤íŒ¨: {e}. ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return None


def evaluate_individual_models(X_train: np.ndarray, y_train: np.ndarray,
                              X_val: np.ndarray, y_val: np.ndarray,
                              X_test: np.ndarray, y_test: np.ndarray) -> dict:
    """Evaluate individual base models for comparison."""
    logger = logging.getLogger(__name__)
    logger.info("=== ê°œë³„ ëª¨ë¸ í‰ê°€ ===")
    
    from src.models.traditional_ml import WatermelonRandomForest, WatermelonGBT, WatermelonSVM
    
    # ğŸ”— í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê²°ê³¼ ë¡œë“œ
    tuned_params = load_tuned_hyperparameters()
    
    if tuned_params:
        logger.info("âœ… íŠœë‹ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        # íŠœë‹ ê²°ê³¼ì—ì„œ ìµœì  íŒŒë¼ë¯¸í„° ì‚¬ìš©
        rf_config = {
            'model': tuned_params.get('random_forest', {
                'n_estimators': 200,
                'max_depth': 15,
                'min_samples_split': 2,
                'min_samples_leaf': 1,
                'max_features': 'sqrt'
            })
        }
        
        gbt_config = {
            'model': tuned_params.get('gradient_boosting', {
                'n_estimators': 200,
                'learning_rate': 0.1,
                'max_depth': 6,
                'subsample': 0.8
            })
        }
        
        svm_config = {
            'model': tuned_params.get('svm', {
                'kernel': 'rbf',
                'C': 10,
                'gamma': 'scale',
                'epsilon': 0.01
            })
        }
    else:
        logger.warning("âš ï¸  íŠœë‹ ê²°ê³¼ê°€ ì—†ì–´ ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        # ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° (fallback)
        rf_config = {
            'model': {
                'n_estimators': 200,
                'max_depth': 15,
                'min_samples_split': 2,
                'min_samples_leaf': 1,
                'max_features': 'sqrt'
            }
        }
        
        gbt_config = {
            'model': {
                'n_estimators': 200,
                'learning_rate': 0.1,
                'max_depth': 6,
                'subsample': 0.8
            }
        }
        
        svm_config = {
            'model': {
                'kernel': 'rbf',
                'C': 10,
                'gamma': 'scale',
                'epsilon': 0.01
            }
        }
    
    models = {
        'Random Forest': WatermelonRandomForest(config=rf_config, random_state=42),
        'Gradient Boosting': WatermelonGBT(config=gbt_config, random_state=42),
        'SVM': WatermelonSVM(config=svm_config, random_state=42)
    }
    
    individual_results = {}
    
    for name, model in models.items():
        logger.info(f"  {name} í‰ê°€ ì¤‘...")
        
        # Train model
        model.fit(X_train, y_train)
        
        # Predictions
        val_pred = model.predict(X_val)
        test_pred = model.predict(X_test)
        
        # Metrics
        val_mae = mean_absolute_error(y_val, val_pred)
        val_r2 = r2_score(y_val, val_pred)
        test_mae = mean_absolute_error(y_test, test_pred)
        test_r2 = r2_score(y_test, test_pred)
        
        individual_results[name] = {
            'val_mae': val_mae,
            'val_r2': val_r2,
            'test_mae': test_mae,
            'test_r2': test_r2
        }
        
        logger.info(f"    ê²€ì¦ - MAE: {val_mae:.4f}, RÂ²: {val_r2:.4f}")
        logger.info(f"    í…ŒìŠ¤íŠ¸ - MAE: {test_mae:.4f}, RÂ²: {test_r2:.4f}")
    
    return individual_results


def create_performance_comparison_plot(individual_results: dict, 
                                     ensemble_results: dict,
                                     test_results: dict,
                                     save_dir: Path) -> None:
    """Create comprehensive performance comparison plots."""
    logger = logging.getLogger(__name__)
    logger.info("ì„±ëŠ¥ ë¹„êµ ì‹œê°í™” ìƒì„± ì¤‘...")
    
    # Prepare data for plotting
    models = []
    val_maes = []
    test_maes = []
    val_r2s = []
    test_r2s = []
    model_types = []
    
    # Individual models
    for name, results in individual_results.items():
        models.append(name)
        val_maes.append(results['val_mae'])
        test_maes.append(results['test_mae'])
        val_r2s.append(results['val_r2'])
        test_r2s.append(results['test_r2'])
        model_types.append('Individual')
    
    # Ensemble models
    for name, val_results in ensemble_results.items():
        test_results_model = test_results[name]
        models.append(name.replace('ensemble_', '').replace('_', ' ').title())
        val_maes.append(val_results['val_mae'])
        test_maes.append(test_results_model['test_mae'])
        val_r2s.append(val_results['val_r2'])
        test_r2s.append(test_results_model['test_r2'])
        model_types.append('Ensemble')
    
    # Create comprehensive comparison plot
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # MAE comparison
    ax1 = axes[0, 0]
    x_pos = np.arange(len(models))
    colors = ['skyblue' if t == 'Individual' else 'lightcoral' for t in model_types]
    
    bars1 = ax1.bar(x_pos - 0.2, val_maes, 0.4, label='Validation', color=colors, alpha=0.7)
    bars2 = ax1.bar(x_pos + 0.2, test_maes, 0.4, label='Test', color=colors, alpha=1.0)
    
    ax1.set_xlabel('Models')
    ax1.set_ylabel('MAE (Brix)')
    ax1.set_title('MAE Comparison: Individual vs Ensemble Models')
    ax1.set_xticks(x_pos)
    ax1.set_xticklabels(models, rotation=45, ha='right')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Add value labels on bars
    for bar in bars1:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.001,
                f'{height:.3f}', ha='center', va='bottom', fontsize=8)
    for bar in bars2:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.001,
                f'{height:.3f}', ha='center', va='bottom', fontsize=8)
    
    # RÂ² comparison
    ax2 = axes[0, 1]
    bars3 = ax2.bar(x_pos - 0.2, val_r2s, 0.4, label='Validation', color=colors, alpha=0.7)
    bars4 = ax2.bar(x_pos + 0.2, test_r2s, 0.4, label='Test', color=colors, alpha=1.0)
    
    ax2.set_xlabel('Models')
    ax2.set_ylabel('RÂ² Score')
    ax2.set_title('RÂ² Comparison: Individual vs Ensemble Models')
    ax2.set_xticks(x_pos)
    ax2.set_xticklabels(models, rotation=45, ha='right')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # Add value labels on bars
    for bar in bars3:
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.001,
                f'{height:.3f}', ha='center', va='bottom', fontsize=8)
    for bar in bars4:
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.001,
                f'{height:.3f}', ha='center', va='bottom', fontsize=8)
    
    # Performance improvement heatmap
    ax3 = axes[1, 0]
    
    # Calculate improvement over best individual model
    best_individual_mae = min([r['test_mae'] for r in individual_results.values()])
    best_individual_r2 = max([r['test_r2'] for r in individual_results.values()])
    
    improvement_data = []
    improvement_labels = []
    
    for name, test_result in test_results.items():
        mae_improvement = (best_individual_mae - test_result['test_mae']) / best_individual_mae * 100
        r2_improvement = (test_result['test_r2'] - best_individual_r2) / best_individual_r2 * 100
        
        improvement_data.append([mae_improvement, r2_improvement])
        improvement_labels.append(name.replace('ensemble_', '').replace('_', ' ').title())
    
    improvement_matrix = np.array(improvement_data)
    
    sns.heatmap(improvement_matrix, 
                xticklabels=['MAE Improvement (%)', 'RÂ² Improvement (%)'],
                yticklabels=improvement_labels,
                annot=True, fmt='.2f', cmap='RdYlGn', center=0,
                ax=ax3)
    ax3.set_title('Performance Improvement over Best Individual Model')
    
    # Model complexity comparison
    ax4 = axes[1, 1]
    
    ensemble_names = [name.replace('ensemble_', '').replace('_', ' ').title() for name in ensemble_results.keys()]
    ensemble_test_maes = [test_results[name]['test_mae'] for name in ensemble_results.keys()]
    
    scatter_colors = plt.cm.viridis(np.linspace(0, 1, len(ensemble_names)))
    
    for i, (name, mae) in enumerate(zip(ensemble_names, ensemble_test_maes)):
        complexity = 3 if 'stacking' in name.lower() else 2 if 'weighted' in name.lower() else 1
        ax4.scatter(complexity, mae, s=200, c=[scatter_colors[i]], 
                   label=name, alpha=0.7, edgecolors='black')
    
    ax4.set_xlabel('Model Complexity (1=Voting, 2=Weighted, 3=Stacking)')
    ax4.set_ylabel('Test MAE (Brix)')
    ax4.set_title('Ensemble Complexity vs Performance')
    ax4.grid(True, alpha=0.3)
    ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    
    plt.tight_layout()
    plt.savefig(save_dir / 'ensemble_performance_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    logger.info(f"ì„±ëŠ¥ ë¹„êµ ì‹œê°í™” ì €ì¥: {save_dir / 'ensemble_performance_comparison.png'}")


def save_results(ensemble_results: dict, test_results: dict, 
                individual_results: dict, config: dict, 
                experiment_dir: Path) -> None:
    """Save comprehensive experiment results."""
    logger = logging.getLogger(__name__)
    
    # Save configuration
    config_file = experiment_dir / 'ensemble_config.yaml'
    with open(config_file, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
    
    # Save detailed results
    all_results = {
        'experiment_info': {
            'timestamp': datetime.now().isoformat(),
            'experiment_type': 'ensemble_model_comparison',
            'config': config
        },
        'individual_model_results': individual_results,
        'ensemble_validation_results': ensemble_results,
        'ensemble_test_results': test_results
    }
    
    results_file = experiment_dir / 'ensemble_results.yaml'
    with open(results_file, 'w', encoding='utf-8') as f:
        yaml.dump(all_results, f, default_flow_style=False, allow_unicode=True)
    
    # Create summary report
    create_ensemble_report(individual_results, ensemble_results, test_results, experiment_dir)
    
    logger.info(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {experiment_dir}")


def create_ensemble_report(individual_results: dict, ensemble_results: dict, 
                         test_results: dict, experiment_dir: Path) -> None:
    """Create a comprehensive ensemble experiment report."""
    
    report_file = experiment_dir / 'ENSEMBLE_EXPERIMENT_REPORT.md'
    
    # Find best models
    best_individual = min(individual_results.items(), key=lambda x: x[1]['test_mae'])
    best_ensemble = min(test_results.items(), key=lambda x: x[1]['test_mae'])
    
    report_content = f"""# ğŸ¤– ì•™ìƒë¸” ëª¨ë¸ ì‹¤í—˜ ë³´ê³ ì„œ

## ğŸ“Š ì‹¤í—˜ ê°œìš”

- **ì‹¤í—˜ ì¼ì‹œ**: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %H:%M:%S')}
- **ì‹¤í—˜ ëª©ì **: ì „í†µì ì¸ ML ëª¨ë¸ë“¤ì˜ ì•™ìƒë¸”ì„ í†µí•œ ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ ì„±ëŠ¥ í–¥ìƒ
- **ì•™ìƒë¸” ì „ëµ**: Voting, Weighted Average, Stacking (Ridge/Linear/Lasso)

## ğŸ† ì£¼ìš” ê²°ê³¼

### ìµœê³  ì„±ëŠ¥ ëª¨ë¸

**ê°œë³„ ëª¨ë¸ ìµœê³  ì„±ëŠ¥:**
- **ëª¨ë¸**: {best_individual[0]}
- **í…ŒìŠ¤íŠ¸ MAE**: {best_individual[1]['test_mae']:.4f} Brix
- **í…ŒìŠ¤íŠ¸ RÂ²**: {best_individual[1]['test_r2']:.4f}

**ì•™ìƒë¸” ëª¨ë¸ ìµœê³  ì„±ëŠ¥:**
- **ëª¨ë¸**: {best_ensemble[0].replace('ensemble_', '').replace('_', ' ').title()}
- **í…ŒìŠ¤íŠ¸ MAE**: {best_ensemble[1]['test_mae']:.4f} Brix
- **í…ŒìŠ¤íŠ¸ RÂ²**: {best_ensemble[1]['test_r2']:.4f}

**ì„±ëŠ¥ ê°œì„ :**
- **MAE ê°œì„ **: {(best_individual[1]['test_mae'] - best_ensemble[1]['test_mae']):.4f} Brix ({((best_individual[1]['test_mae'] - best_ensemble[1]['test_mae']) / best_individual[1]['test_mae'] * 100):.2f}%)
- **RÂ² ê°œì„ **: {(best_ensemble[1]['test_r2'] - best_individual[1]['test_r2']):.4f} ({((best_ensemble[1]['test_r2'] - best_individual[1]['test_r2']) / best_individual[1]['test_r2'] * 100):.2f}%)

## ğŸ“ˆ ê°œë³„ ëª¨ë¸ ì„±ëŠ¥

| ëª¨ë¸ | ê²€ì¦ MAE | ê²€ì¦ RÂ² | í…ŒìŠ¤íŠ¸ MAE | í…ŒìŠ¤íŠ¸ RÂ² |
|------|----------|---------|------------|-----------|"""

    for name, results in individual_results.items():
        report_content += f"\n| {name} | {results['val_mae']:.4f} | {results['val_r2']:.4f} | {results['test_mae']:.4f} | {results['test_r2']:.4f} |"

    report_content += f"""

## ğŸ¤– ì•™ìƒë¸” ëª¨ë¸ ì„±ëŠ¥

| ì•™ìƒë¸” ì „ëµ | ê²€ì¦ MAE | ê²€ì¦ RÂ² | í…ŒìŠ¤íŠ¸ MAE | í…ŒìŠ¤íŠ¸ RÂ² |
|-------------|----------|---------|------------|-----------|"""

    for name, val_results in ensemble_results.items():
        test_res = test_results[name]
        ensemble_name = name.replace('ensemble_', '').replace('_', ' ').title()
        report_content += f"\n| {ensemble_name} | {val_results['val_mae']:.4f} | {val_results['val_r2']:.4f} | {test_res['test_mae']:.4f} | {test_res['test_r2']:.4f} |"

    report_content += f"""

## ğŸ” ìƒì„¸ ë¶„ì„

### ì•™ìƒë¸” ì „ëµë³„ íŠ¹ì§•

**1. Voting Ensemble (ë‹¨ìˆœ í‰ê· )**
- ëª¨ë“  ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ì„ ë™ì¼í•œ ê°€ì¤‘ì¹˜ë¡œ í‰ê· 
- ê°€ì¥ ë‹¨ìˆœí•˜ì§€ë§Œ ì•ˆì •ì ì¸ ì„±ëŠ¥
- ê°œë³„ ëª¨ë¸ì˜ í¸í–¥ì„ ìƒí˜¸ ë³´ì™„

**2. Weighted Average (ê°€ì¤‘ í‰ê· )**
- êµì°¨ ê²€ì¦ ì„±ëŠ¥ì— ë”°ë¼ ê°€ì¤‘ì¹˜ ì°¨ë“± ì ìš©
- ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬
- Votingë³´ë‹¤ ì¼ë°˜ì ìœ¼ë¡œ ìš°ìˆ˜í•œ ì„±ëŠ¥

**3. Stacking (ìŠ¤íƒœí‚¹)**
- ê¸°ë³¸ ëª¨ë¸ë“¤ì˜ ì˜ˆì¸¡ì„ ì…ë ¥ìœ¼ë¡œ í•˜ëŠ” ë©”íƒ€ ëª¨ë¸ í•™ìŠµ
- ê°€ì¥ ë³µì¡í•˜ì§€ë§Œ ë†’ì€ ì„±ëŠ¥ ì ì¬ë ¥
- ë©”íƒ€ ëª¨ë¸ì— ë”°ë¼ ì„±ëŠ¥ ì°¨ì´ ë°œìƒ

### ë©”íƒ€ ëª¨ë¸ ë¹„êµ (Stacking)

"""

    stacking_results = {k: v for k, v in test_results.items() if 'stacking' in k}
    for name, results in stacking_results.items():
        meta_learner = name.split('_')[-1].title()
        report_content += f"- **{meta_learner}**: MAE {results['test_mae']:.4f}, RÂ² {results['test_r2']:.4f}\n"

    report_content += f"""

## ğŸ¯ ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„±ë„

- **ëª©í‘œ MAE < 1.0 Brix**: âœ… ë‹¬ì„± (ìµœê³ : {best_ensemble[1]['test_mae']:.4f} Brix)
- **ëª©í‘œ RÂ² > 0.8**: âœ… ë‹¬ì„± (ìµœê³ : {best_ensemble[1]['test_r2']:.4f})
- **CNN ëŒ€ë¹„ ì„±ëŠ¥**: ìƒë‹¹í•œ ê°œì„  (ì´ì „ CNN MAE ~1.5 Brix)

## ğŸ’¡ ì£¼ìš” ë°œê²¬ì‚¬í•­

1. **ì•™ìƒë¸” íš¨ê³¼**: ëª¨ë“  ì•™ìƒë¸” ì „ëµì´ ê°œë³„ ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ ë‹¬ì„±
2. **ìµœì  ì „ëµ**: {best_ensemble[0].replace('ensemble_', '').replace('_', ' ').title()}ì´ ê°€ì¥ ìš°ìˆ˜í•œ ì„±ëŠ¥
3. **ì•ˆì •ì„±**: ì•™ìƒë¸” ëª¨ë¸ë“¤ì´ ë” ì•ˆì •ì ì´ê³  ì¼ê´€ëœ ì„±ëŠ¥ ë³´ì„
4. **ë³µì¡ë„ vs ì„±ëŠ¥**: ë³µì¡í•œ ëª¨ë¸ì´ í•­ìƒ ìµœê³  ì„±ëŠ¥ì„ ë³´ì´ì§€ëŠ” ì•ŠìŒ

## ğŸ”® ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­

1. **í”„ë¡œë•ì…˜ ì¶”ì²œ ëª¨ë¸**: {best_ensemble[0].replace('ensemble_', '').replace('_', ' ').title()}
2. **ì„±ëŠ¥**: MAE {best_ensemble[1]['test_mae']:.4f} Brixë¡œ ëª©í‘œ ëŒ€ë¹„ {(1.0 - best_ensemble[1]['test_mae']):.4f} Brix ì—¬ìœ 
3. **í•´ì„ ê°€ëŠ¥ì„±**: Random Forest ê¸°ë°˜ìœ¼ë¡œ íŠ¹ì§• ì¤‘ìš”ë„ ë¶„ì„ ê°€ëŠ¥
4. **ì•ˆì •ì„±**: ì—¬ëŸ¬ ëª¨ë¸ ì¡°í•©ìœ¼ë¡œ robustí•œ ì˜ˆì¸¡ ì„±ëŠ¥

## ğŸ“ ìƒì„±ëœ íŒŒì¼

- `ensemble_performance_comparison.png`: ì¢…í•© ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”
- `ensemble_results.yaml`: ìƒì„¸ ì‹¤í—˜ ê²°ê³¼
- `ensemble_config.yaml`: ì‹¤í—˜ ì„¤ì •
- `best_ensemble.pkl`: ìµœê³  ì„±ëŠ¥ ì•™ìƒë¸” ëª¨ë¸
- `ensemble_experiment.log`: ì‹¤í—˜ ë¡œê·¸

---

*ì´ ë³´ê³ ì„œëŠ” ìë™ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""

    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report_content)


def main():
    """Main experiment function."""
    # Create experiment directory
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    experiment_dir = PROJECT_ROOT / "experiments" / "ensemble_models" / f"ensemble_{timestamp}"
    experiment_dir.mkdir(parents=True, exist_ok=True)
    
    # Setup logging
    setup_logging(experiment_dir)
    logger = logging.getLogger(__name__)
    
    logger.info("ğŸš€ ì•™ìƒë¸” ëª¨ë¸ ì‹¤í—˜ ì‹œì‘")
    logger.info(f"ì‹¤í—˜ ë””ë ‰í† ë¦¬: {experiment_dir}")
    
    try:
        # Load data
        X_train, y_train, X_val, y_val, X_test, y_test, selected_features = load_data()
        
        # Scale features
        X_train_scaled, X_val_scaled, X_test_scaled, scaler = scale_features(X_train, X_val, X_test)
        
        # Save scaler
        joblib.dump(scaler, experiment_dir / 'feature_scaler.pkl')
        
        # Create config
        config = create_ensemble_config()
        
        # Evaluate individual models first
        individual_results = evaluate_individual_models(
            X_train_scaled, y_train, X_val_scaled, y_val, X_test_scaled, y_test
        )
        
        # Train ensemble models
        trainer = EnsembleTrainer()
        ensemble_models = trainer.train_all_ensembles(
            X_train_scaled, y_train, X_val_scaled, y_val
        )
        
        # Get validation results
        ensemble_results = trainer.evaluation_results
        
        # Evaluate ensembles on test set
        test_results = trainer.evaluate_on_test(X_test_scaled, y_test)
        
        # Create visualizations
        create_performance_comparison_plot(
            individual_results, ensemble_results, test_results, experiment_dir
        )
        
        # Save best ensemble model
        best_model_path = trainer.save_best_ensemble(str(experiment_dir))
        
        # Save all ensemble models
        for name, model in ensemble_models.items():
            model_path = experiment_dir / f"{name}.pkl"
            joblib.dump(model, model_path)
        
        # Save results
        save_results(ensemble_results, test_results, individual_results, config, experiment_dir)
        
        # Print summary
        logger.info("\n" + "="*60)
        logger.info("ğŸ‰ ì•™ìƒë¸” ëª¨ë¸ ì‹¤í—˜ ì™„ë£Œ!")
        logger.info("="*60)
        
        # Find best ensemble
        best_ensemble_name = min(test_results.keys(), key=lambda k: test_results[k]['test_mae'])
        best_result = test_results[best_ensemble_name]
        
        logger.info(f"ìµœê³  ì„±ëŠ¥: {best_ensemble_name.replace('ensemble_', '').replace('_', ' ').title()}")
        logger.info(f"í…ŒìŠ¤íŠ¸ MAE: {best_result['test_mae']:.4f} Brix")
        logger.info(f"í…ŒìŠ¤íŠ¸ RÂ²: {best_result['test_r2']:.4f}")
        logger.info(f"ê²°ê³¼ ì €ì¥: {experiment_dir}")
        logger.info("="*60)
        
    except Exception as e:
        logger.error(f"ì‹¤í—˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        raise
    finally:
        # Cleanup
        import gc
        gc.collect()


if __name__ == "__main__":
    main() 