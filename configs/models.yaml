# ğŸ‰ ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ - ì „í†µì ì¸ ML ëª¨ë¸ ì„¤ì •
# scikit-learn ê¸°ë°˜ GBT, SVM, Random Forest ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •

# =============================================================================
# ì „ì—­ ì„¤ì •
# =============================================================================
global:
  random_state: 42
  n_jobs: -1  # ëª¨ë“  CPU ì½”ì–´ ì‚¬ìš©
  verbose: 1

# =============================================================================
# ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ (í›ˆë ¨í•  ëª¨ë¸ë“¤)
# =============================================================================
models:
  gradient_boosting: true
  svm: true
  random_forest: true

# =============================================================================
# ë°ì´í„° ì „ì²˜ë¦¬ ì„¤ì •
# =============================================================================
preprocessing:
  # íŠ¹ì§• ìŠ¤ì¼€ì¼ë§ (SVMì— í•„ìˆ˜, ë‹¤ë¥¸ ëª¨ë¸ì—ë„ ë„ì›€)
  scaler_type: "standard"  # standard, minmax, robust
  feature_scaling:
    enabled: true
    method: "standard"  # standard, minmax, robust
    
  # íŠ¹ì§• ì„ íƒ (ì„ íƒì‚¬í•­)
  feature_selection:
    enabled: false
    method: "rfe"  # rfe, selectkbest, selectfrommodel
    n_features: 30

# =============================================================================
# êµì°¨ ê²€ì¦ ì„¤ì •
# =============================================================================
cross_validation:
  n_folds: 5
  cv_folds: 5
  scoring: "accuracy"  # ë¶„ë¥˜ ì •í™•ë„
  shuffle: true
  stratify: true  # ë¶„ë¥˜ ë¬¸ì œì´ë¯€ë¡œ true

# =============================================================================
# Gradient Boosting Trees (GBT) ì„¤ì •
# =============================================================================
gradient_boosting:
  # ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°
  base_params:
    n_estimators: 200
    learning_rate: 0.1
    max_depth: 6
    min_samples_split: 5
    min_samples_leaf: 2
    subsample: 0.8
    max_features: "sqrt"
    random_state: 42
    
  # ê³ ê¸‰ ì„¤ì •
  advanced:
    loss: "log_loss"  # ë¶„ë¥˜ìš©
    criterion: "friedman_mse"
    warm_start: false
    validation_fraction: 0.1
    n_iter_no_change: 10
    tol: 1e-4
    
  # í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ìš© ê·¸ë¦¬ë“œ (Phase 4ì—ì„œ ì‚¬ìš©)
  hyperparameter_grid:
    n_estimators: [100, 200, 300, 500]
    learning_rate: [0.01, 0.05, 0.1, 0.2]
    max_depth: [3, 4, 6, 8]
    min_samples_split: [2, 5, 10]
    min_samples_leaf: [1, 2, 4]
    subsample: [0.7, 0.8, 0.9, 1.0]
    max_features: ["sqrt", "log2", 0.3, 0.5, 0.7]

# =============================================================================
# Support Vector Machine (SVM) ì„¤ì •
# =============================================================================
svm:
  # ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°
  base_params:
    kernel: "rbf"
    C: 10.0
    gamma: "scale"
    epsilon: 0.1
    tol: 0.001
    cache_size: 200
    max_iter: -1
    
  # í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ìš© ê·¸ë¦¬ë“œ
  hyperparameter_grid:
    kernel: ["rbf", "poly", "sigmoid"]
    C: [0.1, 1, 10, 100, 1000]
    gamma: ["scale", "auto", 0.001, 0.01, 0.1, 1]
    epsilon: [0.01, 0.1, 0.2, 0.5]
    
  # ë‹¤í•­ì‹ ì»¤ë„ ì „ìš© ì„¤ì •
  poly_specific:
    degree: [2, 3, 4]
    coef0: [0, 1, 2]

# =============================================================================
# Random Forest ì„¤ì •
# =============================================================================
random_forest:
  # ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°
  base_params:
    n_estimators: 200
    max_depth: 10
    min_samples_split: 5
    min_samples_leaf: 2
    max_features: "sqrt"
    bootstrap: true
    oob_score: true
    random_state: 42
    
  # ê³ ê¸‰ ì„¤ì •
  advanced:
    criterion: "gini"  # ë¶„ë¥˜ìš©
    max_samples: null  # bootstrap ìƒ˜í”Œ ìˆ˜
    min_weight_fraction_leaf: 0.0
    max_leaf_nodes: null
    min_impurity_decrease: 0.0
    warm_start: false
    ccp_alpha: 0.0
    
  # í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ìš© ê·¸ë¦¬ë“œ
  hyperparameter_grid:
    n_estimators: [100, 200, 300, 500]
    max_depth: [3, 5, 10, 15, 20, null]
    min_samples_split: [2, 5, 10, 20]
    min_samples_leaf: [1, 2, 4, 8]
    max_features: ["sqrt", "log2", 0.3, 0.5, 0.7, 1.0]
    bootstrap: [true, false]

# =============================================================================
# ì•™ìƒë¸” ëª¨ë¸ ì„¤ì • (Phase 4ì—ì„œ ì‚¬ìš©)
# =============================================================================
ensemble:
  # Voting Regressor ì„¤ì •
  voting:
    voting: "soft"  # ë¶„ë¥˜ì˜ ê²½ìš°, íšŒê·€ì—ì„œëŠ” í‰ê· 
    weights: null  # ë™ì¼ ê°€ì¤‘ì¹˜, ë‚˜ì¤‘ì— ì„±ëŠ¥ ê¸°ë°˜ìœ¼ë¡œ ì¡°ì •
    
  # Stacking ì„¤ì •
  stacking:
    cv: 5
    final_estimator: "linear_regression"  # ë©”íƒ€ ëª¨ë¸
    passthrough: false
    
  # Bagging ì„¤ì •
  bagging:
    n_estimators: 10
    max_samples: 1.0
    max_features: 1.0
    bootstrap: true
    bootstrap_features: false

# =============================================================================
# í›ˆë ¨ ì„¤ì •
# =============================================================================
training:
  # ë°ì´í„° ë¶„í•  ë¹„ìœ¨ (ì´ë¯¸ ë¶„í• ë˜ì–´ ìˆì§€ë§Œ ì°¸ê³ ìš©)
  train_ratio: 0.7
  val_ratio: 0.15
  test_ratio: 0.15
  
  # ë°°ì¹˜ í¬ê¸° (ë©”ëª¨ë¦¬ ê´€ë¦¬ìš©)
  batch_size: 32
  
  # ì¡°ê¸° ì¤‘ë‹¨ (GBTì—ë§Œ ì ìš©)
  early_stopping:
    enabled: true
    patience: 10
    restore_best_weights: true

# =============================================================================
# í‰ê°€ ë©”íŠ¸ë¦­ ì„¤ì • (ë¶„ë¥˜)
# =============================================================================
evaluation:
  # ì£¼ìš” ë¶„ë¥˜ ë©”íŠ¸ë¦­
  primary_metrics:
    - "accuracy"      # ì •í™•ë„ (ì£¼ìš” ëª©í‘œ: > 90%)
    - "f1_score"      # F1-score (ëª©í‘œ: > 0.85)
    - "precision"     # ì •ë°€ë„ (ëª©í‘œ: > 0.85)
    - "recall"        # ì¬í˜„ìœ¨ (ëª©í‘œ: > 0.85)
    - "auc_roc"       # AUC-ROC (ëª©í‘œ: > 0.90)
    
  # ì¶”ê°€ ë©”íŠ¸ë¦­
  additional_metrics:
    - "balanced_accuracy"  # ê· í˜• ì •í™•ë„
    - "matthews_corrcoef"  # ë§¤íŠœ ìƒê´€ê³„ìˆ˜
    - "cohen_kappa"        # ì½”í—¨ ì¹´íŒŒ
    
  # ì‚¬ìš©ì ì •ì˜ ë©”íŠ¸ë¦­
  custom_metrics:
    class_balance: "check_class_balance"  # í´ë˜ìŠ¤ ê· í˜• í™•ì¸

# =============================================================================
# ì‹¤í—˜ ì¶”ì  ì„¤ì •
# =============================================================================
experiment:
  # ì‹¤í—˜ ì´ë¦„ ë° íƒœê·¸
  name: "watermelon_pitch_classification"
  tags: ["traditional_ml", "audio_features", "pitch_classification"]
  
  # ë¡œê¹… ì„¤ì •
  logging:
    level: "INFO"
    save_logs: true
    log_file: "experiments/training.log"
    
  # ëª¨ë¸ ì €ì¥ ì„¤ì •
  model_saving:
    save_best_only: true
    save_all_models: false
    model_dir: "models/saved"
    
  # ê²°ê³¼ ì €ì¥ ì„¤ì •
  results:
    save_predictions: true
    save_metrics: true
    save_feature_importance: true
    results_dir: "experiments"

# =============================================================================
# ì‹œê°í™” ì„¤ì •
# =============================================================================
visualization:
  # í”Œë¡¯ ìŠ¤íƒ€ì¼
  style: "seaborn-v0_8"
  figure_size: [10, 8]
  dpi: 300
  
  # ìƒ‰ìƒ íŒ”ë ˆíŠ¸
  color_palette: "husl"
  
  # ì €ì¥ í˜•ì‹
  save_formats: ["png", "pdf"]
  save_dir: "experiments/plots"
  
  # ìƒì„±í•  í”Œë¡¯ ìœ í˜•
  plot_types:
    - "performance_comparison"
    - "confusion_matrix"
    - "roc_curve"
    - "feature_importance"
    - "learning_curve"
    - "cross_validation_scores"

# =============================================================================
# ì„±ëŠ¥ ëª©í‘œ (ë¶„ë¥˜ ë¬¸ì œ)
# =============================================================================
performance:
  # ì£¼ìš” ëª©í‘œ (CNN ëª¨ë¸ ëŒ€ë¹„ ê°œì„ )
  target_accuracy: 0.90     # ì •í™•ë„ > 90%
  target_f1_score: 0.85     # F1-score > 0.85
  target_precision: 0.85    # Precision > 0.85
  target_recall: 0.85       # Recall > 0.85
  
  # ì¶”ê°€ ëª©í‘œ
  training_time_max: 600    # 10ë¶„ ì´ë‚´
  inference_time_max: 0.001 # 1ms ì´ë‚´
  
  # ë¶„ë¥˜ ì •í™•ë„ ëª©í‘œ
  target_auc_roc: 0.90      # AUC-ROC > 0.90

# =============================================================================
# í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì„¤ì • (Phase 4ìš©)
# =============================================================================
hyperparameter_tuning:
  # íŠœë‹ ë°©ë²•
  method: "grid_search"  # grid_search, random_search, bayesian
  
  # GridSearchCV ì„¤ì •
  grid_search:
    cv: 5
    scoring: "accuracy"
    n_jobs: -1
    verbose: 2
    return_train_score: true
    
  # RandomizedSearchCV ì„¤ì •
  random_search:
    n_iter: 100
    cv: 5
    scoring: "accuracy"
    n_jobs: -1
    verbose: 2
    random_state: 42
    
  # ë² ì´ì§€ì•ˆ ìµœì í™” ì„¤ì • (optuna ì‚¬ìš©ì‹œ)
  bayesian:
    n_trials: 100
    sampler: "TPE"
    pruner: "MedianPruner" 