"""
ğŸ‰ ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ - í›ˆë ¨ íŒŒì´í”„ë¼ì¸

ì „í†µì ì¸ ML ëª¨ë¸ë“¤ì˜ í†µí•© í›ˆë ¨ ê´€ë¦¬
- ë‹¤ì¤‘ ëª¨ë¸ ë™ì‹œ í›ˆë ¨
- ë°ì´í„° ë¡œë”© ë° ê²€ì¦
- êµì°¨ ê²€ì¦ ê´€ë¦¬
- í›ˆë ¨ ë¡œê¹… ë° ê²°ê³¼ ì¶”ì 
- ëª¨ë¸ ì €ì¥ ë° ë¹„êµ
"""

import os
import time
import yaml
import logging
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime
from pathlib import Path

from sklearn.model_selection import cross_validate
from sklearn.metrics import mean_absolute_error, r2_score, accuracy_score, f1_score, precision_score, recall_score

from ..models.traditional_ml import (
    BaseWatermelonModel, ModelFactory, load_config
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class TrainingResults:
    """
    í›ˆë ¨ ê²°ê³¼ë¥¼ ì €ì¥í•˜ê³  ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤
    """
    
    def __init__(self):
        self.results = {}
        self.best_model = None
        self.best_metric = 0.0  # F1-scoreëŠ” ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
        self.training_start_time: Optional[datetime] = None
        self.training_end_time: Optional[datetime] = None
    
    def add_model_result(self, model_name: str, result: Dict[str, Any]):
        """ëª¨ë¸ ê²°ê³¼ ì¶”ê°€"""
        self.results[model_name] = result
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì—…ë°ì´íŠ¸ (F1-score ê¸°ì¤€, ë¶„ë¥˜ ë¬¸ì œ)
        if 'val_f1_score' in result and result['val_f1_score'] > self.best_metric:
            self.best_metric = result['val_f1_score']
            self.best_model = model_name
    
    def get_summary(self) -> pd.DataFrame:
        """ê²°ê³¼ ìš”ì•½ì„ DataFrameìœ¼ë¡œ ë°˜í™˜"""
        if not self.results:
            return pd.DataFrame()
        
        summary_data = []
        for model_name, result in self.results.items():
            summary_data.append({
                'model': model_name,
                'train_mae': result.get('train_mae', np.nan),
                'val_mae': result.get('val_mae', np.nan),
                'test_mae': result.get('test_mae', np.nan),
                'train_r2': result.get('train_r2', np.nan),
                'val_r2': result.get('val_r2', np.nan),
                'test_r2': result.get('test_r2', np.nan),
                'training_time': result.get('training_time', np.nan),
                'cv_mae_mean': result.get('cv_mae_mean', np.nan),
                'cv_mae_std': result.get('cv_mae_std', np.nan)
            })
        
        return pd.DataFrame(summary_data)
    
    def save_results(self, save_dir: str):
        """ê²°ê³¼ ì €ì¥"""
        os.makedirs(save_dir, exist_ok=True)
        
        # ìš”ì•½ í…Œì´ë¸” ì €ì¥
        summary_df = self.get_summary()
        summary_path = os.path.join(save_dir, 'training_summary.csv')
        summary_df.to_csv(summary_path, index=False)
        
        # ì „ì²´ ê²°ê³¼ ì €ì¥
        results_path = os.path.join(save_dir, 'detailed_results.yaml')
        with open(results_path, 'w', encoding='utf-8') as f:
            yaml.dump(self.results, f, default_flow_style=False, allow_unicode=True)
        
        logger.info(f"Training results saved to {save_dir}")


class MLTrainer:
    """
    ì „í†µì ì¸ ML ëª¨ë¸ë“¤ì˜ í†µí•© í›ˆë ¨ ê´€ë¦¬ì
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None, log_file: Optional[str] = None):
        """
        í›ˆë ¨ì ì´ˆê¸°í™”
        
        Args:
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
            log_file: ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
        """
        self.config = config or load_config()
        self.models = {}
        self.results = TrainingResults()
        
        # ë¡œê¹… ì„¤ì •
        if log_file:
            self._setup_file_logging(log_file)
        
        # ëª¨ë¸ ìƒì„±
        self._create_models()
        
        logger.info("MLTrainer initialized successfully")
    
    def _setup_file_logging(self, log_file: str):
        """íŒŒì¼ ë¡œê¹… ì„¤ì •"""
        os.makedirs(os.path.dirname(log_file), exist_ok=True)
        
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(formatter)
        
        logger.addHandler(file_handler)
    
    def _create_models(self):
        """ì„¤ì •ì— ë”°ë¼ ëª¨ë¸ë“¤ ìƒì„±"""
        self.models = ModelFactory.create_all_models(self.config)
        logger.info(f"Created models: {list(self.models.keys())}")
    
    def load_data(self, train_path: str, val_path: str, test_path: str, 
                  target_column: str = 'sweetness') -> Tuple[Dict[str, np.ndarray], List[str]]:
        """
        ë°ì´í„° ë¡œë“œ ë° ê²€ì¦
        
        Args:
            train_path: í›ˆë ¨ ë°ì´í„° ê²½ë¡œ
            val_path: ê²€ì¦ ë°ì´í„° ê²½ë¡œ  
            test_path: í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ
            target_column: íƒ€ê²Ÿ ì»¬ëŸ¼ëª…
            
        Returns:
            ë°ì´í„° ë”•ì…”ë„ˆë¦¬ ë° íŠ¹ì§• ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        """
        logger.info("Loading training data...")
        
        # ë°ì´í„° ë¡œë“œ
        train_df = pd.read_csv(train_path)
        val_df = pd.read_csv(val_path)
        test_df = pd.read_csv(test_path)
        
        logger.info(f"Train: {train_df.shape}, Val: {val_df.shape}, Test: {test_df.shape}")
        
        # íŠ¹ì§•ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
        feature_columns = [col for col in train_df.columns if col != target_column]
        
        data = {
            'X_train': np.array(train_df[feature_columns].values),
            'y_train': np.array(train_df[target_column].values),
            'X_val': np.array(val_df[feature_columns].values),
            'y_val': np.array(val_df[target_column].values),
            'X_test': np.array(test_df[feature_columns].values),
            'y_test': np.array(test_df[target_column].values)
        }
        
        # ë°ì´í„° í’ˆì§ˆ ê²€ì¦
        self._validate_data(data)
        
        logger.info(f"Data loaded successfully. Features: {len(feature_columns)}")
        return data, feature_columns
    
    def _validate_data(self, data: Dict[str, np.ndarray]):
        """ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
        for name, array in data.items():
            if np.any(np.isnan(array)):
                raise ValueError(f"NaN values found in {name}")
            if np.any(np.isinf(array)):
                raise ValueError(f"Infinite values found in {name}")
        
        logger.info("Data validation passed âœ…")
    
    def train_single_model(self, model_name: str, data: Dict[str, np.ndarray], 
                          feature_names: List[str], perform_cv: bool = True) -> Dict[str, Any]:
        """
        ë‹¨ì¼ ëª¨ë¸ í›ˆë ¨
        
        Args:
            model_name: ëª¨ë¸ ì´ë¦„
            data: ë°ì´í„° ë”•ì…”ë„ˆë¦¬
            feature_names: íŠ¹ì§• ì´ë¦„ ë¦¬ìŠ¤íŠ¸
            perform_cv: êµì°¨ ê²€ì¦ ìˆ˜í–‰ ì—¬ë¶€
            
        Returns:
            í›ˆë ¨ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if model_name not in self.models:
            raise ValueError(f"Unknown model: {model_name}")
        
        model = self.models[model_name]
        logger.info(f"Training {model_name} model...")
        
        start_time = time.time()
        
        # ëª¨ë¸ í›ˆë ¨
        model.fit(
            data['X_train'], data['y_train'],
            data['X_val'], data['y_val']
        )
        
        training_time = time.time() - start_time
        
        # ì„±ëŠ¥ í‰ê°€
        train_metrics = model.evaluate(data['X_train'], data['y_train'])
        val_metrics = model.evaluate(data['X_val'], data['y_val'])
        test_metrics = model.evaluate(data['X_test'], data['y_test'])
        
        # ê²°ê³¼ ì •ë¦¬ (ë¶„ë¥˜ ë©”íŠ¸ë¦­)
        result = {
            'train_accuracy': train_metrics['accuracy'],
            'train_f1_score': train_metrics['f1_score'],
            'train_precision': train_metrics['precision'],
            'train_recall': train_metrics['recall'],
            'val_accuracy': val_metrics['accuracy'],
            'val_f1_score': val_metrics['f1_score'],
            'val_precision': val_metrics['precision'],
            'val_recall': val_metrics['recall'],
            'test_accuracy': test_metrics['accuracy'],
            'test_f1_score': test_metrics['f1_score'],
            'test_precision': test_metrics['precision'],
            'test_recall': test_metrics['recall'],
            'training_time': training_time,
            'feature_names': feature_names
        }
        
        # êµì°¨ ê²€ì¦
        if perform_cv:
            cv_config = self.config.get('cross_validation', {})
            cv_folds = cv_config.get('cv_folds', 5)
            
            cv_results = model.cross_validate(
                data['X_train'], data['y_train'], 
                cv=cv_folds
            )
            
            result.update({
                'cv_accuracy_mean': cv_results['test_accuracy_mean'],
                'cv_accuracy_std': cv_results['test_accuracy_std'],
                'cv_f1_score_mean': cv_results['test_f1_mean'],
                'cv_f1_score_std': cv_results['test_f1_std']
            })
        
        # íŠ¹ì§• ì¤‘ìš”ë„
        importance = model.get_feature_importance()
        if importance is not None:
            # ìƒìœ„ 10ê°œ ì¤‘ìš” íŠ¹ì§•
            top_indices = np.argsort(importance)[-10:]
            result['feature_importance'] = {
                'importance_values': importance.tolist(),
                'top_features': [feature_names[i] for i in top_indices],
                'top_importance': [importance[i] for i in top_indices]
            }
        
        logger.info(f"{model_name} training completed in {training_time:.2f}s. "
                   f"Val F1: {val_metrics['f1_score']:.3f}, Test F1: {test_metrics['f1_score']:.3f}")
        
        return result
    
    def train_all_models(self, data: Dict[str, np.ndarray], feature_names: List[str], 
                        model_subset: Optional[List[str]] = None, 
                        perform_cv: bool = True) -> TrainingResults:
        """
        ëª¨ë“  ëª¨ë¸ í›ˆë ¨
        
        Args:
            data: ë°ì´í„° ë”•ì…”ë„ˆë¦¬
            feature_names: íŠ¹ì§• ì´ë¦„ ë¦¬ìŠ¤íŠ¸
            model_subset: í›ˆë ¨í•  ëª¨ë¸ subset (Noneì´ë©´ ì „ì²´)
            perform_cv: êµì°¨ ê²€ì¦ ìˆ˜í–‰ ì—¬ë¶€
            
        Returns:
            í›ˆë ¨ ê²°ê³¼ ê°ì²´
        """
        self.results.training_start_time = datetime.now()
        
        models_to_train = model_subset or list(self.models.keys())
        total_models = len(models_to_train)
        
        logger.info(f"Starting training for {total_models} models...")
        
        for i, model_name in enumerate(models_to_train, 1):
            logger.info(f"[{i}/{total_models}] Training {model_name}...")
            
            try:
                result = self.train_single_model(
                    model_name, data, feature_names, perform_cv
                )
                self.results.add_model_result(model_name, result)
                
            except Exception as e:
                logger.error(f"Failed to train {model_name}: {e}")
                continue
        
        self.results.training_end_time = datetime.now()
        total_time = (self.results.training_end_time - self.results.training_start_time).total_seconds()
        
        logger.info(f"All models training completed in {total_time:.2f}s")
        logger.info(f"Best model: {self.results.best_model} (F1: {self.results.best_metric:.3f})")
        
        return self.results
    
    def save_models(self, save_dir: str, save_best_only: bool = False):
        """
        í›ˆë ¨ëœ ëª¨ë¸ë“¤ ì €ì¥
        
        Args:
            save_dir: ì €ì¥ ë””ë ‰í† ë¦¬
            save_best_only: ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë§Œ ì €ì¥ ì—¬ë¶€
        """
        os.makedirs(save_dir, exist_ok=True)
        
        if save_best_only and self.results.best_model:
            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë§Œ ì €ì¥
            model_name = self.results.best_model
            model = self.models[model_name]
            
            if model.is_fitted:
                model_path = os.path.join(save_dir, f"best_model_{model_name}.pkl")
                model.save_model(model_path)
                logger.info(f"Best model ({model_name}) saved to {model_path}")
        else:
            # ëª¨ë“  í›ˆë ¨ëœ ëª¨ë¸ ì €ì¥
            for model_name, model in self.models.items():
                if model.is_fitted:
                    model_path = os.path.join(save_dir, f"{model_name}_model.pkl")
                    model.save_model(model_path)
            
            logger.info(f"All trained models saved to {save_dir}")
    
    def get_model_comparison(self) -> pd.DataFrame:
        """ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ í…Œì´ë¸” ìƒì„±"""
        summary_df = self.results.get_summary()
        
        if summary_df.empty:
            return summary_df
        
        # ì„±ëŠ¥ ìˆœìœ„ ì¶”ê°€
        summary_df['mae_rank'] = summary_df['val_mae'].rank()
        summary_df['r2_rank'] = summary_df['val_r2'].rank(ascending=False)
        
        # ì •ë ¬
        summary_df = summary_df.sort_values('val_mae').reset_index(drop=True)
        
        return summary_df
    
    def print_summary(self):
        """í›ˆë ¨ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ‰ WATERMELON SWEETNESS PREDICTION - TRAINING SUMMARY")
        logger.info("="*60)
        
        if not self.results.results:
            logger.info("No training results available.")
            return
        
        summary_df = self.get_model_comparison()
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
        best_row = summary_df.iloc[0]
        logger.info(f"ğŸ† BEST MODEL: {best_row['model']}")
        logger.info(f"   Validation MAE: {best_row['val_mae']:.3f} Brix")
        logger.info(f"   Test MAE: {best_row['test_mae']:.3f} Brix")
        logger.info(f"   RÂ² Score: {best_row['val_r2']:.3f}")
        
        # ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
        target_mae = self.config.get('performance_targets', {}).get('mae_target', 1.0)
        target_r2 = self.config.get('performance_targets', {}).get('r2_target', 0.8)
        
        mae_achieved = best_row['test_mae'] < target_mae
        r2_achieved = best_row['val_r2'] > target_r2
        
        logger.info(f"\nğŸ“Š PERFORMANCE TARGETS:")
        logger.info(f"   MAE < {target_mae} Brix: {'âœ… ACHIEVED' if mae_achieved else 'âŒ NOT ACHIEVED'}")
        logger.info(f"   RÂ² > {target_r2}: {'âœ… ACHIEVED' if r2_achieved else 'âŒ NOT ACHIEVED'}")
        
        # ì „ì²´ ëª¨ë¸ ë¹„êµ
        logger.info(f"\nğŸ“ˆ ALL MODELS COMPARISON:")
        for _, row in summary_df.iterrows():
            logger.info(f"   {row['model']:15} | MAE: {row['val_mae']:.3f} | RÂ²: {row['val_r2']:.3f} | Time: {row['training_time']:.2f}s")
        
        logger.info("="*60)


def create_trainer_from_config(config_path: str = "configs/models.yaml", 
                              log_file: str = "experiments/training.log") -> MLTrainer:
    """
    ì„¤ì • íŒŒì¼ì—ì„œ í›ˆë ¨ì ìƒì„±
    
    Args:
        config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
        log_file: ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
        
    Returns:
        MLTrainer ì¸ìŠ¤í„´ìŠ¤
    """
    config = load_config(config_path)
    return MLTrainer(config, log_file)


def quick_train(train_path: str = "data/splits/full_dataset/train.csv",
                val_path: str = "data/splits/full_dataset/val.csv",
                test_path: str = "data/splits/full_dataset/test.csv",
                target_column: str = "sweetness",
                save_dir: str = "models/saved",
                results_dir: str = "experiments") -> TrainingResults:
    """
    ë¹ ë¥¸ í›ˆë ¨ ì‹¤í–‰ í•¨ìˆ˜
    
    Args:
        train_path: í›ˆë ¨ ë°ì´í„° ê²½ë¡œ
        val_path: ê²€ì¦ ë°ì´í„° ê²½ë¡œ
        test_path: í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ
        target_column: íƒ€ê²Ÿ ì»¬ëŸ¼ëª…
        save_dir: ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
        results_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        
    Returns:
        í›ˆë ¨ ê²°ê³¼
    """
    # í›ˆë ¨ì ìƒì„±
    trainer = create_trainer_from_config()
    
    # ë°ì´í„° ë¡œë“œ
    data, feature_names = trainer.load_data(
        train_path, val_path, test_path, target_column
    )
    
    # ëª¨ë“  ëª¨ë¸ í›ˆë ¨
    results = trainer.train_all_models(data, feature_names)
    
    # ê²°ê³¼ ì¶œë ¥
    trainer.print_summary()
    
    # ëª¨ë¸ ë° ê²°ê³¼ ì €ì¥
    trainer.save_models(save_dir, save_best_only=True)
    results.save_results(results_dir)
    
    return results


def test_trainer():
    """í›ˆë ¨ì í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸"""
    logger.info("Testing MLTrainer...")
    
    # ê°€ìƒ ë°ì´í„° ìƒì„±
    np.random.seed(42)
    n_samples = 100
    n_features = 51
    
    X = np.random.randn(n_samples, n_features)
    y = np.random.uniform(8.0, 13.0, n_samples)
    
    # ë°ì´í„° ë¶„í• 
    train_size = int(0.7 * n_samples)
    val_size = int(0.15 * n_samples)
    
    data = {
        'X_train': X[:train_size],
        'y_train': y[:train_size],
        'X_val': X[train_size:train_size+val_size],
        'y_val': y[train_size:train_size+val_size],
        'X_test': X[train_size+val_size:],
        'y_test': y[train_size+val_size:]
    }
    
    feature_names = [f"feature_{i}" for i in range(n_features)]
    
    # í›ˆë ¨ì ìƒì„±
    config = load_config()
    trainer = MLTrainer(config)
    
    # ëª¨ë“  ëª¨ë¸ í›ˆë ¨
    results = trainer.train_all_models(data, feature_names, perform_cv=False)
    
    # ê²°ê³¼ ì¶œë ¥
    trainer.print_summary()
    
    # ê²°ê³¼ ì €ì¥ í…ŒìŠ¤íŠ¸
    results.save_results("experiments/test")
    
    # ëª¨ë¸ ì €ì¥ í…ŒìŠ¤íŠ¸
    trainer.save_models("models/test", save_best_only=True)
    
    logger.info("MLTrainer test completed successfully! âœ…")


if __name__ == "__main__":
    test_trainer() 