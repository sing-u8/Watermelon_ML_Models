"""
ğŸ‰ ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ ML í”„ë¡œì íŠ¸ - ë°ì´í„°ì…‹ ë¹Œë” ëª¨ë“ˆ
DatasetBuilder í´ë˜ìŠ¤: ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•œ íŠ¹ì§• ì¶”ì¶œ ë° ë°ì´í„°ì…‹ êµ¬ì¶•
"""

import logging
import time
from typing import List, Dict, Optional, Union, Tuple
import numpy as np
import pandas as pd
from pathlib import Path
from tqdm import tqdm
import gc

from .audio_loader import AudioLoader
from .preprocessor import AudioPreprocessor
from .feature_extractor import AudioFeatureExtractor

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DatasetBuilder:
    """
    ì „ì²´ ë°ì´í„°ì…‹ì˜ íŠ¹ì§• ì¶”ì¶œ ë° êµ¬ì¶•ì„ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤
    
    ê¸°ëŠ¥:
    - ë©”íƒ€ë°ì´í„° CSV íŒŒì¼ ë¡œë“œ
    - ì˜¤ë””ì˜¤ íŒŒì¼ ì¼ê´„ ì²˜ë¦¬
    - íŠ¹ì§• ì¶”ì¶œ ë° ì €ì¥
    - ë°ì´í„° í’ˆì§ˆ ê²€ì¦
    """
    
    def __init__(self, config_path: Optional[Union[str, Path]] = None):
        """
        DatasetBuilder ì´ˆê¸°í™”
        
        Args:
            config_path (Optional[Union[str, Path]]): ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        self.config_path = config_path
        self.audio_loader = AudioLoader(sample_rate=16000, mono=True)
        self.preprocessor = AudioPreprocessor(config_path=config_path)
        self.feature_extractor = AudioFeatureExtractor(config_path=config_path)
        
        self.stats = {
            'total_files': 0,
            'processed_files': 0,
            'failed_files': 0,
            'total_processing_time': 0.0,
            'failed_file_list': []
        }
        
        logger.info("DatasetBuilder ì´ˆê¸°í™” ì™„ë£Œ")
    
    def load_metadata(self, metadata_path: Union[str, Path]) -> pd.DataFrame:
        """
        ë©”íƒ€ë°ì´í„° CSV íŒŒì¼ ë¡œë“œ
        
        Args:
            metadata_path (Union[str, Path]): ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ
            
        Returns:
            pd.DataFrame: ë©”íƒ€ë°ì´í„° DataFrame
        """
        metadata_path = Path(metadata_path)
        
        try:
            metadata_df = pd.read_csv(metadata_path)
            logger.info(f"ë©”íƒ€ë°ì´í„° ë¡œë“œ ì„±ê³µ: {len(metadata_df)}ê°œ íŒŒì¼")
            logger.info(f"ì»¬ëŸ¼: {list(metadata_df.columns)}")
            
            # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
            required_columns = ['file_path', 'sweetness']
            missing_columns = [col for col in required_columns if col not in metadata_df.columns]
            
            if missing_columns:
                raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_columns}")
            
            return metadata_df
            
        except Exception as e:
            logger.error(f"ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def process_single_file(self, file_path: Union[str, Path], 
                           sweetness: float) -> Tuple[Optional[np.ndarray], Dict]:
        """
        ë‹¨ì¼ ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ (ë¡œë”© -> ì „ì²˜ë¦¬ -> íŠ¹ì§• ì¶”ì¶œ)
        
        Args:
            file_path (Union[str, Path]): ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            sweetness (float): ë‹¹ë„ê°’
            
        Returns:
            Tuple[Optional[np.ndarray], Dict]: (íŠ¹ì§• ë²¡í„°, ì²˜ë¦¬ ì •ë³´)
        """
        processing_info = {
            'file_path': str(file_path),
            'sweetness': sweetness,
            'success': False,
            'error': None,
            'processing_time': 0.0,
            'audio_duration': 0.0,
            'feature_count': 0
        }
        
        start_time = time.time()
        
        try:
            # 1. ì˜¤ë””ì˜¤ ë¡œë”©
            audio_data, sample_rate = self.audio_loader.load_audio(file_path)
            processing_info['audio_duration'] = len(audio_data) / sample_rate
            
            # 2. ì „ì²˜ë¦¬
            processed_audio, preprocess_info = self.preprocessor.preprocess_audio(
                audio_data, sample_rate
            )
            
            # 3. íŠ¹ì§• ì¶”ì¶œ
            features = self.feature_extractor.extract_all_features(
                processed_audio, sample_rate
            )
            
            # 4. íŠ¹ì§• ê²€ì¦
            if np.any(np.isnan(features)) or np.any(np.isinf(features)):
                raise ValueError("NaN ë˜ëŠ” Inf ê°’ì´ í¬í•¨ëœ íŠ¹ì§• ë°œê²¬")
            
            processing_info['success'] = True
            processing_info['feature_count'] = len(features)
            processing_info['preprocess_info'] = preprocess_info
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del audio_data, processed_audio
            gc.collect()
            
            return features, processing_info
            
        except Exception as e:
            error_msg = f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"
            logger.warning(f"{file_path} - {error_msg}")
            processing_info['error'] = error_msg
            return None, processing_info
            
        finally:
            processing_info['processing_time'] = time.time() - start_time
    
    def build_dataset(self, metadata_path: Union[str, Path],
                     output_dir: Union[str, Path],
                     batch_size: int = 10) -> Dict:
        """
        ì „ì²´ ë°ì´í„°ì…‹ êµ¬ì¶•
        
        Args:
            metadata_path (Union[str, Path]): ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ
            output_dir (Union[str, Path]): ì¶œë ¥ ë””ë ‰í† ë¦¬
            batch_size (int): ë°°ì¹˜ í¬ê¸° (ë©”ëª¨ë¦¬ ê´€ë¦¬ìš©)
            
        Returns:
            Dict: ë°ì´í„°ì…‹ êµ¬ì¶• ê²°ê³¼ ì •ë³´
        """
        logger.info("ë°ì´í„°ì…‹ êµ¬ì¶• ì‹œì‘")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        metadata_df = self.load_metadata(metadata_path)
        self.stats['total_files'] = len(metadata_df)
        
        # ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
        all_features = []
        all_labels = []
        processing_results = []
        
        # íŠ¹ì§• ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
        feature_names = self.feature_extractor.get_feature_names()
        
        logger.info(f"ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜: {len(metadata_df)}")
        logger.info(f"ë°°ì¹˜ í¬ê¸°: {batch_size}")
        
        # ë°°ì¹˜ë³„ ì²˜ë¦¬
        for batch_start in tqdm(range(0, len(metadata_df), batch_size), 
                               desc="ë°ì´í„°ì…‹ êµ¬ì¶•"):
            batch_end = min(batch_start + batch_size, len(metadata_df))
            batch_df = metadata_df.iloc[batch_start:batch_end]
            
            batch_features = []
            batch_labels = []
            
            # ë°°ì¹˜ ë‚´ ê° íŒŒì¼ ì²˜ë¦¬
            for _, row in batch_df.iterrows():
                file_path = row['file_path']
                sweetness = row['sweetness']
                
                features, processing_info = self.process_single_file(file_path, sweetness)
                processing_results.append(processing_info)
                
                if features is not None:
                    batch_features.append(features)
                    batch_labels.append(sweetness)
                    self.stats['processed_files'] += 1
                else:
                    self.stats['failed_files'] += 1
                    self.stats['failed_file_list'].append(str(file_path))
            
            # ë°°ì¹˜ ê²°ê³¼ ì¶”ê°€
            if batch_features:
                all_features.extend(batch_features)
                all_labels.extend(batch_labels)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
            
            # ì§„í–‰ ìƒí™© ë¡œê·¸
            if batch_end % (batch_size * 5) == 0 or batch_end == len(metadata_df):
                success_rate = self.stats['processed_files'] / (batch_end) * 100
                logger.info(f"ì§„í–‰ë¥ : {batch_end}/{len(metadata_df)} "
                           f"({success_rate:.1f}% ì„±ê³µ)")
        
        # ê²°ê³¼ ì •ë¦¬
        if all_features:
            # NumPy ë°°ì—´ë¡œ ë³€í™˜
            feature_array = np.array(all_features)
            label_array = np.array(all_labels)
            
            logger.info(f"íŠ¹ì§• ë°°ì—´ í˜•íƒœ: {feature_array.shape}")
            logger.info(f"ë¼ë²¨ ë°°ì—´ í˜•íƒœ: {label_array.shape}")
            
            # DataFrame ìƒì„±
            feature_df = pd.DataFrame(feature_array, columns=feature_names)
            feature_df['sweetness'] = label_array
            
            # CSV íŒŒì¼ ì €ì¥
            features_csv_path = output_dir / "features.csv"
            feature_df.to_csv(features_csv_path, index=False)
            logger.info(f"íŠ¹ì§• ë°ì´í„° ì €ì¥: {features_csv_path}")
            
            # íŠ¹ì§• ì´ë¦„ ì €ì¥
            feature_names_path = output_dir / "feature_names.txt"
            with open(feature_names_path, 'w', encoding='utf-8') as f:
                for name in feature_names:
                    f.write(f"{name}\n")
            logger.info(f"íŠ¹ì§• ì´ë¦„ ì €ì¥: {feature_names_path}")
            
            # ì²˜ë¦¬ ê²°ê³¼ ì €ì¥
            processing_df = pd.DataFrame(processing_results)
            processing_csv_path = output_dir / "processing_results.csv"
            processing_df.to_csv(processing_csv_path, index=False)
            logger.info(f"ì²˜ë¦¬ ê²°ê³¼ ì €ì¥: {processing_csv_path}")
            
        else:
            logger.error("ì¶”ì¶œëœ íŠ¹ì§•ì´ ì—†ìŠµë‹ˆë‹¤!")
            feature_array = np.array([])
            label_array = np.array([])
        
        # ìµœì¢… í†µê³„ ê³„ì‚°
        total_time = sum(result['processing_time'] for result in processing_results)
        self.stats['total_processing_time'] = total_time
        
        build_summary = {
            'total_files': self.stats['total_files'],
            'processed_files': self.stats['processed_files'],
            'failed_files': self.stats['failed_files'],
            'success_rate': self.stats['processed_files'] / self.stats['total_files'] * 100,
            'total_processing_time': total_time,
            'avg_processing_time': total_time / self.stats['total_files'] if self.stats['total_files'] > 0 else 0,
            'feature_shape': feature_array.shape if all_features else (0, 0),
            'label_shape': label_array.shape if all_features else (0,),
            'output_files': {
                'features_csv': str(output_dir / "features.csv") if all_features else None,
                'feature_names_txt': str(output_dir / "feature_names.txt") if all_features else None,
                'processing_results_csv': str(output_dir / "processing_results.csv")
            },
            'failed_files': self.stats['failed_file_list']
        }
        
        logger.info("ë°ì´í„°ì…‹ êµ¬ì¶• ì™„ë£Œ")
        logger.info(f"ì„±ê³µë¥ : {build_summary['success_rate']:.1f}%")
        logger.info(f"ì´ ì²˜ë¦¬ ì‹œê°„: {build_summary['total_processing_time']:.1f}ì´ˆ")
        
        return build_summary
    
    def validate_dataset(self, features_csv_path: Union[str, Path]) -> Dict:
        """
        ìƒì„±ëœ ë°ì´í„°ì…‹ì˜ í’ˆì§ˆ ê²€ì¦
        
        Args:
            features_csv_path (Union[str, Path]): íŠ¹ì§• CSV íŒŒì¼ ê²½ë¡œ
            
        Returns:
            Dict: ê²€ì¦ ê²°ê³¼
        """
        logger.info("ë°ì´í„°ì…‹ í’ˆì§ˆ ê²€ì¦ ì‹œì‘")
        
        try:
            # ë°ì´í„° ë¡œë“œ
            df = pd.read_csv(features_csv_path)
            logger.info(f"ë°ì´í„° ë¡œë“œ: {df.shape}")
            
            # ê¸°ë³¸ ì •ë³´
            validation_result = {
                'shape': df.shape,
                'feature_count': df.shape[1] - 1,  # sweetness ì»¬ëŸ¼ ì œì™¸
                'sample_count': df.shape[0],
                'issues': []
            }
            
            # ê²°ì¸¡ê°’ í™•ì¸
            missing_values = df.isnull().sum().sum()
            validation_result['missing_values'] = missing_values
            if missing_values > 0:
                validation_result['issues'].append(f"ê²°ì¸¡ê°’ {missing_values}ê°œ ë°œê²¬")
            
            # ë¬´í•œê°’ í™•ì¸
            numeric_columns = df.select_dtypes(include=[np.number]).columns
            inf_values = np.isinf(df[numeric_columns]).sum().sum()
            validation_result['infinite_values'] = inf_values
            if inf_values > 0:
                validation_result['issues'].append(f"ë¬´í•œê°’ {inf_values}ê°œ ë°œê²¬")
            
            # ë‹¹ë„ê°’ ê²€ì¦
            if 'sweetness' in df.columns:
                sweetness_stats = {
                    'min': df['sweetness'].min(),
                    'max': df['sweetness'].max(),
                    'mean': df['sweetness'].mean(),
                    'std': df['sweetness'].std(),
                    'unique_count': df['sweetness'].nunique()
                }
                validation_result['sweetness_stats'] = sweetness_stats
                
                # ë‹¹ë„ê°’ ë²”ìœ„ í™•ì¸ (ì¼ë°˜ì ìœ¼ë¡œ 8-13 Brix)
                if sweetness_stats['min'] < 5 or sweetness_stats['max'] > 15:
                    validation_result['issues'].append(
                        f"ë¹„ì •ìƒì ì¸ ë‹¹ë„ ë²”ìœ„: {sweetness_stats['min']:.1f} - {sweetness_stats['max']:.1f}"
                    )
            
            # íŠ¹ì§•ê°’ ë¶„í¬ í™•ì¸
            feature_columns = [col for col in df.columns if col != 'sweetness']
            feature_stats = {
                'zero_variance_features': [],
                'high_variance_features': [],
                'skewed_features': []
            }
            
            for col in feature_columns:
                values = df[col]
                variance = values.var()
                
                # ë¶„ì‚°ì´ 0ì¸ íŠ¹ì§• (ìƒìˆ˜ íŠ¹ì§•)
                if variance == 0:
                    feature_stats['zero_variance_features'].append(col)
                
                # ë¶„ì‚°ì´ ë§¤ìš° í° íŠ¹ì§•
                elif variance > 1000:
                    feature_stats['high_variance_features'].append(col)
                
                # ì™œë„ê°€ ë†’ì€ íŠ¹ì§•
                skewness = abs(values.skew())
                if skewness > 3:
                    feature_stats['skewed_features'].append(col)
            
            validation_result['feature_stats'] = feature_stats
            
            # ìƒìˆ˜ íŠ¹ì§•ì— ëŒ€í•œ ê²½ê³ 
            if feature_stats['zero_variance_features']:
                validation_result['issues'].append(
                    f"ìƒìˆ˜ íŠ¹ì§• {len(feature_stats['zero_variance_features'])}ê°œ ë°œê²¬"
                )
            
            # ìƒê´€ê´€ê³„ê°€ ë†’ì€ íŠ¹ì§• ìŒ ì°¾ê¸°
            correlation_matrix = df[feature_columns].corr()
            high_corr_pairs = []
            
            for i in range(len(feature_columns)):
                for j in range(i+1, len(feature_columns)):
                    corr = abs(correlation_matrix.iloc[i, j])
                    if corr > 0.95:  # 95% ì´ìƒ ìƒê´€ê´€ê³„
                        high_corr_pairs.append((feature_columns[i], feature_columns[j], corr))
            
            validation_result['high_correlation_pairs'] = high_corr_pairs
            if high_corr_pairs:
                validation_result['issues'].append(
                    f"ë†’ì€ ìƒê´€ê´€ê³„ íŠ¹ì§• ìŒ {len(high_corr_pairs)}ê°œ ë°œê²¬"
                )
            
            # ì „ì²´ í’ˆì§ˆ ë“±ê¸‰
            issue_count = len(validation_result['issues'])
            if issue_count == 0:
                validation_result['quality_grade'] = 'excellent'
            elif issue_count <= 2:
                validation_result['quality_grade'] = 'good'
            elif issue_count <= 4:
                validation_result['quality_grade'] = 'fair'
            else:
                validation_result['quality_grade'] = 'poor'
            
            logger.info(f"ë°ì´í„°ì…‹ í’ˆì§ˆ ë“±ê¸‰: {validation_result['quality_grade']}")
            if validation_result['issues']:
                logger.warning(f"ë°œê²¬ëœ ì´ìŠˆ: {validation_result['issues']}")
            
            return validation_result
            
        except Exception as e:
            logger.error(f"ë°ì´í„°ì…‹ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def get_stats(self) -> Dict:
        """í†µê³„ ì •ë³´ ë°˜í™˜"""
        return self.stats.copy()
    
    def reset_stats(self):
        """í†µê³„ ì •ë³´ ì´ˆê¸°í™”"""
        self.stats = {
            'total_files': 0,
            'processed_files': 0,
            'failed_files': 0,
            'total_processing_time': 0.0,
            'failed_file_list': []
        }
        logger.info("DatasetBuilder í†µê³„ ì •ë³´ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def __repr__(self) -> str:
        return (f"DatasetBuilder(processed={self.stats['processed_files']}, "
                f"failed={self.stats['failed_files']})")


# í¸ì˜ í•¨ìˆ˜ë“¤
def build_watermelon_dataset(metadata_path: Union[str, Path],
                            output_dir: Union[str, Path],
                            config_path: Optional[Union[str, Path]] = None,
                            batch_size: int = 10) -> Dict:
    """
    ìˆ˜ë°• ë°ì´í„°ì…‹ êµ¬ì¶•ì„ ìœ„í•œ í¸ì˜ í•¨ìˆ˜
    
    Args:
        metadata_path (Union[str, Path]): ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ
        output_dir (Union[str, Path]): ì¶œë ¥ ë””ë ‰í† ë¦¬
        config_path (Optional[Union[str, Path]]): ì„¤ì • íŒŒì¼ ê²½ë¡œ
        batch_size (int): ë°°ì¹˜ í¬ê¸°
        
    Returns:
        Dict: êµ¬ì¶• ê²°ê³¼ ì •ë³´
    """
    builder = DatasetBuilder(config_path=config_path)
    return builder.build_dataset(metadata_path, output_dir, batch_size)


if __name__ == "__main__":
    # ì‚¬ìš© ì˜ˆì œ
    from pathlib import Path
    
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
    project_root = Path(__file__).parent.parent.parent
    
    # ê²½ë¡œ ì„¤ì •
    metadata_path = project_root / "data" / "metadata.csv"
    output_dir = project_root / "data" / "processed"
    config_path = project_root / "configs" / "preprocessing.yaml"
    
    if metadata_path.exists():
        print(f"\nğŸ—ï¸ DatasetBuilder í…ŒìŠ¤íŠ¸")
        print(f"ë©”íƒ€ë°ì´í„°: {metadata_path}")
        print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
        
        # DatasetBuilder ìƒì„±
        builder = DatasetBuilder(config_path=config_path)
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ í…ŒìŠ¤íŠ¸
        try:
            metadata_df = builder.load_metadata(metadata_path)
            print(f"ë©”íƒ€ë°ì´í„° ë¡œë“œ ì„±ê³µ: {len(metadata_df)}ê°œ íŒŒì¼")
            print(f"ì»¬ëŸ¼: {list(metadata_df.columns)}")
            
            # ì²˜ìŒ ëª‡ ê°œ íŒŒì¼ë§Œ í…ŒìŠ¤íŠ¸
            test_metadata = metadata_df.head(5)  # ì²˜ìŒ 5ê°œë§Œ
            test_output_dir = output_dir / "test"
            
            print(f"\ní…ŒìŠ¤íŠ¸ ì‹¤í–‰: {len(test_metadata)}ê°œ íŒŒì¼")
            
            # ì„ì‹œ ë©”íƒ€ë°ì´í„° ì €ì¥
            test_metadata_path = test_output_dir / "test_metadata.csv"
            test_output_dir.mkdir(parents=True, exist_ok=True)
            test_metadata.to_csv(test_metadata_path, index=False)
            
            # ë°ì´í„°ì…‹ êµ¬ì¶• ì‹¤í–‰
            result = builder.build_dataset(
                metadata_path=test_metadata_path,
                output_dir=test_output_dir,
                batch_size=2
            )
            
            print(f"\nêµ¬ì¶• ê²°ê³¼:")
            print(f"  - ì„±ê³µë¥ : {result['success_rate']:.1f}%")
            print(f"  - ì²˜ë¦¬ ì‹œê°„: {result['total_processing_time']:.1f}ì´ˆ")
            print(f"  - íŠ¹ì§• í˜•íƒœ: {result['feature_shape']}")
            
            if result['processed_files'] > 0:
                # ë°ì´í„°ì…‹ ê²€ì¦
                features_csv = test_output_dir / "features.csv"
                if features_csv.exists():
                    validation_result = builder.validate_dataset(features_csv)
                    print(f"  - ë°ì´í„°ì…‹ í’ˆì§ˆ: {validation_result['quality_grade']}")
                    if validation_result['issues']:
                        print(f"  - ì´ìŠˆ: {validation_result['issues']}")
            
            # í†µê³„ ì •ë³´
            stats = builder.get_stats()
            print(f"\nBuilder í†µê³„: {stats}")
            
        except Exception as e:
            print(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    else:
        print(f"ë©”íƒ€ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {metadata_path}") 