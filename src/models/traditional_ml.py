"""
ğŸ‰ ìˆ˜ë°• ìŒ ë†’ë‚®ì´ ë¶„ë¥˜ - ì „í†µì ì¸ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í´ë˜ìŠ¤

scikit-learn ê¸°ë°˜ GBT, SVM, Random Forest ë¶„ë¥˜ ëª¨ë¸ êµ¬í˜„
- ê³µí†µ ì¸í„°í˜ì´ìŠ¤ ì œê³µ
- íŠ¹ì§• ìŠ¤ì¼€ì¼ë§ í†µí•©
- êµì°¨ ê²€ì¦ ë° ì„±ëŠ¥ í‰ê°€
- íŠ¹ì§• ì¤‘ìš”ë„ ë¶„ì„
- ëª¨ë¸ ì €ì¥/ë¡œë“œ ê¸°ëŠ¥
"""

import os
import yaml
import pickle
import joblib
import logging
import numpy as np
import pandas as pd
from abc import ABC, abstractmethod
from typing import Dict, List, Tuple, Optional, Union, Any
from pathlib import Path

from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.model_selection import cross_val_score, cross_validate
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.metrics import (
    accuracy_score, f1_score, precision_score, recall_score,
    roc_auc_score, confusion_matrix, classification_report
)
import warnings
warnings.filterwarnings('ignore')

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class BaseWatermelonModel(BaseEstimator, ClassifierMixin, ABC):
    """
    ìˆ˜ë°• ìŒ ë†’ë‚®ì´ ë¶„ë¥˜ ëª¨ë¸ì„ ìœ„í•œ ì¶”ìƒ ë² ì´ìŠ¤ í´ë˜ìŠ¤
    
    ëª¨ë“  ì „í†µì ì¸ ML ë¶„ë¥˜ ëª¨ë¸ì´ ê³µí†µìœ¼ë¡œ êµ¬í˜„í•´ì•¼ í•˜ëŠ” ì¸í„°í˜ì´ìŠ¤ ì •ì˜
    scikit-learnì˜ BaseEstimatorì™€ ClassifierMixinì„ ìƒì†ë°›ì•„ 
    GridSearchCV/RandomizedSearchCVì™€ í˜¸í™˜ë©ë‹ˆë‹¤.
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None, model_name: str = "base_model"):
        """
        ë² ì´ìŠ¤ ëª¨ë¸ ì´ˆê¸°í™”
        
        Args:
            config: ëª¨ë¸ ì„¤ì • ë”•ì…”ë„ˆë¦¬
            model_name: ëª¨ë¸ ì´ë¦„
        """
        self.model_name = model_name
        self.config = config or {}
        self.model = None
        self.scaler = None
        self.is_fitted = False
        self.feature_names = None
        self.training_history = {}
        self.classes_ = None
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì´ˆê¸°í™”
        self._init_scaler()
        
    def _init_scaler(self):
        """íŠ¹ì§• ìŠ¤ì¼€ì¼ë§ì„ ìœ„í•œ ìŠ¤ì¼€ì¼ëŸ¬ ì´ˆê¸°í™”"""
        scaler_type = self.config.get('preprocessing', {}).get('feature_scaling', {}).get('method', 'standard')
        
        if scaler_type == 'standard':
            self.scaler = StandardScaler()
        elif scaler_type == 'minmax':
            self.scaler = MinMaxScaler()
        elif scaler_type == 'robust':
            self.scaler = RobustScaler()
        else:
            logger.warning(f"Unknown scaler type: {scaler_type}. Using StandardScaler.")
            self.scaler = StandardScaler()
    
    @abstractmethod
    def _create_model(self) -> Any:
        """ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        pass
    
    def fit(self, X: np.ndarray, y: np.ndarray, X_val: Optional[np.ndarray] = None, y_val: Optional[np.ndarray] = None):
        """
        ëª¨ë¸ í›ˆë ¨
        
        Args:
            X: í›ˆë ¨ íŠ¹ì§•
            y: í›ˆë ¨ íƒ€ê²Ÿ (0: ë‚®ìŒ, 1: ë†’ìŒ)
            X_val: ê²€ì¦ íŠ¹ì§• (ì„ íƒì‚¬í•­)
            y_val: ê²€ì¦ íƒ€ê²Ÿ (ì„ íƒì‚¬í•­)
        """
        logger.info(f"Training {self.model_name} model...")
        
        # ì…ë ¥ ê²€ì¦
        X, y = self._validate_input(X, y)
        
        # í´ë˜ìŠ¤ ì •ë³´ ì €ì¥
        self.classes_ = np.unique(y)
        
        # íŠ¹ì§• ìŠ¤ì¼€ì¼ë§
        X_scaled = self._fit_transform_features(X)
        
        # ëª¨ë¸ ìƒì„± ë° í›ˆë ¨
        self.model = self._create_model()
        self.model.fit(X_scaled, y)
        
        # í›ˆë ¨ ê¸°ë¡
        self.training_history['train_score'] = self.model.score(X_scaled, y)
        
        if X_val is not None and y_val is not None:
            if self.scaler is not None:
                X_val_scaled = self.scaler.transform(X_val)
                self.training_history['val_score'] = self.model.score(X_val_scaled, y_val)
        
        self.is_fitted = True
        logger.info(f"{self.model_name} training completed.")
        
        return self
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        ë¶„ë¥˜ ì˜ˆì¸¡ ìˆ˜í–‰
        
        Args:
            X: ì˜ˆì¸¡í•  íŠ¹ì§•
            
        Returns:
            ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ (0: ë‚®ìŒ, 1: ë†’ìŒ)
        """
        if not self.is_fitted:
            raise ValueError("Model must be fitted before prediction.")
        
        if self.model is None:
            raise ValueError("Model is not initialized.")
        
        if self.scaler is None:
            raise ValueError("Scaler is not initialized.")
        
        X = self._validate_input_single(X)
        X_scaled = self.scaler.transform(X)
        predictions = self.model.predict(X_scaled)
        
        return predictions
    
    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        """
        ë¶„ë¥˜ í™•ë¥  ì˜ˆì¸¡ ìˆ˜í–‰
        
        Args:
            X: ì˜ˆì¸¡í•  íŠ¹ì§•
            
        Returns:
            ê° í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥ 
        """
        if not self.is_fitted:
            raise ValueError("Model must be fitted before prediction.")
        
        if self.model is None:
            raise ValueError("Model is not initialized.")
        
        if self.scaler is None:
            raise ValueError("Scaler is not initialized.")
        
        X = self._validate_input_single(X)
        X_scaled = self.scaler.transform(X)
        probabilities = self.model.predict_proba(X_scaled)
        
        return probabilities
    
    def cross_validate(self, X: np.ndarray, y: np.ndarray, cv: int = 5, 
                      scoring: str = 'accuracy') -> Dict[str, Any]:
        """
        êµì°¨ ê²€ì¦ ìˆ˜í–‰
        
        Args:
            X: íŠ¹ì§•
            y: íƒ€ê²Ÿ (0: ë‚®ìŒ, 1: ë†’ìŒ)
            cv: êµì°¨ ê²€ì¦ í´ë“œ ìˆ˜
            scoring: í‰ê°€ ì§€í‘œ
            
        Returns:
            êµì°¨ ê²€ì¦ ê²°ê³¼
        """
        logger.info(f"Performing {cv}-fold cross validation for {self.model_name}...")
        
        X, y = self._validate_input(X, y)
        X_scaled = self._fit_transform_features(X)
        
        # ëª¨ë¸ ìƒì„±
        model = self._create_model()
        
        # êµì°¨ ê²€ì¦ ì‹¤í–‰
        cv_results = cross_validate(
            model, X_scaled, y,
            cv=cv,
            scoring=['accuracy', 'f1', 'precision', 'recall', 'roc_auc'],
            return_train_score=True,
            n_jobs=-1
        )
        
        # ê²°ê³¼ ì •ë¦¬
        results = {
            'test_accuracy': cv_results['test_accuracy'],
            'test_f1': cv_results['test_f1'],
            'test_precision': cv_results['test_precision'],
            'test_recall': cv_results['test_recall'],
            'test_roc_auc': cv_results['test_roc_auc'],
            'train_accuracy': cv_results['train_accuracy'],
            'train_f1': cv_results['train_f1'],
            'train_precision': cv_results['train_precision'],
            'train_recall': cv_results['train_recall'],
            'train_roc_auc': cv_results['train_roc_auc'],
        }
        
        # í†µê³„ ìš”ì•½
        for metric in ['test_accuracy', 'test_f1', 'test_precision', 'test_recall', 'test_roc_auc',
                      'train_accuracy', 'train_f1', 'train_precision', 'train_recall', 'train_roc_auc']:
            values = results[metric]
            results[f'{metric}_mean'] = np.mean(values)
            results[f'{metric}_std'] = np.std(values)
        
        logger.info(f"Cross validation completed. Test Accuracy: {results['test_accuracy_mean']:.3f} Â± {results['test_accuracy_std']:.3f}")
        
        return results
    
    def evaluate(self, X: np.ndarray, y: np.ndarray) -> Dict[str, float]:
        """
        ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
        
        Args:
            X: í…ŒìŠ¤íŠ¸ íŠ¹ì§•
            y: í…ŒìŠ¤íŠ¸ íƒ€ê²Ÿ (0: ë‚®ìŒ, 1: ë†’ìŒ)
            
        Returns:
            í‰ê°€ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
        """
        if not self.is_fitted:
            raise ValueError("Model must be fitted before evaluation.")
        
        X, y = self._validate_input(X, y)
        predictions = self.predict(X)
        probabilities = self.predict_proba(X)
        
        # ë¶„ë¥˜ ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = {
            'accuracy': accuracy_score(y, predictions),
            'f1_score': f1_score(y, predictions),
            'precision': precision_score(y, predictions),
            'recall': recall_score(y, predictions),
            'roc_auc': roc_auc_score(y, probabilities[:, 1]),
        }
        
        # í˜¼ë™ í–‰ë ¬
        cm = confusion_matrix(y, predictions)
        metrics['confusion_matrix'] = cm
        
        # ë¶„ë¥˜ ë¦¬í¬íŠ¸
        report = classification_report(y, predictions, output_dict=True)
        metrics['classification_report'] = report
        
        return metrics
    
    def get_feature_importance(self) -> Optional[np.ndarray]:
        """
        íŠ¹ì§• ì¤‘ìš”ë„ ë°˜í™˜
        
        Returns:
            íŠ¹ì§• ì¤‘ìš”ë„ ë°°ì—´
        """
        if not self.is_fitted or self.model is None:
            return None
        
        if hasattr(self.model, 'feature_importances_'):
            return self.model.feature_importances_
        else:
            logger.warning(f"{self.model_name} does not support feature importance.")
            return None
    
    def save_model(self, filepath: str):
        """
        ëª¨ë¸ ì €ì¥
        
        Args:
            filepath: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
        """
        if not self.is_fitted:
            raise ValueError("Model must be fitted before saving.")
        
        # ì €ì¥í•  ê°ì²´ ì¤€ë¹„
        save_data = {
            'model': self.model,
            'scaler': self.scaler,
            'feature_names': self.feature_names,
            'classes_': self.classes_,
            'config': self.config,
            'model_name': self.model_name,
            'training_history': self.training_history
        }
        
        # ëª¨ë¸ ì €ì¥
        joblib.dump(save_data, filepath)
        logger.info(f"Model saved to {filepath}")
    
    @classmethod
    def load_model(cls, filepath: str):
        """
        ëª¨ë¸ ë¡œë“œ
        
        Args:
            filepath: ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ë¡œë“œëœ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
        """
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"Model file not found: {filepath}")
        
        # ëª¨ë¸ ë¡œë“œ
        save_data = joblib.load(filepath)
        
        # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        model_instance = cls(config=save_data.get('config'))
        model_instance.model = save_data['model']
        model_instance.scaler = save_data['scaler']
        model_instance.feature_names = save_data.get('feature_names')
        model_instance.classes_ = save_data.get('classes_')
        model_instance.training_history = save_data.get('training_history', {})
        model_instance.is_fitted = True
        
        logger.info(f"Model loaded from {filepath}")
        return model_instance
    
    def _validate_input(self, X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """ì…ë ¥ ë°ì´í„° ê²€ì¦"""
        if X is None or y is None:
            raise ValueError("X and y cannot be None")
        
        if len(X) != len(y):
            raise ValueError("X and y must have the same length")
        
        if len(X) == 0:
            raise ValueError("X and y cannot be empty")
        
        # í´ë˜ìŠ¤ ê²€ì¦
        unique_classes = np.unique(y)
        if len(unique_classes) != 2:
            raise ValueError(f"Expected 2 classes, got {len(unique_classes)}")
        
        if not np.array_equal(unique_classes, [0, 1]):
            raise ValueError("Classes must be 0 and 1")
        
        return X, y
    
    def _validate_input_single(self, X: np.ndarray) -> np.ndarray:
        """ë‹¨ì¼ ì˜ˆì¸¡ì„ ìœ„í•œ ì…ë ¥ ê²€ì¦"""
        if X is None:
            raise ValueError("X cannot be None")
        
        if len(X.shape) == 1:
            X = X.reshape(1, -1)
        
        return X
    
    def _fit_transform_features(self, X: np.ndarray) -> np.ndarray:
        """íŠ¹ì§• ìŠ¤ì¼€ì¼ë§ ìˆ˜í–‰"""
        if self.scaler is not None:
            return self.scaler.fit_transform(X)
        return X
    
    def __str__(self) -> str:
        return f"{self.model_name} (fitted: {self.is_fitted})"
    
    def __repr__(self) -> str:
        return self.__str__()


class WatermelonGBT(BaseWatermelonModel):
    """Gradient Boosting Classifier for watermelon pitch classification"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None, random_state: int = 42):
        super().__init__(config, "WatermelonGBT")
        self.random_state = random_state
    
    def _create_model(self) -> GradientBoostingClassifier:
        """Gradient Boosting Classifier ìƒì„±"""
        gbt_config = self.config.get('gradient_boosting', {})
        
        return GradientBoostingClassifier(
            n_estimators=gbt_config.get('n_estimators', 100),
            learning_rate=gbt_config.get('learning_rate', 0.1),
            max_depth=gbt_config.get('max_depth', 3),
            min_samples_split=gbt_config.get('min_samples_split', 2),
            min_samples_leaf=gbt_config.get('min_samples_leaf', 1),
            subsample=gbt_config.get('subsample', 1.0),
            random_state=self.random_state
        )


class WatermelonSVM(BaseWatermelonModel):
    """Support Vector Classifier for watermelon pitch classification"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None, random_state: int = 42):
        super().__init__(config, "WatermelonSVM")
        self.random_state = random_state
    
    def _create_model(self) -> SVC:
        """Support Vector Classifier ìƒì„±"""
        svm_config = self.config.get('svm', {})
        
        return SVC(
            kernel=svm_config.get('kernel', 'rbf'),
            C=svm_config.get('C', 1.0),
            gamma=svm_config.get('gamma', 'scale'),
            probability=True,  # í™•ë¥  ì˜ˆì¸¡ì„ ìœ„í•´ í•„ìš”
            random_state=self.random_state
        )


class WatermelonRandomForest(BaseWatermelonModel):
    """Random Forest Classifier for watermelon pitch classification"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None, random_state: int = 42):
        super().__init__(config, "WatermelonRandomForest")
        self.random_state = random_state
    
    def _create_model(self) -> RandomForestClassifier:
        """Random Forest Classifier ìƒì„±"""
        rf_config = self.config.get('random_forest', {})
        
        return RandomForestClassifier(
            n_estimators=rf_config.get('n_estimators', 100),
            max_depth=rf_config.get('max_depth', None),
            min_samples_split=rf_config.get('min_samples_split', 2),
            min_samples_leaf=rf_config.get('min_samples_leaf', 1),
            max_features=rf_config.get('max_features', 'sqrt'),
            random_state=self.random_state
        )


class ModelFactory:
    """ëª¨ë¸ íŒ©í† ë¦¬ í´ë˜ìŠ¤"""
    
    @staticmethod
    def create_model(model_type: str, config: Optional[Dict[str, Any]] = None) -> BaseWatermelonModel:
        """
        ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        
        Args:
            model_type: ëª¨ë¸ íƒ€ì… ('gbt', 'svm', 'rf')
            config: ëª¨ë¸ ì„¤ì •
            
        Returns:
            ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
        """
        model_type = model_type.lower()
        
        if model_type == 'gbt':
            return WatermelonGBT(config)
        elif model_type == 'svm':
            return WatermelonSVM(config)
        elif model_type == 'rf':
            return WatermelonRandomForest(config)
        else:
            raise ValueError(f"Unknown model type: {model_type}")
    
    @staticmethod
    def create_all_models(config: Optional[Dict[str, Any]] = None) -> Dict[str, BaseWatermelonModel]:
        """
        ëª¨ë“  ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        
        Args:
            config: ëª¨ë¸ ì„¤ì •
            
        Returns:
            ëª¨ë¸ ë”•ì…”ë„ˆë¦¬
        """
        models = {
            'gbt': WatermelonGBT(config),
            'svm': WatermelonSVM(config),
            'rf': WatermelonRandomForest(config)
        }
        return models


def load_config(config_path: str = "configs/models.yaml") -> Dict[str, Any]:
    """
    ì„¤ì • íŒŒì¼ ë¡œë“œ
    
    Args:
        config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
        
    Returns:
        ì„¤ì • ë”•ì…”ë„ˆë¦¬
    """
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        logger.info(f"Configuration loaded from {config_path}")
        return config
    except FileNotFoundError:
        logger.warning(f"Configuration file not found: {config_path}. Using default settings.")
        return {}
    except yaml.YAMLError as e:
        logger.error(f"Error parsing configuration file: {e}")
        return {}


def test_models():
    """ëª¨ë¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    np.random.seed(42)
    X = np.random.randn(100, 10)
    y = np.random.randint(0, 2, 100)
    
    # ì„¤ì • ë¡œë“œ
    config = load_config()
    
    # ëª¨ë“  ëª¨ë¸ í…ŒìŠ¤íŠ¸
    models = ModelFactory.create_all_models(config)
    
    for name, model in models.items():
        print(f"\nTesting {name}...")
        
        # êµì°¨ ê²€ì¦
        cv_results = model.cross_validate(X, y, cv=3)
        print(f"CV Accuracy: {cv_results['test_accuracy_mean']:.3f} Â± {cv_results['test_accuracy_std']:.3f}")
        
        # í›ˆë ¨ ë° í‰ê°€
        model.fit(X, y)
        predictions = model.predict(X)
        probabilities = model.predict_proba(X)
        
        print(f"Training Accuracy: {accuracy_score(y, predictions):.3f}")
        print(f"F1 Score: {f1_score(y, predictions):.3f}")
        print(f"ROC AUC: {roc_auc_score(y, probabilities[:, 1]):.3f}")


if __name__ == "__main__":
    test_models() 