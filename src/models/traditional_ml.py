"""
ğŸ‰ ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ - ì „í†µì ì¸ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í´ë˜ìŠ¤

scikit-learn ê¸°ë°˜ GBT, SVM, Random Forest ëª¨ë¸ êµ¬í˜„
- ê³µí†µ ì¸í„°í˜ì´ìŠ¤ ì œê³µ
- íŠ¹ì§• ìŠ¤ì¼€ì¼ë§ í†µí•©
- êµì°¨ ê²€ì¦ ë° ì„±ëŠ¥ í‰ê°€
- íŠ¹ì§• ì¤‘ìš”ë„ ë¶„ì„
- ëª¨ë¸ ì €ì¥/ë¡œë“œ ê¸°ëŠ¥
"""

import os
import yaml
import pickle
import joblib
import logging
import numpy as np
import pandas as pd
from abc import ABC, abstractmethod
from typing import Dict, List, Tuple, Optional, Union, Any
from pathlib import Path

from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.model_selection import cross_val_score, cross_validate
from sklearn.base import BaseEstimator, RegressorMixin
from sklearn.metrics import (
    mean_absolute_error, mean_squared_error, r2_score,
    mean_absolute_percentage_error, median_absolute_error,
    max_error, explained_variance_score
)
import warnings
warnings.filterwarnings('ignore')

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class BaseWatermelonModel(BaseEstimator, RegressorMixin, ABC):
    """
    ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ ëª¨ë¸ì„ ìœ„í•œ ì¶”ìƒ ë² ì´ìŠ¤ í´ë˜ìŠ¤
    
    ëª¨ë“  ì „í†µì ì¸ ML ëª¨ë¸ì´ ê³µí†µìœ¼ë¡œ êµ¬í˜„í•´ì•¼ í•˜ëŠ” ì¸í„°í˜ì´ìŠ¤ ì •ì˜
    scikit-learnì˜ BaseEstimatorì™€ RegressorMixinì„ ìƒì†ë°›ì•„ 
    GridSearchCV/RandomizedSearchCVì™€ í˜¸í™˜ë©ë‹ˆë‹¤.
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None, model_name: str = "base_model"):
        """
        ë² ì´ìŠ¤ ëª¨ë¸ ì´ˆê¸°í™”
        
        Args:
            config: ëª¨ë¸ ì„¤ì • ë”•ì…”ë„ˆë¦¬
            model_name: ëª¨ë¸ ì´ë¦„
        """
        self.model_name = model_name
        self.config = config or {}
        self.model = None
        self.scaler = None
        self.is_fitted = False
        self.feature_names = None
        self.training_history = {}
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì´ˆê¸°í™”
        self._init_scaler()
        
    def _init_scaler(self):
        """íŠ¹ì§• ìŠ¤ì¼€ì¼ë§ì„ ìœ„í•œ ìŠ¤ì¼€ì¼ëŸ¬ ì´ˆê¸°í™”"""
        scaler_type = self.config.get('preprocessing', {}).get('feature_scaling', {}).get('method', 'standard')
        
        if scaler_type == 'standard':
            self.scaler = StandardScaler()
        elif scaler_type == 'minmax':
            self.scaler = MinMaxScaler()
        elif scaler_type == 'robust':
            self.scaler = RobustScaler()
        else:
            logger.warning(f"Unknown scaler type: {scaler_type}. Using StandardScaler.")
            self.scaler = StandardScaler()
    
    @abstractmethod
    def _create_model(self) -> Any:
        """ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        pass
    
    def fit(self, X: np.ndarray, y: np.ndarray, X_val: Optional[np.ndarray] = None, y_val: Optional[np.ndarray] = None):
        """
        ëª¨ë¸ í›ˆë ¨
        
        Args:
            X: í›ˆë ¨ íŠ¹ì§•
            y: í›ˆë ¨ íƒ€ê²Ÿ
            X_val: ê²€ì¦ íŠ¹ì§• (ì„ íƒì‚¬í•­)
            y_val: ê²€ì¦ íƒ€ê²Ÿ (ì„ íƒì‚¬í•­)
        """
        logger.info(f"Training {self.model_name} model...")
        
        # ì…ë ¥ ê²€ì¦
        X, y = self._validate_input(X, y)
        
        # íŠ¹ì§• ìŠ¤ì¼€ì¼ë§
        X_scaled = self._fit_transform_features(X)
        
        # ëª¨ë¸ ìƒì„± ë° í›ˆë ¨
        self.model = self._create_model()
        self.model.fit(X_scaled, y)
        
        # í›ˆë ¨ ê¸°ë¡
        self.training_history['train_score'] = self.model.score(X_scaled, y)
        
        if X_val is not None and y_val is not None:
            if self.scaler is not None:
                X_val_scaled = self.scaler.transform(X_val)
                self.training_history['val_score'] = self.model.score(X_val_scaled, y_val)
        
        self.is_fitted = True
        logger.info(f"{self.model_name} training completed.")
        
        return self
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        ì˜ˆì¸¡ ìˆ˜í–‰
        
        Args:
            X: ì˜ˆì¸¡í•  íŠ¹ì§•
            
        Returns:
            ì˜ˆì¸¡ ê²°ê³¼
        """
        if not self.is_fitted:
            raise ValueError("Model must be fitted before prediction.")
        
        if self.model is None:
            raise ValueError("Model is not initialized.")
        
        if self.scaler is None:
            raise ValueError("Scaler is not initialized.")
        
        X = self._validate_input_single(X)
        X_scaled = self.scaler.transform(X)
        predictions = self.model.predict(X_scaled)
        
        return predictions
    
    def cross_validate(self, X: np.ndarray, y: np.ndarray, cv: int = 5, 
                      scoring: str = 'neg_mean_absolute_error') -> Dict[str, Any]:
        """
        êµì°¨ ê²€ì¦ ìˆ˜í–‰
        
        Args:
            X: íŠ¹ì§•
            y: íƒ€ê²Ÿ
            cv: êµì°¨ ê²€ì¦ í´ë“œ ìˆ˜
            scoring: í‰ê°€ ì§€í‘œ
            
        Returns:
            êµì°¨ ê²€ì¦ ê²°ê³¼
        """
        logger.info(f"Performing {cv}-fold cross validation for {self.model_name}...")
        
        X, y = self._validate_input(X, y)
        X_scaled = self._fit_transform_features(X)
        
        # ëª¨ë¸ ìƒì„±
        model = self._create_model()
        
        # êµì°¨ ê²€ì¦ ì‹¤í–‰
        cv_results = cross_validate(
            model, X_scaled, y,
            cv=cv,
            scoring=['neg_mean_absolute_error', 'neg_mean_squared_error', 'r2'],
            return_train_score=True,
            n_jobs=-1
        )
        
        # ê²°ê³¼ ì •ë¦¬
        results = {
            'test_mae': -cv_results['test_neg_mean_absolute_error'],
            'test_mse': -cv_results['test_neg_mean_squared_error'],
            'test_r2': cv_results['test_r2'],
            'train_mae': -cv_results['train_neg_mean_absolute_error'],
            'train_mse': -cv_results['train_neg_mean_squared_error'],
            'train_r2': cv_results['train_r2'],
        }
        
        # í†µê³„ ìš”ì•½
        for metric in ['test_mae', 'test_mse', 'test_r2', 'train_mae', 'train_mse', 'train_r2']:
            values = results[metric]
            results[f'{metric}_mean'] = np.mean(values)
            results[f'{metric}_std'] = np.std(values)
        
        logger.info(f"Cross validation completed. Test MAE: {results['test_mae_mean']:.3f} Â± {results['test_mae_std']:.3f}")
        
        return results
    
    def evaluate(self, X: np.ndarray, y: np.ndarray) -> Dict[str, float]:
        """
        ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
        
        Args:
            X: íŠ¹ì§•
            y: ì‹¤ì œ íƒ€ê²Ÿ
            
        Returns:
            í‰ê°€ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
        """
        if not self.is_fitted:
            raise ValueError("Model must be fitted before evaluation.")
        
        predictions = self.predict(X)
        
        metrics = {
            'mae': mean_absolute_error(y, predictions),
            'mse': mean_squared_error(y, predictions),
            'rmse': np.sqrt(mean_squared_error(y, predictions)),
            'r2': r2_score(y, predictions),
            'mape': mean_absolute_percentage_error(y, predictions),
            'median_ae': median_absolute_error(y, predictions),
            'max_error': max_error(y, predictions),
            'explained_variance': explained_variance_score(y, predictions)
        }
        
        # ì‚¬ìš©ì ì •ì˜ ë©”íŠ¸ë¦­
        metrics['accuracy_0_5'] = self._accuracy_within_threshold(y, predictions, 0.5)
        metrics['accuracy_1_0'] = self._accuracy_within_threshold(y, predictions, 1.0)
        
        return metrics
    
    def get_feature_importance(self) -> Optional[np.ndarray]:
        """
        íŠ¹ì§• ì¤‘ìš”ë„ ë°˜í™˜ (ì§€ì›í•˜ëŠ” ëª¨ë¸ë§Œ)
        
        Returns:
            íŠ¹ì§• ì¤‘ìš”ë„ ë°°ì—´ (ì—†ìœ¼ë©´ None)
        """
        if not self.is_fitted:
            raise ValueError("Model must be fitted before getting feature importance.")
        
        if self.model is not None and hasattr(self.model, 'feature_importances_'):
            return self.model.feature_importances_
        else:
            logger.warning(f"{self.model_name} does not support feature importance.")
            return None
    
    def save_model(self, filepath: str):
        """
        ëª¨ë¸ ì €ì¥
        
        Args:
            filepath: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ (.pkl)
        """
        if not self.is_fitted:
            raise ValueError("Cannot save unfitted model.")
        
        model_data = {
            'model': self.model,
            'scaler': self.scaler,
            'config': self.config,
            'model_name': self.model_name,
            'feature_names': self.feature_names,
            'training_history': self.training_history,
            'is_fitted': self.is_fitted
        }
        
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        joblib.dump(model_data, filepath)
        logger.info(f"Model saved to {filepath}")
    
    @classmethod
    def load_model(cls, filepath: str):
        """
        ëª¨ë¸ ë¡œë“œ
        
        Args:
            filepath: ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ë¡œë“œëœ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
        """
        model_data = joblib.load(filepath)
        
        # ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        instance = cls(config=model_data['config'], model_name=model_data['model_name'])
        
        # ë°ì´í„° ë³µì›
        instance.model = model_data['model']
        instance.scaler = model_data['scaler']
        instance.feature_names = model_data['feature_names']
        instance.training_history = model_data['training_history']
        instance.is_fitted = model_data['is_fitted']
        
        logger.info(f"Model loaded from {filepath}")
        return instance
    
    def _validate_input(self, X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """ì…ë ¥ ë°ì´í„° ê²€ì¦ (í›ˆë ¨ìš©)"""
        if not isinstance(X, np.ndarray):
            X = np.array(X)
        
        if not isinstance(y, np.ndarray):
            y = np.array(y)
        
        if len(X.shape) == 1:
            X = X.reshape(1, -1)
        
        return X, y
    
    def _validate_input_single(self, X: np.ndarray) -> np.ndarray:
        """ì…ë ¥ ë°ì´í„° ê²€ì¦ (ì˜ˆì¸¡ìš©)"""
        if not isinstance(X, np.ndarray):
            X = np.array(X)
        
        if len(X.shape) == 1:
            X = X.reshape(1, -1)
        
        return X
    
    def _fit_transform_features(self, X: np.ndarray) -> np.ndarray:
        """íŠ¹ì§• ìŠ¤ì¼€ì¼ë§ (fit & transform)"""
        if self.scaler is None:
            raise ValueError("Scaler is not initialized.")
        return self.scaler.fit_transform(X)
    
    def _accuracy_within_threshold(self, y_true: np.ndarray, y_pred: np.ndarray, threshold: float) -> float:
        """ì„ê³„ê°’ ë‚´ ì •í™•ë„ ê³„ì‚°"""
        return float(np.mean(np.abs(y_true - y_pred) <= threshold))
    
    def __str__(self) -> str:
        """ë¬¸ìì—´ í‘œí˜„"""
        return f"{self.model_name}(fitted={self.is_fitted})"
    
    def __repr__(self) -> str:
        """ê°ì²´ í‘œí˜„"""
        return self.__str__()


class WatermelonGBT(BaseWatermelonModel):
    """
    ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ì„ ìœ„í•œ Gradient Boosting Trees ëª¨ë¸
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None, random_state: int = 42):
        super().__init__(config, "WatermelonGBT")
        self.random_state = random_state
    
    def _create_model(self) -> GradientBoostingRegressor:
        """GBT ëª¨ë¸ ìƒì„±"""
        gbt_config = self.config.get('gradient_boosting', {})
        base_params = gbt_config.get('base_params', {})
        base_params['random_state'] = self.random_state
        
        return GradientBoostingRegressor(**base_params)


class WatermelonSVM(BaseWatermelonModel):
    """
    ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ì„ ìœ„í•œ Support Vector Machine ëª¨ë¸
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None, random_state: int = 42):
        super().__init__(config, "WatermelonSVM")
        self.random_state = random_state
    
    def _create_model(self) -> SVR:
        """SVM ëª¨ë¸ ìƒì„±"""
        svm_config = self.config.get('svm', {})
        base_params = svm_config.get('base_params', {})
        
        return SVR(**base_params)


class WatermelonRandomForest(BaseWatermelonModel):
    """
    ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ì„ ìœ„í•œ Random Forest ëª¨ë¸
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None, random_state: int = 42):
        super().__init__(config, "WatermelonRandomForest")
        self.random_state = random_state
    
    def _create_model(self) -> RandomForestRegressor:
        """Random Forest ëª¨ë¸ ìƒì„±"""
        rf_config = self.config.get('random_forest', {})
        base_params = rf_config.get('base_params', {})
        base_params['random_state'] = self.random_state
        
        return RandomForestRegressor(**base_params)


class ModelFactory:
    """
    ëª¨ë¸ ìƒì„±ì„ ìœ„í•œ íŒ©í† ë¦¬ í´ë˜ìŠ¤
    """
    
    @staticmethod
    def create_model(model_type: str, config: Optional[Dict[str, Any]] = None) -> BaseWatermelonModel:
        """
        ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        
        Args:
            model_type: ëª¨ë¸ íƒ€ì… ('gbt', 'svm', 'random_forest')
            config: ëª¨ë¸ ì„¤ì •
            
        Returns:
            ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
        """
        if model_type.lower() in ['gbt', 'gradient_boosting']:
            return WatermelonGBT(config)
        elif model_type.lower() in ['svm', 'support_vector_machine']:
            return WatermelonSVM(config)
        elif model_type.lower() in ['rf', 'random_forest']:
            return WatermelonRandomForest(config)
        else:
            raise ValueError(f"Unknown model type: {model_type}")
    
    @staticmethod
    def create_all_models(config: Optional[Dict[str, Any]] = None) -> Dict[str, BaseWatermelonModel]:
        """
        ëª¨ë“  ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        
        Args:
            config: ëª¨ë¸ ì„¤ì •
            
        Returns:
            ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ë”•ì…”ë„ˆë¦¬
        """
        return {
            'gbt': WatermelonGBT(config),
            'svm': WatermelonSVM(config),
            'random_forest': WatermelonRandomForest(config)
        }


def load_config(config_path: str = "configs/models.yaml") -> Dict[str, Any]:
    """
    ì„¤ì • íŒŒì¼ ë¡œë“œ
    
    Args:
        config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
        
    Returns:
        ì„¤ì • ë”•ì…”ë„ˆë¦¬
    """
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    except FileNotFoundError:
        logger.warning(f"Config file not found: {config_path}. Using default settings.")
        return {}
    except Exception as e:
        logger.error(f"Error loading config: {e}")
        return {}


def test_models():
    """ëª¨ë¸ í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    logger.info("Testing traditional ML models...")
    
    # ê°€ìƒ ë°ì´í„° ìƒì„±
    np.random.seed(42)
    X = np.random.randn(100, 51)  # 51ê°œ íŠ¹ì§•
    y = np.random.uniform(8.0, 13.0, 100)  # 8-13 Brix ë²”ìœ„
    
    # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
    split_idx = 80
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    # ì„¤ì • ë¡œë“œ
    config = load_config()
    
    # ëª¨ë“  ëª¨ë¸ í…ŒìŠ¤íŠ¸
    models = ModelFactory.create_all_models(config)
    
    for model_name, model in models.items():
        logger.info(f"\nTesting {model_name.upper()} model:")
        
        # í›ˆë ¨
        model.fit(X_train, y_train, X_test, y_test)
        
        # ì˜ˆì¸¡
        predictions = model.predict(X_test)
        logger.info(f"Predictions shape: {predictions.shape}")
        
        # í‰ê°€
        metrics = model.evaluate(X_test, y_test)
        logger.info(f"Test MAE: {metrics['mae']:.3f}")
        logger.info(f"Test RÂ²: {metrics['r2']:.3f}")
        
        # êµì°¨ ê²€ì¦
        cv_results = model.cross_validate(X_train, y_train, cv=3)
        logger.info(f"CV MAE: {cv_results['test_mae_mean']:.3f} Â± {cv_results['test_mae_std']:.3f}")
        
        # íŠ¹ì§• ì¤‘ìš”ë„
        importance = model.get_feature_importance()
        if importance is not None:
            logger.info(f"Feature importance shape: {importance.shape}")
            logger.info(f"Top 3 important features: {np.argsort(importance)[-3:]}")
        
        # ëª¨ë¸ ì €ì¥/ë¡œë“œ í…ŒìŠ¤íŠ¸
        save_path = f"models/test_{model_name}.pkl"
        model.save_model(save_path)
        
        loaded_model = model.__class__.load_model(save_path)
        test_pred = loaded_model.predict(X_test[:5])
        logger.info(f"Loaded model prediction (first 5): {test_pred}")
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        if os.path.exists(save_path):
            os.remove(save_path)
    
    logger.info("\nAll models tested successfully! âœ…")


if __name__ == "__main__":
    test_models() 