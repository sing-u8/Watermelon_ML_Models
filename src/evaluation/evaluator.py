"""
ğŸ‰ ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ - ì„±ëŠ¥ í‰ê°€ ëª¨ë“ˆ

ì „í†µì ì¸ ML ëª¨ë¸ë“¤ì˜ í¬ê´„ì  ì„±ëŠ¥ í‰ê°€
- íšŒê·€ ë©”íŠ¸ë¦­ ê³„ì‚° ë° ë¶„ì„
- í†µê³„ì  ìœ ì˜ì„± ê²€ì •
- ëª¨ë¸ ê°„ ë¹„êµ ë¶„ì„
- ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±
- ì”ì°¨ ë¶„ì„ ë° ì§„ë‹¨
"""

import os
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any, Union
from scipy import stats
from sklearn.metrics import (
    mean_absolute_error, mean_squared_error, r2_score,
    mean_absolute_percentage_error, median_absolute_error,
    max_error, explained_variance_score
)
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ModelEvaluator:
    """
    ML ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ë¥¼ ìœ„í•œ ì¢…í•© í´ë˜ìŠ¤
    """
    
    def __init__(self, target_range: Tuple[float, float] = (8.0, 13.0)):
        """
        í‰ê°€ì ì´ˆê¸°í™”
        
        Args:
            target_range: ë‹¹ë„ ì˜ˆì¸¡ ë²”ìœ„ (min, max)
        """
        self.target_range = target_range
        self.evaluation_results = {}
        
    def calculate_regression_metrics(self, y_true: np.ndarray, y_pred: np.ndarray, 
                                   model_name: str = "model") -> Dict[str, float]:
        """
        íšŒê·€ ë©”íŠ¸ë¦­ ê³„ì‚°
        
        Args:
            y_true: ì‹¤ì œ ê°’
            y_pred: ì˜ˆì¸¡ ê°’
            model_name: ëª¨ë¸ ì´ë¦„
            
        Returns:
            ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
        """
        try:
            metrics = {
                # ê¸°ë³¸ íšŒê·€ ë©”íŠ¸ë¦­
                'mae': float(mean_absolute_error(y_true, y_pred)),
                'mse': float(mean_squared_error(y_true, y_pred)),
                'rmse': float(np.sqrt(mean_squared_error(y_true, y_pred))),
                'r2': float(r2_score(y_true, y_pred)),
                'mape': float(mean_absolute_percentage_error(y_true, y_pred)),
                'median_ae': float(median_absolute_error(y_true, y_pred)),
                'max_error': float(max_error(y_true, y_pred)),
                'explained_variance': float(explained_variance_score(y_true, y_pred)),
                
                # ì¶”ê°€ ë©”íŠ¸ë¦­
                'relative_mae': float(mean_absolute_error(y_true, y_pred) / np.mean(y_true)),
                'std_residuals': float(np.std(y_true - y_pred)),
                'mean_residuals': float(np.mean(y_true - y_pred)),
                
                # ì •í™•ë„ ë©”íŠ¸ë¦­ (Â±0.5, Â±1.0 Brix ë‚´)
                'accuracy_0_5': self._accuracy_within_threshold(y_true, y_pred, 0.5),
                'accuracy_1_0': self._accuracy_within_threshold(y_true, y_pred, 1.0),
                
                # ìƒê´€ê³„ìˆ˜
                'pearson_corr': float(stats.pearsonr(y_true, y_pred)[0]),
                'spearman_corr': float(stats.spearmanr(y_true, y_pred)[0]),
                
                # ë²”ìœ„ ì í•©ì„±
                'predictions_in_range': self._predictions_in_valid_range(y_pred),
                'range_coverage': self._range_coverage(y_pred)
            }
            
            # ì”ì°¨ ë¶„ì„
            residuals = y_true - y_pred
            metrics.update(self._analyze_residuals(residuals))
            
            logger.info(f"Calculated {len(metrics)} metrics for {model_name}")
            return metrics
            
        except Exception as e:
            logger.error(f"Error calculating metrics for {model_name}: {e}")
            return {}
    
    def _accuracy_within_threshold(self, y_true: np.ndarray, y_pred: np.ndarray, 
                                  threshold: float) -> float:
        """ì„ê³„ê°’ ë‚´ ì •í™•ë„ ê³„ì‚°"""
        return float(np.mean(np.abs(y_true - y_pred) <= threshold))
    
    def _predictions_in_valid_range(self, y_pred: np.ndarray) -> float:
        """ìœ íš¨ ë²”ìœ„ ë‚´ ì˜ˆì¸¡ ë¹„ìœ¨"""
        min_val, max_val = self.target_range
        in_range = np.logical_and(y_pred >= min_val, y_pred <= max_val)
        return float(np.mean(in_range))
    
    def _range_coverage(self, y_pred: np.ndarray) -> float:
        """ì˜ˆì¸¡ ë²”ìœ„ ì»¤ë²„ë¦¬ì§€"""
        pred_range = np.max(y_pred) - np.min(y_pred)
        target_range = self.target_range[1] - self.target_range[0]
        return float(min(pred_range / target_range, 1.0))
    
    def _analyze_residuals(self, residuals: np.ndarray) -> Dict[str, float]:
        """ì”ì°¨ ë¶„ì„"""
        residuals_analysis = {
            'residuals_skewness': float(stats.skew(residuals)),
            'residuals_kurtosis': float(stats.kurtosis(residuals)),
            'residuals_normality_pvalue': float(stats.normaltest(residuals)[1]),
            'residuals_q25': float(np.percentile(residuals, 25)),
            'residuals_q75': float(np.percentile(residuals, 75)),
            'residuals_iqr': float(np.percentile(residuals, 75) - np.percentile(residuals, 25))
        }
        
        return residuals_analysis
    
    def evaluate_model_performance(self, y_true: np.ndarray, y_pred: np.ndarray, 
                                 model_name: str, dataset_name: str = "test") -> Dict[str, Any]:
        """
        ë‹¨ì¼ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
        
        Args:
            y_true: ì‹¤ì œ ê°’
            y_pred: ì˜ˆì¸¡ ê°’
            model_name: ëª¨ë¸ ì´ë¦„
            dataset_name: ë°ì´í„°ì…‹ ì´ë¦„
            
        Returns:
            í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        metrics = self.calculate_regression_metrics(y_true, y_pred, model_name)
        
        # ì„±ëŠ¥ ë“±ê¸‰ ê²°ì •
        performance_grade = self._determine_performance_grade(metrics)
        
        # ê°•ì /ì•½ì  ë¶„ì„
        strengths, weaknesses = self._analyze_strengths_weaknesses(metrics)
        
        result = {
            'model_name': model_name,
            'dataset': dataset_name,
            'n_samples': len(y_true),
            'metrics': metrics,
            'performance_grade': performance_grade,
            'strengths': strengths,
            'weaknesses': weaknesses,
            'summary': self._generate_summary(model_name, metrics, performance_grade)
        }
        
        # ê²°ê³¼ ì €ì¥
        if model_name not in self.evaluation_results:
            self.evaluation_results[model_name] = {}
        self.evaluation_results[model_name][dataset_name] = result
        
        return result
    
    def _determine_performance_grade(self, metrics: Dict[str, float]) -> str:
        """ì„±ëŠ¥ ë“±ê¸‰ ê²°ì •"""
        mae = metrics.get('mae', float('inf'))
        r2 = metrics.get('r2', 0)
        accuracy_1_0 = metrics.get('accuracy_1_0', 0)
        
        # ë“±ê¸‰ ê¸°ì¤€
        if mae < 0.5 and r2 > 0.9 and accuracy_1_0 > 0.95:
            return "EXCELLENT"
        elif mae < 0.8 and r2 > 0.8 and accuracy_1_0 > 0.9:
            return "VERY_GOOD"
        elif mae < 1.0 and r2 > 0.7 and accuracy_1_0 > 0.85:
            return "GOOD"
        elif mae < 1.5 and r2 > 0.5 and accuracy_1_0 > 0.75:
            return "FAIR"
        else:
            return "POOR"
    
    def _analyze_strengths_weaknesses(self, metrics: Dict[str, float]) -> Tuple[List[str], List[str]]:
        """ê°•ì /ì•½ì  ë¶„ì„"""
        strengths = []
        weaknesses = []
        
        # MAE ë¶„ì„
        mae = metrics.get('mae', float('inf'))
        if mae < 0.5:
            strengths.append("Very low prediction error (MAE < 0.5)")
        elif mae < 1.0:
            strengths.append("Low prediction error (MAE < 1.0)")
        elif mae > 2.0:
            weaknesses.append("High prediction error (MAE > 2.0)")
        
        # RÂ² ë¶„ì„
        r2 = metrics.get('r2', 0)
        if r2 > 0.9:
            strengths.append("Excellent variance explanation (RÂ² > 0.9)")
        elif r2 > 0.7:
            strengths.append("Good variance explanation (RÂ² > 0.7)")
        elif r2 < 0.5:
            weaknesses.append("Poor variance explanation (RÂ² < 0.5)")
        
        # ì •í™•ë„ ë¶„ì„
        acc_1_0 = metrics.get('accuracy_1_0', 0)
        if acc_1_0 > 0.9:
            strengths.append("High accuracy within Â±1.0 Brix")
        elif acc_1_0 < 0.7:
            weaknesses.append("Low accuracy within Â±1.0 Brix")
        
        # ì”ì°¨ ë¶„ì„
        residuals_normality = metrics.get('residuals_normality_pvalue', 0)
        if residuals_normality > 0.05:
            strengths.append("Normally distributed residuals")
        else:
            weaknesses.append("Non-normal residual distribution")
        
        # ì˜ˆì¸¡ ë²”ìœ„ ë¶„ì„
        in_range = metrics.get('predictions_in_range', 0)
        if in_range > 0.95:
            strengths.append("Predictions within valid range")
        elif in_range < 0.9:
            weaknesses.append("Some predictions outside valid range")
        
        return strengths, weaknesses
    
    def _generate_summary(self, model_name: str, metrics: Dict[str, float], 
                         grade: str) -> str:
        """ì„±ëŠ¥ ìš”ì•½ ìƒì„±"""
        mae = metrics.get('mae', 0)
        r2 = metrics.get('r2', 0)
        acc_1_0 = metrics.get('accuracy_1_0', 0)
        
        summary = (
            f"{model_name} achieved {grade} performance with "
            f"MAE of {mae:.3f} Brix, RÂ² of {r2:.3f}, and "
            f"{acc_1_0:.1%} accuracy within Â±1.0 Brix."
        )
        
        return summary
    
    def compare_models(self, model_results: Dict[str, Dict[str, Any]], 
                      dataset_name: str = "test") -> pd.DataFrame:
        """
        ì—¬ëŸ¬ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
        
        Args:
            model_results: ëª¨ë¸ë³„ í‰ê°€ ê²°ê³¼
            dataset_name: ë¹„êµí•  ë°ì´í„°ì…‹
            
        Returns:
            ë¹„êµ ê²°ê³¼ DataFrame
        """
        comparison_data = []
        
        for model_name, results in model_results.items():
            if dataset_name in results:
                result = results[dataset_name]
                metrics = result['metrics']
                
                comparison_data.append({
                    'model': model_name,
                    'mae': metrics.get('mae', np.nan),
                    'rmse': metrics.get('rmse', np.nan),
                    'r2': metrics.get('r2', np.nan),
                    'mape': metrics.get('mape', np.nan),
                    'accuracy_0_5': metrics.get('accuracy_0_5', np.nan),
                    'accuracy_1_0': metrics.get('accuracy_1_0', np.nan),
                    'pearson_corr': metrics.get('pearson_corr', np.nan),
                    'performance_grade': result.get('performance_grade', 'UNKNOWN'),
                    'n_samples': result.get('n_samples', 0)
                })
        
        if not comparison_data:
            return pd.DataFrame()
        
        df = pd.DataFrame(comparison_data)
        
        # ìˆœìœ„ ì¶”ê°€
        df['mae_rank'] = df['mae'].rank()
        df['r2_rank'] = df['r2'].rank(ascending=False)
        df['overall_rank'] = (df['mae_rank'] + df['r2_rank']) / 2
        
        # ì •ë ¬
        df = df.sort_values('overall_rank').reset_index(drop=True)
        
        return df
    
    def get_performance_summary(self, model_results: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """ì „ì²´ ì„±ëŠ¥ ìš”ì•½"""
        summary = {
            'total_models': len(model_results),
            'datasets_evaluated': set(),
            'best_performers': {},
            'performance_distribution': {},
            'key_insights': []
        }
        
        # ë°ì´í„°ì…‹ë³„ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
        all_metrics = {}
        for model_name, results in model_results.items():
            for dataset_name, result in results.items():
                summary['datasets_evaluated'].add(dataset_name)
                
                if dataset_name not in all_metrics:
                    all_metrics[dataset_name] = []
                
                metrics = result['metrics']
                all_metrics[dataset_name].append({
                    'model': model_name,
                    'mae': metrics.get('mae', float('inf')),
                    'r2': metrics.get('r2', 0),
                    'grade': result.get('performance_grade', 'UNKNOWN')
                })
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ê²°ì •
        for dataset, metrics_list in all_metrics.items():
            best_mae = min(metrics_list, key=lambda x: x['mae'])
            best_r2 = max(metrics_list, key=lambda x: x['r2'])
            
            summary['best_performers'][dataset] = {
                'best_mae': best_mae['model'],
                'best_r2': best_r2['model'],
                'mae_value': best_mae['mae'],
                'r2_value': best_r2['r2']
            }
        
        # ì„±ëŠ¥ ë¶„í¬
        grades = [result['performance_grade'] 
                 for results in model_results.values() 
                 for result in results.values()]
        
        grade_counts = pd.Series(grades).value_counts().to_dict()
        summary['performance_distribution'] = grade_counts
        
        # ì£¼ìš” ì¸ì‚¬ì´íŠ¸
        summary['key_insights'] = self._generate_key_insights(model_results, summary)
        
        return summary
    
    def _generate_key_insights(self, model_results: Dict[str, Dict[str, Any]], 
                              summary: Dict[str, Any]) -> List[str]:
        """ì£¼ìš” ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []
        
        # ìµœê³  ì„±ëŠ¥ ë¶„ì„
        test_best = summary['best_performers'].get('test', {})
        if test_best:
            mae_value = test_best.get('mae_value', 0)
            if mae_value < 0.5:
                insights.append("Achieved excellent prediction accuracy (MAE < 0.5 Brix)")
            elif mae_value < 1.0:
                insights.append("Achieved target prediction accuracy (MAE < 1.0 Brix)")
            else:
                insights.append("Did not achieve target prediction accuracy (MAE â‰¥ 1.0 Brix)")
        
        # ëª¨ë¸ ì¼ê´€ì„± ë¶„ì„
        model_grades = {}
        for model_name, results in model_results.items():
            grades = [result['performance_grade'] for result in results.values()]
            model_grades[model_name] = grades
        
        consistent_models = [model for model, grades in model_grades.items() 
                           if len(set(grades)) == 1]
        
        if consistent_models:
            insights.append(f"Models with consistent performance: {', '.join(consistent_models)}")
        
        # ì„±ëŠ¥ ë¶„í¬ ë¶„ì„
        grade_dist = summary['performance_distribution']
        excellent_count = grade_dist.get('EXCELLENT', 0)
        poor_count = grade_dist.get('POOR', 0)
        
        if excellent_count > 0:
            insights.append(f"{excellent_count} model(s) achieved excellent performance")
        if poor_count > 0:
            insights.append(f"{poor_count} model(s) showed poor performance")
        
        return insights


class ComparisonAnalyzer:
    """
    ëª¨ë¸ ê°„ í†µê³„ì  ë¹„êµ ë¶„ì„
    """
    
    def __init__(self, alpha: float = 0.05):
        """
        ë¹„êµ ë¶„ì„ì ì´ˆê¸°í™”
        
        Args:
            alpha: ìœ ì˜ìˆ˜ì¤€
        """
        self.alpha = alpha
    
    def statistical_comparison(self, results1: np.ndarray, results2: np.ndarray, 
                             model1_name: str, model2_name: str) -> Dict[str, Any]:
        """
        ë‘ ëª¨ë¸ì˜ í†µê³„ì  ë¹„êµ
        
        Args:
            results1: ëª¨ë¸1 ì˜ˆì¸¡ ê²°ê³¼ (ì”ì°¨ ë˜ëŠ” ì—ëŸ¬)
            results2: ëª¨ë¸2 ì˜ˆì¸¡ ê²°ê³¼ (ì”ì°¨ ë˜ëŠ” ì—ëŸ¬)
            model1_name: ëª¨ë¸1 ì´ë¦„
            model2_name: ëª¨ë¸2 ì´ë¦„
            
        Returns:
            í†µê³„ì  ë¹„êµ ê²°ê³¼
        """
        comparison = {
            'models': f"{model1_name} vs {model2_name}",
            'n_samples': len(results1)
        }
        
        # í‰ê·  ë¹„êµ (t-test)
        t_stat, t_pvalue = stats.ttest_rel(results1, results2)
        comparison['paired_ttest'] = {
            't_statistic': float(t_stat),
            'p_value': float(t_pvalue),
            'significant': t_pvalue < self.alpha,
            'interpretation': self._interpret_ttest(t_pvalue, model1_name, model2_name)
        }
        
        # ë¶„í¬ ë¹„êµ (Wilcoxon signed-rank test)
        wilcoxon_stat, wilcoxon_pvalue = stats.wilcoxon(results1, results2)
        comparison['wilcoxon_test'] = {
            'statistic': float(wilcoxon_stat),
            'p_value': float(wilcoxon_pvalue),
            'significant': wilcoxon_pvalue < self.alpha,
            'interpretation': self._interpret_wilcoxon(wilcoxon_pvalue, model1_name, model2_name)
        }
        
        # íš¨ê³¼ í¬ê¸° (Cohen's d)
        cohens_d = self._calculate_cohens_d(results1, results2)
        comparison['effect_size'] = {
            'cohens_d': cohens_d,
            'magnitude': self._interpret_effect_size(cohens_d)
        }
        
        # ì‹¤ìš©ì  ì°¨ì´
        mean_diff = np.mean(results1) - np.mean(results2)
        comparison['practical_difference'] = {
            'mean_difference': float(mean_diff),
            'relative_difference': float(mean_diff / np.mean(np.abs(results1)) * 100),
            'practically_significant': abs(mean_diff) > 0.1  # 0.1 Brix ì´ìƒ ì°¨ì´
        }
        
        return comparison
    
    def _interpret_ttest(self, p_value: float, model1: str, model2: str) -> str:
        """t-test ê²°ê³¼ í•´ì„"""
        if p_value < self.alpha:
            return f"Significant difference between {model1} and {model2} (p < {self.alpha})"
        else:
            return f"No significant difference between {model1} and {model2} (p â‰¥ {self.alpha})"
    
    def _interpret_wilcoxon(self, p_value: float, model1: str, model2: str) -> str:
        """Wilcoxon test ê²°ê³¼ í•´ì„"""
        if p_value < self.alpha:
            return f"Significant difference in distributions between {model1} and {model2}"
        else:
            return f"No significant difference in distributions between {model1} and {model2}"
    
    def _calculate_cohens_d(self, x1: np.ndarray, x2: np.ndarray) -> float:
        """Cohen's d íš¨ê³¼ í¬ê¸° ê³„ì‚°"""
        pooled_std = np.sqrt(((len(x1) - 1) * np.var(x1, ddof=1) + 
                             (len(x2) - 1) * np.var(x2, ddof=1)) / 
                            (len(x1) + len(x2) - 2))
        return float((np.mean(x1) - np.mean(x2)) / pooled_std)
    
    def _interpret_effect_size(self, cohens_d: float) -> str:
        """íš¨ê³¼ í¬ê¸° í•´ì„"""
        abs_d = abs(cohens_d)
        if abs_d < 0.2:
            return "negligible"
        elif abs_d < 0.5:
            return "small"
        elif abs_d < 0.8:
            return "medium"
        else:
            return "large"
    
    def multiple_model_comparison(self, model_errors: Dict[str, np.ndarray]) -> Dict[str, Any]:
        """
        ì—¬ëŸ¬ ëª¨ë¸ ê°„ ì¢…í•© ë¹„êµ
        
        Args:
            model_errors: ëª¨ë¸ë³„ ì—ëŸ¬ ë°°ì—´
            
        Returns:
            ì¢…í•© ë¹„êµ ê²°ê³¼
        """
        models = list(model_errors.keys())
        n_models = len(models)
        
        if n_models < 2:
            return {"error": "At least 2 models required for comparison"}
        
        # ì¼ì› ë¶„ì‚°ë¶„ì„ (ANOVA)
        error_arrays = [model_errors[model] for model in models]
        f_stat, anova_pvalue = stats.f_oneway(*error_arrays)
        
        comparison = {
            'models': models,
            'n_models': n_models,
            'anova': {
                'f_statistic': float(f_stat),
                'p_value': float(anova_pvalue),
                'significant': anova_pvalue < self.alpha,
                'interpretation': self._interpret_anova(anova_pvalue)
            }
        }
        
        # ì‚¬í›„ ë¶„ì„ (pairwise comparisons)
        if anova_pvalue < self.alpha:
            pairwise_results = []
            for i in range(n_models):
                for j in range(i + 1, n_models):
                    pair_result = self.statistical_comparison(
                        model_errors[models[i]], 
                        model_errors[models[j]],
                        models[i], 
                        models[j]
                    )
                    pairwise_results.append(pair_result)
            
            comparison['pairwise_comparisons'] = pairwise_results
        
        return comparison
    
    def _interpret_anova(self, p_value: float) -> str:
        """ANOVA ê²°ê³¼ í•´ì„"""
        if p_value < self.alpha:
            return f"Significant differences exist among models (p < {self.alpha})"
        else:
            return f"No significant differences among models (p â‰¥ {self.alpha})"
    
    def performance_degradation_analysis(self, train_metrics: Dict[str, float], 
                                       val_metrics: Dict[str, float],
                                       test_metrics: Dict[str, float],
                                       model_name: str) -> Dict[str, Any]:
        """
        ì„±ëŠ¥ ì €í•˜ ë¶„ì„ (ê³¼ì í•© íƒì§€)
        
        Args:
            train_metrics: í›ˆë ¨ ì„±ëŠ¥
            val_metrics: ê²€ì¦ ì„±ëŠ¥  
            test_metrics: í…ŒìŠ¤íŠ¸ ì„±ëŠ¥
            model_name: ëª¨ë¸ ì´ë¦„
            
        Returns:
            ì„±ëŠ¥ ì €í•˜ ë¶„ì„ ê²°ê³¼
        """
        analysis = {
            'model_name': model_name,
            'overfitting_indicators': {},
            'generalization_gap': {},
            'recommendations': []
        }
        
        # MAE ê¸°ë°˜ ê³¼ì í•© ë¶„ì„
        train_mae = train_metrics.get('mae', 0)
        val_mae = val_metrics.get('mae', 0)
        test_mae = test_metrics.get('mae', 0)
        
        train_val_gap = val_mae - train_mae
        val_test_gap = test_mae - val_mae
        
        analysis['generalization_gap'] = {
            'train_val_gap': float(train_val_gap),
            'val_test_gap': float(val_test_gap),
            'train_val_ratio': float(val_mae / train_mae) if train_mae > 0 else np.inf,
            'val_test_ratio': float(test_mae / val_mae) if val_mae > 0 else np.inf
        }
        
        # ê³¼ì í•© ì§€í‘œ
        analysis['overfitting_indicators'] = {
            'significant_train_val_gap': train_val_gap > 0.2,  # 0.2 Brix ì´ìƒ
            'large_performance_ratio': (val_mae / train_mae) > 1.5 if train_mae > 0 else False,
            'degraded_test_performance': test_mae > val_mae * 1.2
        }
        
        # ê¶Œì¥ì‚¬í•­
        if any(analysis['overfitting_indicators'].values()):
            analysis['recommendations'].extend([
                "Consider regularization techniques",
                "Increase training data size",
                "Reduce model complexity",
                "Apply cross-validation"
            ])
        
        if train_val_gap > 0.5:
            analysis['recommendations'].append("Strong overfitting detected - review model architecture")
        
        if val_test_gap > 0.3:
            analysis['recommendations'].append("Poor generalization - validate data distribution")
        
        return analysis


def test_evaluator():
    """í‰ê°€ì í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸"""
    logger.info("Testing ModelEvaluator...")
    
    # ê°€ìƒ ë°ì´í„° ìƒì„±
    np.random.seed(42)
    n_samples = 100
    
    # ì‹¤ì œ ê°’
    y_true = np.random.uniform(8.0, 13.0, n_samples)
    
    # ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê°’ ìƒì„±
    models_predictions = {
        'GBT': y_true + np.random.normal(0, 0.3, n_samples),  # ì¢‹ì€ ì„±ëŠ¥
        'SVM': y_true + np.random.normal(0, 0.8, n_samples),  # ë³´í†µ ì„±ëŠ¥
        'RF': y_true + np.random.normal(0, 0.5, n_samples)    # ì¤‘ê°„ ì„±ëŠ¥
    }
    
    # í‰ê°€ì ìƒì„±
    evaluator = ModelEvaluator()
    
    # ê° ëª¨ë¸ í‰ê°€
    model_results = {}
    for model_name, y_pred in models_predictions.items():
        result = evaluator.evaluate_model_performance(
            y_true, y_pred, model_name, "test"
        )
        model_results[model_name] = {'test': result}
        
        logger.info(f"{model_name} - Grade: {result['performance_grade']}, "
                   f"MAE: {result['metrics']['mae']:.3f}")
    
    # ëª¨ë¸ ë¹„êµ
    comparison_df = evaluator.compare_models(model_results, "test")
    logger.info(f"Model comparison completed. Best model: {comparison_df.iloc[0]['model']}")
    
    # í†µê³„ì  ë¹„êµ
    analyzer = ComparisonAnalyzer()
    
    # GBT vs SVM ë¹„êµ
    gbt_errors = np.abs(y_true - models_predictions['GBT'])
    svm_errors = np.abs(y_true - models_predictions['SVM'])
    
    stat_comparison = analyzer.statistical_comparison(
        gbt_errors, svm_errors, "GBT", "SVM"
    )
    
    logger.info(f"Statistical comparison: {stat_comparison['paired_ttest']['interpretation']}")
    
    # ì„±ëŠ¥ ìš”ì•½
    summary = evaluator.get_performance_summary(model_results)
    logger.info(f"Performance summary: {len(summary['key_insights'])} key insights generated")
    
    logger.info("ModelEvaluator test completed successfully! âœ…")


if __name__ == "__main__":
    test_evaluator() 